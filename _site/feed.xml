<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-06-04T23:04:24+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Mr.Q’s HUB</title><entry><title type="html">Probability</title><link href="http://localhost:4000/PS/prob/" rel="alternate" type="text/html" title="Probability" /><published>2020-04-25T12:47:53+09:00</published><updated>2020-04-25T12:47:53+09:00</updated><id>http://localhost:4000/PS/PS-prob</id><content type="html" xml:base="http://localhost:4000/PS/prob/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Basics
    &lt;ul&gt;
      &lt;li&gt;Sampling Table&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;basics&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;prob&quot;&gt;&lt;/a&gt;&lt;strong&gt;Sampling Table&lt;/strong&gt;: #diff ways to take a sample of $k$ out of a population $n$&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;Order&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;No Order&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Replacement&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$n^k$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}n+k-1 \\ k \end{pmatrix}=\frac{(n+k-1)!}{k!(n-1)!}&lt;/script&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;No replacement&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{n!}{(n-k)!}&lt;/script&gt;&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}n \\ k \end{pmatrix}=\frac{n!}{k!(n-k)!}&lt;/script&gt;&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Examples:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Draw 1 card out of 5 cards and put it back for 2 times with order: $5^2=25$&lt;/li&gt;
          &lt;li&gt;Draw 1 card out of 5 cards and put it back for 2 times with no order: &lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}5+2-1 \\ 2\end{pmatrix}=15&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Draw 2 cards out of 5 cards at once with order: $\frac{5!}{(5-2)!}=20$&lt;/li&gt;
          &lt;li&gt;Draw 2 cards out of 5 cards at once with no order: &lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}5 \\ 2 \end{pmatrix}=10&lt;/script&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Roadmap: Basics Sampling Table</summary></entry><entry><title type="html">Differential Equations</title><link href="http://localhost:4000/math/diffeq/" rel="alternate" type="text/html" title="Differential Equations" /><published>2020-04-25T12:47:53+09:00</published><updated>2020-04-25T12:47:53+09:00</updated><id>http://localhost:4000/math/math-diffeq</id><content type="html" xml:base="http://localhost:4000/math/diffeq/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1oode&quot;&gt;Linear 1st-Order ODE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2oode&quot;&gt;Linear 2nd-Order ODE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;1oode&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;linear-1st-order-ode&quot;&gt;Linear 1st-Order ODE&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Problem&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  y'+p(x)y=q(x)
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Solution&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  y=Ce^{-\int{p(x)dx}}+e^{-\int{p(x)dx}}\times\int{q(x)e^{\int{p(x)dx}}}dx
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Derivation&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;Solve for $y_h$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 y_h'+p(x)y_h&amp;=0 \\
 \int{\frac{dy_h}{y_h}}&amp;=\int{-p(x)dx} \\
 y_h&amp;=e^{-\int{p(x)dx}}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Let $u(x)$ be an integrating factor. Multiply this on both sides:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 u(x)y'+u(x)p(x)y=u(x)q(x)
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Recall product rule $(fg)’=f’g+fg’$. Notice:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 f=y &amp;\Rightarrow f'=y'\\
 g=u(x) &amp;\Rightarrow g'=u(x)p(x) \\
 (fg)'&amp;=u(x)q(x)
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Solve for $u(x)$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 u'(x)&amp;=u(x)p(x) \\
 \int{\frac{du}{u}}&amp;=\int{p(x)dx} \\
 u(x)&amp;=e^{\int{p(x)dx}}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Solve for $y_g$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \big(u(x)y_g\big)'&amp;=u(x)q(x) \\
 u(x)y_g&amp;=\int{u(x)q(x)dx} \\
 y_g&amp;=\frac{\int{u(x)q(x)dx}}{u(x)} \\
 y_g&amp;=\frac{\int{e^{\int{p(x)dx}}q(x)dx}}{e^{\int{p(x)dx}}}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Roadmap: Linear 1st-Order ODE Linear 2nd-Order ODE</summary></entry><entry><title type="html">Financial Foundations</title><link href="http://localhost:4000/quant/foundation/" rel="alternate" type="text/html" title="Financial Foundations" /><published>2020-03-18T22:29:53+09:00</published><updated>2020-03-18T22:29:53+09:00</updated><id>http://localhost:4000/quant/quant-foundation</id><content type="html" xml:base="http://localhost:4000/quant/foundation/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#tvm&quot;&gt;Time Value of Money&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ff&quot;&gt;Forwards &amp;amp; Futures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#opt&quot;&gt;Options&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;tvm&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;time-value-of-money&quot;&gt;Time Value of Money&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Discrete Compounding&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  M(n)=M(0)\big(1+\frac{r}{m}\big)^{mn}
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$M(n)$: FV of your investment&lt;/li&gt;
      &lt;li&gt;$M(0)$: PV of your investment&lt;/li&gt;
      &lt;li&gt;$r$: annual interest rate&lt;/li&gt;
      &lt;li&gt;$m$: #times you receive interest per annum&lt;/li&gt;
      &lt;li&gt;$n$: #years&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Law of 72&lt;/strong&gt;: the calculation of how many years it takes to double my money&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  n\times (r\times100)\approx72
  \end{equation}&lt;/script&gt;

    &lt;p&gt;Derivation:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  (1+r)^n&amp;=2 \\
  n&amp;=\log_{1+r}{2} \\
  n&amp;\approx\frac{72}{r\times100}  
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Continuous Compounding&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;\text{FV}=M(t)=M(0)\cdot e^{rt} \\
  &amp;\text{PV}=M(t)=M(T)\cdot e^{-r(T-t)}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;Derivation 1:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \lim_{m\to\infty} \big(1+\frac{r}{m}\big)^{mt}&amp;=\lim_{a\to 0} e^{\frac{rt}{a}\ln{(1+a)}} \\
  &amp;=e^{rt\lim_{a\to 0} \frac{\ln{(1+a)}}{a}} \\
  &amp;=e^{rt}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;Derivation 2:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;M(t+dt)-M(t)\approx\frac{dM}{dt}dt+\cdots \\
  &amp;\Rightarrow\frac{dM}{dt}=rM(t)
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;ff&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;forwards--futures&quot;&gt;Forwards &amp;amp; Futures&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Forwards vs Futures&lt;/strong&gt;&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;Forwards&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;Futures&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;the &lt;strong&gt;obligation&lt;/strong&gt; to buy an asset&lt;br /&gt;at some &lt;strong&gt;specific time&lt;/strong&gt; (i.e. &lt;strong&gt;maturity&lt;/strong&gt;)&lt;br /&gt;at some &lt;strong&gt;specific price&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;the &lt;strong&gt;obligation&lt;/strong&gt; to buy an asset&lt;br /&gt;with a &lt;strong&gt;gradual price settling&lt;/strong&gt; (daily) until maturity&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Can be privately negotiated&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;highly standardized&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;OTC&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;exchange&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;high counterparty risk (one party defaults)&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;high liquidity (can be traded whenever)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No-Arbitrage Principle&lt;/strong&gt;: There is &lt;strong&gt;&lt;u&gt;NO risk-free profit&lt;/u&gt;&lt;/strong&gt; in the financial markets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Example: simple portfolio&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/quant/simpleport.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;$A$ receives a forward contract from $B$.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;At time $t$, $A$ goes short (sells the asset to $C$) and receives $S(t)$, the spot price of the forward contract at $t$.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$A$ puts $S(t)$ into the bank and receives interest till time $T$.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;At time $T$, i.e. maturity date, $A$ receives the asset and gives $F$ to $B$. $A$’s net position becomes $S(t)e^{r(T-t)}-F$.&lt;/p&gt;

            &lt;p&gt;Eventually, the following holds:&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;If you perceive that $S(t)e^{r(T-t)}&amp;gt;F$, you go along with the forward contract and make riskless profit.&lt;/li&gt;
              &lt;li&gt;If you perceive that $S(t)e^{r(T-t)}&amp;lt;F$, you short the forward contract and make riskless profit.&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;But macro-wise, this will never happen since investors will smell this fresh meat and come for it. Price will automatically adjust to eliminate such riskless profit.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Eventually, you end up with:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 S(t)e^{r(T-t)}=F
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;opt&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;options&quot;&gt;Options&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Call Option&lt;/strong&gt;: the &lt;strong&gt;right&lt;/strong&gt; to &lt;strong&gt;buy&lt;/strong&gt; a particular asset for an agreed amount at a specific time.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Exercise/Strike Price&lt;/strong&gt;: the agreed amount&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Expiry&lt;/strong&gt;: the specific time&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Underlying Asset&lt;/strong&gt;: the particular asset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Payoff Function&lt;/strong&gt;: our return on the option&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \max{(S-E,0)}
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$S$: asset price&lt;/li&gt;
          &lt;li&gt;$E$: strike price&lt;/li&gt;
          &lt;li&gt;$S&amp;gt;E\Rightarrow$ let’s do this&lt;/li&gt;
          &lt;li&gt;$S&amp;lt;E\Rightarrow$ nahh let’s chill&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Put Option&lt;/strong&gt;: the &lt;strong&gt;right&lt;/strong&gt; to &lt;strong&gt;sell&lt;/strong&gt; a particular asset for an agreed amount at a specific time.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Payoff Function&lt;/strong&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \max{(E-S,0)}
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Factors affecting derivatives prices:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Variables&lt;/strong&gt;:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$S$: underlying&lt;/li&gt;
          &lt;li&gt;$t$: time to expiry&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Parameters&lt;/strong&gt;:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;interest rate&lt;/li&gt;
          &lt;li&gt;strike price&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;volatility&lt;/strong&gt;: a measure of #fluctuation in $S$ (i.e. a measure of randomness)&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Leverage&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Today is 2020/04/08. The price of Microsoft (MSFT)’s stock is $163.49.&lt;/li&gt;
      &lt;li&gt;The cost of a 165 call option with expiry 2020/04/15 is $10.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;You would like to profit off the expectation that the stock price will rise dramatically within this week. You have two choices:&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;Buy the stock&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;You buy the stock at $163.49 on 2020/04/08.&lt;/li&gt;
              &lt;li&gt;The stock price becomes $180 on 2020/04/15.&lt;/li&gt;
              &lt;li&gt;Your return on investment will be: $\frac{180-163.49}{163.49}\times 100\% =10.10\%$.&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Buy the call&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;You buy the call at $10 on 2020/04/08.&lt;/li&gt;
              &lt;li&gt;The stock price becomes $180 on 2020/04/15.&lt;/li&gt;
              &lt;li&gt;Your return on investment will be: $\frac{180-165-10}{10}\times 100\% =50\%$.&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This is an example of leverage, where you expect to get a significantly higher payoff for a small investment. The downside is the risk of facing 100% loss with buying the option.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Hedging&lt;/strong&gt;: the offsetting of the writer’s risk of writing a highly-leveraged contract by buying other related contracts.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The confusing &amp;amp; fancy terms&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Premium&lt;/strong&gt;: the amount paid for the contract&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Intrinsic value&lt;/strong&gt;: the payoff that would be received if the underlying is at its current level when the option expires&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Time value&lt;/strong&gt;: any value that the option has above its intrinsic value&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;In the money&lt;/strong&gt;: an option with positive intrinsic value&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Out of the money&lt;/strong&gt;: an option with no intrinsic value&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;At the money&lt;/strong&gt;: a call/put with a strike $\approx$ current asset level&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Long position&lt;/strong&gt;: a positive amount of a quantity&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Short position&lt;/strong&gt;: a negative amount of a quantity&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Writing options&lt;/strong&gt;: The writer of an option promises to deliver the underlying asset, if the option is a call or buy the asset if the option is a put. The writer receives the premium but faces obligations in the future.&lt;/p&gt;

        &lt;p&gt;The purchaser faces a &lt;u&gt;limited downside&lt;/u&gt; of initial premium but an &lt;u&gt;unlimited upside&lt;/u&gt;.&lt;br /&gt;
  The writer faces a &lt;u&gt;limited upside&lt;/u&gt; of guaranteed payment but an &lt;u&gt;unlimited downside&lt;/u&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Clearing houses&lt;/strong&gt;: register &amp;amp; settle options on the deposit of a margin by the writers (&lt;s&gt;default risk)&lt;/s&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Initial margin&lt;/strong&gt;: the amount deposited at the intiation of the contract.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Types of options by exercise:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;European Options&lt;/strong&gt;: exercise is only permitted at expiry.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;American Options&lt;/strong&gt;: exercise is permitted at any time before expiry.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Bermudan Options&lt;/strong&gt;: exercise is permitted on specified dates / in specified periods.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Put-Call Parity&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;On day $t$, you buy an European call option with a strike of $E$ and an expiry of $T$ and write an European put option with the same values.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;You now hold a portfolio of a long call and a short put with the payoff of:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;\text{Today (at t): }\ \ \ \ \ \ \ C-P=S(t)-Ee^{-r(T-t)} \\
  &amp;\text{Future (at T):}\ \ \ \ \ \ \max{(S(T)-E,0)}-\max{(E-S(T),0)}=S(T)-E
  \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;Note that the first equation shows that: long call + short put = long asset + short cash&lt;/li&gt;
      &lt;li&gt;&lt;u&gt;Put-Call Parity&lt;/u&gt;: this equality of CF is independent of the future, meaning that it holds at any time up to expiry.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Binary/Digital Options&lt;/strong&gt;: options with a fixed payoff discontinuous in $S$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;: instead of getting the difference between the underlying $S$ and the strike $E$ as your payoff, you get a &lt;strong&gt;fixed&lt;/strong&gt; amount of payoff that is irrelevant to the value of the underlying.&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;Binary Call&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;Binary Put&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;&lt;img src=&quot;../../images/quant/binary_call.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;&lt;center&gt;&lt;img src=&quot;../../images/quant/binary_put.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;

        &lt;p&gt;&lt;br /&gt;
  As shown in the table, $E=100$.&lt;/p&gt;

        &lt;p&gt;For binary call, when $S&amp;gt;E$, you get $1 regardless of how big the difference is.&lt;/p&gt;

        &lt;p&gt;For binary put, when $S&amp;lt;E$, you get $1 regardless of how big the difference is.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Put-Call Parity&lt;/u&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{Binary call}+\text{Binary put}=E'e^{-r(T-t)}
  \end{equation}&lt;/script&gt;

        &lt;p&gt;where $E’$ represents the payoff value of the binary options.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Roadmap: Time Value of Money Forwards &amp;amp; Futures Options</summary></entry><entry><title type="html">Improvements on Neural Networks</title><link href="http://localhost:4000/DL/imp/" rel="alternate" type="text/html" title="Improvements on Neural Networks" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/DL/DL-imp</id><content type="html" xml:base="http://localhost:4000/DL/imp/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#split&quot;&gt;Train/Test Split&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#init&quot;&gt;Initialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fit&quot;&gt;Data Fitting&lt;/a&gt; with refined &lt;a href=&quot;#pro&quot;&gt;procedure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reg&quot;&gt;Regularization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Regularization on LogReg
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#L2&quot;&gt;L2 Regularization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#L1&quot;&gt;L1 Regularization&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nnreg&quot;&gt;Regularization on NN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dp&quot;&gt;Dropout&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#da&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#es&quot;&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#og&quot;&gt;Orthogonalization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#norm&quot;&gt;Feature Scaling (normalization)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gc&quot;&gt;Gradient Checking&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#op&quot;&gt;Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mbgd&quot;&gt;Mini-Batch Gradient Descent&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Gradient Descent with Momentum
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#ema&quot;&gt;Exponentially Weighted (Moving) Average&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#bc&quot;&gt;Bias Correction&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#m&quot;&gt;Momentum&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rmsprop&quot;&gt;RMSprop&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#adam&quot;&gt;Adam&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lrd&quot;&gt;Learning Rate Decay&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#po&quot;&gt;Problems with optimization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hpt&quot;&gt;Hyperparameter Tuning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bn&quot;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;split&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;traintest-split&quot;&gt;Train/Test Split&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset = training set + development/validation set + test set&lt;/li&gt;
  &lt;li&gt;Split ratio:
    &lt;ul&gt;
      &lt;li&gt;old era: 70/0/30%, 60/20/20%, …&lt;/li&gt;
      &lt;li&gt;big data era: 98/1/1%, 99.5/0.4/0.1%, 99.5/0.5/0%, … &lt;br /&gt;
  (trend: testset as small as possible)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;All 3 subsets should come from the exact same distribution (&lt;strike&gt;mismatch&lt;strike&gt;)&lt;/strike&gt;&lt;/strike&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;init&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;initialization&quot;&gt;Initialization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$W$ should be initialized with &lt;strong&gt;small random values&lt;/strong&gt; to break symmetry (to make sure that different hidden nodes can learn different things)&lt;/li&gt;
  &lt;li&gt;$b$ can be initialized to &lt;strong&gt;zeros&lt;/strong&gt; ($\because$ symmetry is still broken when $W$ is randomly initialized)&lt;/li&gt;
  &lt;li&gt;Different initializations $\rightarrow$ different results&lt;/li&gt;
  &lt;li&gt;Refer to &lt;a href=&quot;https://keras.io/initializers/&quot;&gt;keras documentation&lt;/a&gt; for initializers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;fit&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;data-fitting&quot;&gt;Data Fitting&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Underfitting&lt;/strong&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/uf.png&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Proper fitting&lt;/strong&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/nof.png&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/of.png&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Tradeoff: &lt;em&gt;train error&lt;/em&gt; vs &lt;em&gt;validation error&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;train err&lt;/em&gt; too small $\longrightarrow$ high variance (overfitting)&lt;/strong&gt;&lt;br /&gt;
(e.g. train err = 1%; val err = 11%)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;train err&lt;/em&gt; too big $\longrightarrow$ high bias (underfitting)&lt;/strong&gt;&lt;br /&gt;
(e.g. train err = 17%; val err = 16%)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;train err&lt;/em&gt; too big &amp;amp; &lt;em&gt;val err&lt;/em&gt; even bigger $\longrightarrow$ both probs&lt;/strong&gt; &lt;br /&gt;
(e.g. train err = 17%; val err = 34%)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;train err&lt;/em&gt; too small &amp;amp; &lt;em&gt;val err&lt;/em&gt; also small $\longrightarrow$ congratulations!&lt;/strong&gt;&lt;br /&gt;
(e.g. train err = 0.5%; val err = 1%)
 &lt;br /&gt;
 &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;pro&quot;&gt;&lt;/a&gt;
&lt;strong&gt;The Procedure&lt;/strong&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/fit.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt; &lt;a name=&quot;reg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;regularization&quot;&gt;Regularization&lt;/h2&gt;

&lt;p&gt;Idea: add a regularization term to the original loss function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}f(w)
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\lambda$: regularization parameter&lt;/li&gt;
  &lt;li&gt;$f(w)$: regularization on $w$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How does regularization prevent overfitting?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;set $\lambda$ as big as possible $\Rightarrow w^{[l]}\approx 0$ $\Rightarrow z^{[l]}\approx 0$ $\Rightarrow$ as if some hidden nodes don’t exist any more&lt;/li&gt;
  &lt;li&gt;$\Rightarrow$ less complexity $\Rightarrow$ variance $\downarrow$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;br /&gt;
&lt;strong&gt;Regularization on LogReg&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a name=&quot;L2&quot;&gt;&lt;/a&gt;&lt;strong&gt;L2 Regularization&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\|w\|^2_2 \\
\|w\|^2_2=\sum_{j=1}^{n_x}w_j^2=w^Tw
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a name=&quot;L1&quot;&gt;&lt;/a&gt;&lt;strong&gt;L1 Regularization&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\|w\|_1 \\
\|w\|_1=\sum_{j=1}^{n_x}{|w|}
\end{equation}&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;nnreg&quot;&gt;&lt;/a&gt;&lt;strong&gt;Regularization on NN&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathcal{J}(W^{[k]},b^{[k]})=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\sum_{l=1}^L{\|W^{[l]}\|^2_F}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Frobenius Norm:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\|W^{[l]}\|^2_F=\sum_{i=1}^{n_{l-1}}\sum_{j=1}^{n_l}{(w_{ij}^{[l]})^2}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Weight Decay on GD:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
W^{[l]}&amp;:=w^{[l]}-\alpha\cdot\frac{\partial{\mathcal{L}}}{\partial{W^{[l]}}} \\
&amp;=w^{[l]}-\alpha\cdot\Big(\frac{\partial{\mathcal{L}}}{\partial{W^{[l]}}}(\text{original})+\frac{\lambda}{m}W^{[l]}\Big)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt; &lt;br /&gt;
&lt;a name=&quot;dp&quot;&gt;&lt;/a&gt;&lt;strong&gt;Dropout&lt;/strong&gt;: each node has a probability to be kicked out of the NN ($\Rightarrow$ NN becomes smaller &amp;amp; simpler) [only used in training]&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Make a &lt;strong&gt;Boolean&lt;/strong&gt; matrix corresponding to the matrix of activation values:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 A^{[k]}&amp;=\begin{bmatrix}
 a_{11}^{[k]} &amp; \cdots &amp; a_{1m}^{[k]} \\
 \vdots &amp; \ddots &amp; \vdots \\ 
 a_{n_k1}^{[k]} &amp; \cdots &amp; a_{n_km}^{[k]}
 \end{bmatrix}\quad\quad\quad A^{[k]}\in\mathbb{R}^{n_k\times m} \\ \\
 B^{[k]}&amp;=\begin{bmatrix}
 b_{11}^{[k]} &amp; \cdots &amp; b_{1m}^{[k]} \\
 \vdots &amp; \ddots &amp; \vdots \\ 
 b_{n_k1}^{[k]} &amp; \cdots &amp; b_{n_km}^{[k]}
 \end{bmatrix}\quad\quad\quad B^{[k]}\in\mathbb{R}^{n_k\times m}
 \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;where $b_{ji}^{[k]}\in\{\text{True}, \text{False}\}$. The Boolean values are assigned randomly based on a keep-probability $p$ (can be chosen differently for diff layers).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiply both matrices element-wise:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 A^{[k]}=A^{[k]}* B^{[k]}
 \end{equation}&lt;/script&gt;

    &lt;p&gt;so that some activation values are now zero (they are kicked out of the neural network)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invert the matrix element-wise:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 A^{[k]}=A^{[k]}/p
 \end{equation}&lt;/script&gt;

    &lt;p&gt;to ensure consistency in activation values&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;da&quot;&gt;&lt;/a&gt;&lt;strong&gt;Data Augmentation&lt;/strong&gt;: modify the dataset to get more data (mostly used in Computer Vision) [Benefit: a very low-cost regularization]&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;flip picture&lt;/li&gt;
  &lt;li&gt;slight rotation&lt;/li&gt;
  &lt;li&gt;zoom in/out&lt;/li&gt;
  &lt;li&gt;distortions&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;da&quot;&gt;&lt;/a&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt;: stop the training iterations in the middle&lt;/p&gt;

&lt;p&gt;Why do we stop in the middle?&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/es.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The goal of our training is NOT to finish training BUT to find the optimal weight parameters that minimizes the cost/error.&lt;/p&gt;

&lt;p&gt;As shown in the figure, sometimes we should just stop in the middle with the minimal validation error instead of keeping the training going to get overfitting.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;og&quot;&gt;&lt;/a&gt;&lt;strong&gt;Orthogonalization&lt;/strong&gt;: implement controls that only affect &lt;strong&gt;ONE single component&lt;/strong&gt; of your algorithms performance at a time&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;norm&quot;&gt;&lt;/a&gt;&lt;strong&gt;Feature Scaling (normalization)&lt;/strong&gt;: normalize inputs for higher efficiency&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Set to zero mean:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mu&amp;=\frac{1}{m}\sum_{i=1}^{m}{x^{(i)}} \\
 x&amp;=x-\mu
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Normalize variance:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \sigma^2&amp;=\frac{1}{m}\sum_{i=1}^{m}{x^{(i)}\text{**}2}\quad\quad \text{**: element-wise squaring} \\
 x&amp;=x/\sigma^2
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;gc&quot;&gt;&lt;/a&gt;&lt;strong&gt;Gradient Checking&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Why?&lt;/p&gt;

    &lt;p&gt;Backprop is a very complex system of mathematical computations. It is very possible that there might be some miscalculation or bugs in these tremendous differentiations, even though the entire training appears as if it’s working properly.&lt;/p&gt;

    &lt;p&gt;Gradient Checking is the approach to prevent such issue by checking if each gradient is calculated properly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Equation&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \frac{\partial{\mathcal{J}}}{\partial{w}}=\lim_{\varepsilon\rightarrow 0}\frac{\mathcal{J}(w+\varepsilon)-\mathcal{J}(w-\varepsilon)}{2\varepsilon}\approx\frac{\mathcal{J}(w+\varepsilon)-\mathcal{J}(w-\varepsilon)}{2\varepsilon}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation: Calculate the difference between actual gradient and approximated gradient to see if the difference is reasonable:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{diff}=\frac{||g-g'||_ 2}{||g||_ 2+||g'||_ 2}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;op&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;
&lt;p&gt;&lt;a name=&quot;mbgd&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Mini-Batch Gradient Descent&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Why?&lt;/p&gt;

    &lt;p&gt;To allow faster and more efficient computing when there is a large number of training examples (e.g. $m=10000000$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation (see &lt;a href=&quot;../../DL/ANN/#gd&quot;&gt;gradient descent&lt;/a&gt; for more details)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}\sum_{i=1}^{m'}{(\hat{Y_i}-Y_i)^2} \\
  W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;BGD vs MBGD&lt;/li&gt;
    &lt;/ul&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/bgdvsmbgd.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;BGD vs SGD&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/bgdvssgd.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;
  BGD: large steps, low noise, too long per iteration&lt;br /&gt;
  SGD: small steps, insane noise, lose vectorization&lt;br /&gt;
  MBGD: in between $\rightarrow$ optimal in most cases&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Gradient Descent with Momentum&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;ema&quot;&gt;&lt;/a&gt;&lt;strong&gt;Exponentially Weighted (Moving) Average&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Intuition&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/ema.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;
  The blue dots represent the raw data points, while the red and green curves represent the two EMAs of the blue dots. As clearly indicated by the figure, EMA is used to reduce the huge oscillation of such time-series data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Formula&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  V_t=\beta V_{t-1}+(1-\beta)\theta_t
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$\theta_t$: the original time-series data point at time $t$&lt;/li&gt;
          &lt;li&gt;$V_t$: the EMA data point at time $t$&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$\beta$: an indicator of how many time units (e.g. days) this algorithm is approximately averaging over:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{#time units}=\frac{1}{1-\beta}
  \end{equation}&lt;/script&gt;

            &lt;p&gt;e.g. $\beta=0.9 \rightarrow$ average over 10 days; $\beta=0.96 \rightarrow$ average over 25 days&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Performance: easy computation + one-line code + memory efficiency&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;bc&quot;&gt;&lt;/a&gt;&lt;strong&gt;Bias Correction&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;Assume $\beta=0.99$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;V_0=0 \\
  &amp;V_1=0.99 V_0+0.01\theta_1=0.01\theta_1 \\
  &amp;V_2=0.99 V_2+0.01\theta_2=0.099\theta_1+0.01\theta_2 \\
  &amp;...
  \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;Notice that $V_1 \&amp;amp; V_2$ are very tiny portions of $\theta_1 \&amp;amp; \theta_2$, meaning that they do not accurately represent the actual data points.&lt;/p&gt;

        &lt;p&gt;Thus, it is necessary to rescale the early EMA values, with the following formula:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  V_t:=\frac{V_t}{1-\beta^t}
  \end{equation}&lt;/script&gt;

        &lt;p&gt;In the later calculations, bias correction is not so necessary.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;m&quot;&gt;&lt;/a&gt;&lt;strong&gt;Momentum&lt;/strong&gt;: application of EMA in GD&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Compute $dW,db$ on the current MB&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute EMA&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;V_{dW}:=\beta V_{dW}+(1-\beta)dW \\
 &amp;V_{db}:=\beta V_{db}+(1-\beta)db
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute GD&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;W:=W-\alpha V_{dW} \\
 &amp;b:=b-\alpha V_{db}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;$\beta$ is often chosen as $0.9$ in GD with Momentum.&lt;/p&gt;

    &lt;p&gt;Why named “momentum”? Think of $dW$ as acceleration, $V_{dW}$ as velocity, and $\beta$ as friction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performance&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/momentum.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

    &lt;p&gt;Red steps represent Momentum, while blue steps represent normal GD.&lt;/p&gt;

    &lt;p&gt;Slower learning vertically + Faster learning horizontally&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Momentum is always better than SGD&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;rmsprop&quot;&gt;&lt;/a&gt;&lt;strong&gt;RMSprop (Root Mean Square Propagation)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Intuition: a modified version of GD with Momentum&lt;/p&gt;

    &lt;p&gt;Why? To further minimize the oscillation of GD and maximize the speed of convergence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Steps:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Compute $dW,db$ on the current MB&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute RMS step&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;S_{dW}:=\beta S_{dW}+(1-\beta)dW^2 \\
 &amp;S_{db}:=\beta S_{db}+(1-\beta)db^2
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;where $dW^2=dW* dW$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute GD&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\varepsilon} \\
 &amp;b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\varepsilon}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;$\varepsilon$ is added to ensure $\text{denominator}\neq0$ (normally $\varepsilon=10^{-8}$)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;adam&quot;&gt;&lt;/a&gt;&lt;strong&gt;Adam&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Intuition: &lt;strong&gt;Momentum + RMSprop&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Steps:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Compute $dW,db$ on the current MB&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute Momentum:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;V_{dW}:=\beta_1 V_{dW}+(1-\beta_1)dW \\
 &amp;V_{db}:=\beta_1 V_{db}+(1-\beta_1)db
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;Compute RMSprop:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;S_{dW}:=\beta_2 S_{dW}+(1-\beta_2)dW^2 \\
 &amp;S_{db}:=\beta_2 S_{db}+(1-\beta_2)db^2
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Bias Correction:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;V_{dW}:=\frac{V_{dW}}{1-\beta_1^t}, V_{db}:=\frac{V_{db}}{1-\beta_1^t} \\
 &amp;S_{dW}:=\frac{S_{dW}}{1-\beta_2^t}, S_{db}:=\frac{S_{db}}{1-\beta_2^t}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute GD:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 &amp;W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}}+\varepsilon} \\
 &amp;b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}}+\varepsilon}
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;Hyperparameter choices:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\alpha$: depends&lt;/li&gt;
          &lt;li&gt;$\beta_1: 0.9$&lt;/li&gt;
          &lt;li&gt;$\beta_2: 0.999$&lt;/li&gt;
          &lt;li&gt;$\varepsilon: 10^{-8}$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;lrd&quot;&gt;&lt;/a&gt;&lt;strong&gt;Learning Rate Decay&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Intuition: as $\alpha$ slowly decreases, training steps become smaller $\rightarrow$ oscillating closely around the minimum (instead of jumping over the minimum)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main Method:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \alpha=\frac{1}{1+r_{\text{decay}}\cdot \text{#epoch}}\cdot\alpha_0
  \end{equation}&lt;/script&gt;

    &lt;p&gt;where 1 epoch means passing through data once.&lt;/p&gt;

    &lt;p&gt;Normally, $\alpha_0=0.2,r_{\text{decay}}=1$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other Methods:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Exponential Decay:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \alpha=0.95^{\text{#epoch}}\cdot\alpha_0
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Root Decay:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \alpha=\frac{k}{\sqrt{\text{#epoch}}}\cdot\alpha_0
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Discrete Staircase:&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/staircase.png&quot; width=&quot;150&quot; /&gt;&lt;/center&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Manual Decay&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;po&quot;&gt;&lt;/a&gt;&lt;strong&gt;Problems with optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As learnt in Calculus, no matter how we try to find the optimum, we always have problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Local Optima: we get stuck in local optima instead of moving to global optima&lt;/li&gt;
  &lt;li&gt;Saddle Points: we find GD=0 at saddle points before we find global optima&lt;/li&gt;
  &lt;li&gt;Plateau: long saddle that makes learning super slow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;hpt&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;hyperparameter-tuning&quot;&gt;Hyperparameter Tuning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: try to find the optimal hyperparameter for the NN&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;List of Hyperparameters&lt;/strong&gt; (in the order of priority)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tier 1: $\alpha$&lt;/li&gt;
  &lt;li&gt;Tier 2: #hidden units, MB size&lt;/li&gt;
  &lt;li&gt;Tier 3: #layers, $\alpha$ decay&lt;/li&gt;
  &lt;li&gt;Tier 4: $\beta_1$, $\beta_2$, $\varepsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Random Picking&lt;/strong&gt;: e.g. $n^{[l]}\in [50,100], L\in [2,4]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appropriate Scale&lt;/strong&gt;: e.g. $\alpha\in [0.0001,1]$ is obviously NOT an appropriate scale, because 90% of the values are in $[0.1,1]$.&lt;/p&gt;

&lt;p&gt;Instead, $\alpha\in[0.0001,1]_ {\text{log}}$ is an appropriate scales because the random picking is equally distributed on the log scale.&lt;/p&gt;

&lt;p&gt;e.g. for $\beta\in[0.9,0.999]$, the code implementation should be&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$r\in[-3,-1]$&lt;/li&gt;
  &lt;li&gt;$\beta=1-10^r$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;bn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: Feature scaling normalizes the inputs to speed up learning for the 1st layer. Similarly, can we normalize $a^{[l-1]}$ to train $W^{[l]} \&amp;amp; b^{[l]}$ faster? Obviously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate mean &amp;amp; variance&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mu&amp;=\frac{1}{m}\sum_{i=1}^{m}{z^{[l](i)}} \\
 \sigma^2&amp;=\frac{1}{m}\sum_{i=1}^{m}{(z^{[l](i)}-\mu)^2}
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Normalize Node Output:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 z_{\text{norm}}^{[l](i)}=\gamma\frac{z^{[l](i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}+\beta
 \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$\gamma\ \&amp;amp;\ \beta$ = learnable parameters&lt;/li&gt;
      &lt;li&gt;$\gamma\neq\sqrt{\sigma^2+\varepsilon}$ and $\beta\neq\mu$&lt;/li&gt;
      &lt;li&gt;Make sure to add $\gamma\ \&amp;amp;\ \beta$ to the dictionary of parameter updates during coding&lt;/li&gt;
      &lt;li&gt;Batch Normalization eliminates $b^{[l]}$ during $\mu$ calculation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Roadmap: Train/Test Split Initialization Data Fitting with refined procedure Regularization Regularization on LogReg L2 Regularization L1 Regularization Regularization on NN Dropout Data Augmentation Early Stopping Orthogonalization Feature Scaling (normalization) Gradient Checking Optimization Mini-Batch Gradient Descent Gradient Descent with Momentum Exponentially Weighted (Moving) Average Bias Correction Momentum RMSprop Adam Learning Rate Decay Problems with optimization Hyperparameter Tuning Batch Normalization   Train/Test Split</summary></entry><entry><title type="html">Generalized Linear Models</title><link href="http://localhost:4000/ML/GLM/" rel="alternate" type="text/html" title="Generalized Linear Models" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/ML/ML-GLM</id><content type="html" xml:base="http://localhost:4000/ML/GLM/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#bernoulli&quot;&gt;Bernoulli Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gaussian&quot;&gt;Gaussian Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#poisson&quot;&gt;Poisson Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gamma&quot;&gt;Gamma Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#beta&quot;&gt;Beta Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dirichlet&quot;&gt;Dirichlet Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#construct&quot;&gt;Method of Constructing GLMs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#softmax&quot;&gt;Softmax Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&quot;glm&quot;&gt;GLM&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What are &lt;strong&gt;GLM&lt;/strong&gt;s?&lt;/p&gt;

    &lt;p&gt;Remember the two models we had in the last post?&lt;br /&gt;
  Regression:      $p(y|x,w)\sim N(\mu,\sigma^2)$&lt;br /&gt;
  Classification:   $p(y|x,w)\sim \text{Bernoulli}(\phi)$&lt;/p&gt;

    &lt;p&gt;They belong to GLM, a collection of models that can be applied to Supervised Learning problems. We will show more examples of GLMs in this markdown.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Exponential Family&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(y,\eta)=b(y)\cdot e^{\eta^TT(y)-a(\eta)}
  \end{equation}&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta$: natural parameter (i.e. canonical parameter)&lt;/p&gt;

        &lt;p&gt; different $\eta \rightarrow$ different distributions within the family&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)$: sufficient statistic (usually, $T(y)=y$)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)$: log partition function&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$e^{-a(\eta)}$: normalization constant (to ensure that $\int{p(y,\eta)dy}=1$)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T,a,b$: fixed choice that defines a family of distributions parametrized by $\eta$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;bernoulli&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bernoulli Distribution&lt;/strong&gt; (Classification)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\phi)&amp;=\phi^y(1-\phi)^{1-y} \\
  &amp;=e^{y\log{\phi}+(1-y)\log{(1-\phi)}} \\
  &amp;=e^{y\log{\frac{\phi}{1-\phi}}+\log{(1-\phi)}} \\
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=\log{\frac{\phi}{1-\phi}}\Leftrightarrow \phi=\frac{1}{1+e^{-\eta}}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=y$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=\log{(1+e^\eta)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=1$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;gaussian&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gaussian Distribution&lt;/strong&gt; (Regression)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\mu,\sigma^2)&amp;=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(y-\mu)^2} \\
  &amp;=\frac{1}{\sqrt{2\pi}}e^{\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-\frac{1}{2\sigma^2}\mu^2-\log{\sigma}}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=\begin{bmatrix}
    \frac{\mu}{\sigma^2} ;
    \frac{-1}{2\sigma^2}
   \end{bmatrix}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=\begin{bmatrix}
    y;
    y^2
   \end{bmatrix}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=\frac{1}{2\sigma^2}\mu^2-\log{\sigma}=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log{(-2\eta_2)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=\frac{1}{\sqrt{2\pi}}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;poisson&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Poisson Distribution&lt;/strong&gt; (count-data)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\lambda)&amp;=\frac{\lambda^ye^{-\lambda}}{y!}\\
  &amp;=\frac{1}{y!}e^{y\log{\lambda}-\lambda}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=\log{\lambda}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=y$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=e^\eta$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=\frac{1}{y!}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;gamma&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gamma Distribution&lt;/strong&gt; (continuous non-negative random variables)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\lambda,a)&amp;=\frac{\lambda^ay^{a-1}e^{-\lambda y}}{\Gamma(a)}\\
  &amp;=\frac{y^{a-1}}{\Gamma(a)}e^{-\lambda y+a\log{\lambda}}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=-\lambda$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=y$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=-a\log{(-\eta)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=\frac{y^{a-1}}{\Gamma(a)}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;beta&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Beta Distribution&lt;/strong&gt; (distribution of probabilities)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\alpha,\beta)&amp;=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}y^{\alpha-1}(1-y)^{\beta-1} \\
  &amp;=\frac{(1-y)^\beta}{y(1-y)\Gamma(\beta)}e^{\alpha\log{y}- \log{\frac{\Gamma(\alpha)}{\Gamma(\alpha+\beta)}}} \\
  &amp;=\frac{y^\alpha}{y(1-y)\Gamma(\alpha)}e^{\beta\log{(1-y)}- \log{\frac{\Gamma(\beta)}{\Gamma(\alpha+\beta)}}}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=\alpha\ \text{or}\ \beta$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=\log{y}\ \text{or}\ \log{(1-y)}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=\log{\frac{\Gamma(\eta)}{\Gamma(\alpha+\beta)}}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=\frac{(1-y)^\beta}{y(1-y)\Gamma(\beta)}\ \text{or}\ \frac{y^\alpha}{y(1-y)\Gamma(\alpha)}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;dirichlet&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dirichlet Distribution&lt;/strong&gt; (multivariate beta)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y|\alpha)&amp;=\frac{\Gamma(\sum_k\alpha_k)}{\prod_k\Gamma(\alpha_k)}\prod_k{y_k^{\alpha_k-1}} \\
  &amp;=\exp{\big(\sum_k{(\alpha_k-1)\log{y_k}}-\big[\sum_k{\log{\Gamma(\alpha_k)}}-\log{\Gamma(\sum_k{\alpha_k})}\big]\big)}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=\alpha-1$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$T(y)=\log{y}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$a(\eta)=\sum_k{\log{\Gamma(\alpha_k)}}-\log{\Gamma(\sum_k{\alpha_k})}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$b(y)=1$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;construct&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;method-of-constructing-glms&quot;&gt;Method of Constructing GLMs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;3 Assumptions&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$y|x,w \sim \text{ExponentialFamily}(\eta)$&lt;/p&gt;

        &lt;p&gt;$y$ given $x\&amp;amp;w$ follows some exponential family distribution with natural parameter $\eta$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$h(x)=\text{E}[y|x]$&lt;/p&gt;

        &lt;p&gt;Our hypothetical model $h(x)$ should predict the expected value of $y$ given $x$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\eta=w^Tx$&lt;/p&gt;

        &lt;p&gt;$\eta$ is linearly related to $x$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example 1: OLS (Ordinary Least Squares) (i.e. LinReg)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  h(x)&amp;=\text{E}[y\|x,w]\ \ \ \ \ \ &amp;\text{(Assumption 2)} \\
     &amp;=\mu \\
     &amp;=\eta\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp;\text{(Assumption 1)} \\
     &amp;=w^Tx\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp;\text{(Assumption 3)}
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example 2: Logistic Regression&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  h(x)&amp;=\text{E}[y\|x,w]\ \ \ \ \ \ &amp;\text{(Assumption 2)} \\
     &amp;=\phi \\
     &amp;=\frac{1}{1+e^{-\eta}}\ \ \ \ \ \ &amp;\text{(Assumption 1)} \\
     &amp;=\frac{1}{1+e^{-w^Tx}}\ \ \ \ \ \ &amp;\text{(Assumption 3)}
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example 3: &lt;a name=&quot;softmax&quot;&gt;&lt;/a&gt;&lt;strong&gt;Softmax Regression&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;Softmax is a method used in &lt;strong&gt;multiclass classification&lt;/strong&gt; to select one output value $\phi_i$ of the highest probability among all the output values.&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \hat{y}=\begin{bmatrix}
 \phi_1 \\
 \vdots \\
 \phi_{k-1}
 \end{bmatrix}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;One-hot Encoding&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 y\in \{ 1,\cdots,k \} \Rightarrow T(y)\in \mathbb{R}^{k}
 \end{equation}&lt;/script&gt;

        &lt;p&gt;where&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 T(1)=\begin{bmatrix}
 1 \\ 0 \\ \vdots \\ 0
 \end{bmatrix},
 T(2)=\begin{bmatrix}
 0 \\ 1 \\ \vdots \\ 0
 \end{bmatrix},\cdots,
 T(k)=\begin{bmatrix}
 0 \\ 0 \\ \vdots \\ 1
 \end{bmatrix}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Dummy Encoding&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 y\in \{ 1,\cdots,k \} \Rightarrow T(y)\in \mathbb{R}^{k-1}
 \end{equation}&lt;/script&gt;

        &lt;p&gt;where&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 T(1)=\begin{bmatrix}
 1 \\ 0 \\ \vdots \\ 0
 \end{bmatrix},
 T(2)=\begin{bmatrix}
 0 \\ 1 \\ \vdots \\ 0
 \end{bmatrix},\cdots,
 T(k-1)=\begin{bmatrix}
 0 \\ 0 \\ \vdots \\ 1
 \end{bmatrix},
 T(k)=\begin{bmatrix}
 0 \\ 0 \\ \vdots \\ 0
 \end{bmatrix}
 \end{equation}&lt;/script&gt;

        &lt;p&gt;Why Dummy Encoding &amp;gt; One-hot Encoding? It reduces 1 entire column!&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Indicator Function&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \text{I}\{ \text{True} \}=1,\ \text{I}\{ \text{False} \}=0
 \end{equation}&lt;/script&gt;

        &lt;p&gt;Therefore,&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 T(y)_i =\text{I}\{ y=i \}
 \end{equation}&lt;/script&gt;

        &lt;p&gt;Therefore,&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \text{E}[T(y)_i] =P(y=i)=\phi_i
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Exponential Family form&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 p(y|\phi)&amp;=\prod_{i=1}^{k}{\phi_i^{\text{I}\{ y=i \}}} \\
 &amp;=\prod_{i=1}^{k-1}{\phi_i^{T(y)_i}} \cdot \phi_k^{1-\sum_{i=1}^{k-1}{T(y)_i}} \\
 &amp;=\exp{\big(\sum_{i=1}^{k-1}{T(y)_i\log{(\phi_i)}-\sum_{i=1}^{k-1}{T(y)_i}\log{(\phi_k)}}+\log{(\phi_k)}\big)} \\
 &amp;=\exp{\big(\sum_{i=1}^{k-1}{T(y)_i\log{\big(\frac{\phi_i}{\phi_k}\big)}+\log{(\phi_k)}\big)}} \\
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;$\eta=\begin{bmatrix}\log{\big(\frac{\phi_1}{\phi_k}\big)}\ ;\ \cdots\ ;\ \log{\big(\frac{\phi_{k-1}}{\phi_k}\big)}\end{bmatrix}$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$T(y)=\begin{bmatrix}T(y)_1\ ;\ \cdots\ ;\ T(y)_k-1\end{bmatrix}$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$a(\eta)=-\log{(\phi_k)}$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$b(y)=1$&lt;br /&gt;
  &lt;/p&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt; (derived from $\eta_i=\log{\big(\frac{\phi_i}{\phi_k}\big)}$)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \phi_i=\frac{e^{\eta_i}}{\sum_{j=1}^k{e^{\eta_j}}}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Probabilistic Interpretation of Softmax Regression&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 p(y=i|x,w)=\frac{e^{w_i^Tx}}{\sum_{j=1}^k{e^{w_i^Tx}}}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Log Likelihood&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 l(w)&amp;=\sum_{i=1}^m{\log{p(y^{(i)}|x^{(i)},w)}} \\
 &amp;=\sum_{i=1}^m{\log{\prod_{i=1}^{k}{\Bigg(\frac{e^{w_l^Tx^{(i)}}}{\sum_{j=1}^k{e^{w_l^Tx^{(i)}}}}\Bigg)^{\text{I}\{ y^{(i)}=l \}}}}}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Roadmap: Bernoulli Distribution Gaussian Distribution Poisson Distribution Gamma Distribution Beta Distribution Dirichlet Distribution Method of Constructing GLMs Softmax Regression</summary></entry><entry><title type="html">Basics of Artificial Neural Networks</title><link href="http://localhost:4000/DL/ANN/" rel="alternate" type="text/html" title="Basics of Artificial Neural Networks" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/DL/DL-Basics</id><content type="html" xml:base="http://localhost:4000/DL/ANN/">&lt;p&gt;Claim:&lt;br /&gt;
The dimensions used in this notebook are of my personal preference. They can be modified in whatever way you want as long as they are in consistency.&lt;/p&gt;

&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#nn&quot;&gt;Neural Network Representation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#af&quot;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Training
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fp&quot;&gt;Forward Propagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bp&quot;&gt;Backward Propagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Example: &lt;a href=&quot;#fbss&quot;&gt;Forward &amp;amp; Backward Step: Stochastic&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Example: &lt;a href=&quot;#fbsb&quot;&gt;Forward &amp;amp; Backward Step: Mini-batch&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rd&quot;&gt;Reverse Differentiation&lt;/a&gt; (for a clearer understanding of backpropagation)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gd&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;nn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;neural-network-representation&quot;&gt;Neural Network Representation&lt;/h2&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/NN.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Input Matrix&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
X=\begin{bmatrix}
x_1^{(1)} &amp; \cdots &amp; x_1^{(m)} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{n_x}^{(1)} &amp; \cdots &amp; x_{n_x}^{(m)}
\end{bmatrix}=\begin{bmatrix}
x^{(1)} &amp; \cdots &amp; x^{(m)}
\end{bmatrix}\quad\quad\quad X\in\mathbb{R}^{n_x\times m}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$x_j^{(i)}$: the $j$th feature of the $i$th training example&lt;/li&gt;
  &lt;li&gt;$m$: # training examples: each column vector of $x$ represents one training example&lt;/li&gt;
  &lt;li&gt;$n_x$: # input features: each row vector of $x$ represents one type of input feature&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;for easier understanding in this session, we use one training example / input vector at each training step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
x^{(i)}=\begin{bmatrix}
x_1^{(i)} \\ \vdots \\ x_{n_x}^{(i)}
\end{bmatrix}\quad\quad\quad x^{(i)}\in\mathbb{R}^{n_x}
\end{equation}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Output Vector&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\hat{Y}=\begin{bmatrix}
\hat{y}^{(1)} &amp; \cdots &amp; \hat{y}^{(m)}
\end{bmatrix}\quad\quad\quad \hat{Y}\in\mathbb{R}^{m}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\hat{y}^{(i)}$: the predicted output value of the $i$th training example&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the “$\hat{}$” symbol.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Weight Matrix&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
W^{[k]}=\begin{bmatrix}
w_{1,1}^{[k]} &amp; \cdots &amp; w_{1,n_{k-1}}^{[k]} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{n_k,1}^{[k]} &amp; \cdots &amp; w_{n_k,n_{k-1}}^{[k]}
\end{bmatrix}=\begin{bmatrix}
w_1^{[k]} \\ \vdots \\ w_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad W^{[k]}\in\mathbb{R}^{n_k\times n_{k-1}}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$w_{j,l}^{[k]}$: the weight value for the $l$th input at the $j$th node on the $k$th layer&lt;/li&gt;
  &lt;li&gt;$n_k$: # nodes/neurons on the $k$th layer (the current layer)&lt;/li&gt;
  &lt;li&gt;$n_{k-1}$: # nodes/neurons on the $k-1$th layer (the previous layer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bias Vector&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
b^{[k]}=\begin{bmatrix}
b_1^{[k]} \\ \vdots \\ b_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad b^{[k]}\in\mathbb{R}^{n_k}
\end{equation}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Linear Combination&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
z_j^{[k]}=w_j^{[k]}\cdot a^{[k-1]}+b_j^{[k]} \quad\quad\quad z_j^{[k]}\in\mathbb{R}^{n_k}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$z_j^{[k]}$: the unactivated output value from the $j$th node of the $k$th layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Activation&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
a^{[k]}=\begin{bmatrix}
a_1^{[k]} \\ \vdots \\ a_{n_k}^{[k]}
\end{bmatrix}=\begin{bmatrix}
g(z_1^{[k]}) \\ \vdots \\ g(z_{n_k}^{[k]})
\end{bmatrix}\quad\quad\quad a^{[k]}\in\mathbb{R}^{n_k}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$g(z)$: Activation function (to add &lt;strong&gt;nonlinearity&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;af&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h2&gt;

&lt;p&gt;(Blame github pages for not supporting colspan/rowspan)&lt;/p&gt;
&lt;table&gt;
    &lt;thead&gt;
        &lt;tr style=&quot;text-align: center&quot;&gt;
            &lt;th&gt;Sigmoid&lt;/th&gt;
            &lt;th&gt;Tanh&lt;/th&gt;
            &lt;th&gt;ReLU&lt;/th&gt;
            &lt;th&gt;Leaky ReLU&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody style=&quot;text-align: center&quot;&gt;
        &lt;tr&gt;
            &lt;td&gt;$g(z)=\frac{1}{1+e^{-z}}$&lt;/td&gt;
            &lt;td&gt;$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$&lt;/td&gt;
            &lt;td&gt;$g(z)=\max{(0,z)}$&lt;/td&gt;
            &lt;td&gt;$g(z)=\max{(\varepsilon z,z)}$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;img src=&quot;../../images/DL/sigmoid.png&quot; width=&quot;100&quot; /&gt;&lt;/td&gt;
            &lt;td&gt;&lt;img src=&quot;../../images/DL/tanh.png&quot; width=&quot;100&quot; /&gt;&lt;/td&gt;
            &lt;td&gt;&lt;img src=&quot;../../images/DL/relu.png&quot; width=&quot;100&quot; /&gt;&lt;/td&gt;
            &lt;td&gt;&lt;img src=&quot;../../images/DL/leakyrelu.png&quot; width=&quot;100&quot; /&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;small&gt;$g'(z)=g(z)\cdot (1-g(z))$&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;$g'(z)=1-(g(z))^2$&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;$$g'(z)=\begin{cases} 0&amp;amp;z&amp;lt;0 \\ 1&amp;amp;z&amp;gt;0\end{cases}$$&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;$$g'(z)=\begin{cases} \varepsilon&amp;amp;z&amp;lt;0 \\ 1&amp;amp;z&amp;gt;0\end{cases}$$&lt;/small&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;small&gt;centered at $y=0.5$&lt;br /&gt;$\Rightarrow$only good for binary classification&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;centered at $y=0$&lt;br /&gt;$\Rightarrow$better than sigmoid in many cases&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;faster computing&lt;br /&gt;&lt;strike&gt;vanishing gradient&lt;/strike&gt;&lt;br /&gt;model sparsity (some neurons can be inactivated)&lt;/small&gt;&lt;/td&gt;
            &lt;td&gt;&lt;small&gt;faster computing&lt;br /&gt;&lt;strike&gt;vanishing gradient&lt;/strike&gt;&lt;br /&gt;model sparsity (some neurons can be inactivated)&lt;/small&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;$|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0$&lt;br /&gt;$\Rightarrow$ vanishing gradient&lt;/td&gt;
            &lt;td&gt;$|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0$&lt;br /&gt;$\Rightarrow$ vanishing gradient&lt;/td&gt;
            &lt;td&gt;too many neurons get inactivated&lt;br /&gt;$\Rightarrow$dying ReLU&lt;/td&gt;
            &lt;td&gt;$\varepsilon$ usually set to 0.01&lt;br /&gt;&lt;strike&gt;dying ReLU&lt;/strike&gt;&lt;br /&gt;widely used on Kaggle&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Why need activation funcs? To add nonlinearity.
    &lt;ol&gt;
      &lt;li&gt;Suppose $g(z)=z$ (i.e. $\nexists g(z)$)&lt;/li&gt;
      &lt;li&gt;$\Longrightarrow z^{[1]}=w^{[1]}x+b^{[1]}$&lt;/li&gt;
      &lt;li&gt;$\Longrightarrow z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})=w’x+b’$&lt;/li&gt;
      &lt;li&gt;This is just linear regression. Hidden layers exist for no reason.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;&lt;a name=&quot;fp&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Forward Propagation&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/fp.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;a name=&quot;bp&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Backward Propagation&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/bp.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;a name=&quot;fbss&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Example: Forward &amp;amp; Backward Step: Stochastic&lt;/strong&gt;: 2 nodes &amp;amp; 3 inputs &amp;amp; no bias&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward Step:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}=\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Backward Step:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} &amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1}}x_1 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_2 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_3 \\
\frac{\partial{\mathcal{L}}}{\partial{z_2}}x_1 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_2 &amp; \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_3
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}x^T
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;fbsb&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Example: Forward &amp;amp; Backward Step: Mini-batch&lt;/strong&gt;: 2 nodes &amp;amp; 3 inputs &amp;amp; bias &amp;amp; 2 training examples&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward Step:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1^{(1)} &amp; x_1^{(2)} \\ 
x_2^{(1)} &amp; x_2^{(2)} \\ 
x_3^{(1)} &amp; x_3^{(2)}
\end{bmatrix}+\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}=\begin{bmatrix}
z_1^{(1)} &amp; z_1^{(2)} \\
z_2^{(1)} &amp; z_2^{(2)}
\end{bmatrix}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Backward Step:&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;small&gt;$$\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_1^{(2)} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_2^{(2)} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_3^{(2)} \\
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_1^{(2)} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_2^{(2)} &amp;amp; \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_3^{(2)} \\
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}X^T
\end{equation}$$&lt;/small&gt;&lt;/center&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{b}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{b_1}} \\ \frac{\partial{\mathcal{L}}}{\partial{b_2}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}} \\ 
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}
\end{bmatrix}=\sum_{i=1}^{2}{\frac{\partial{\mathcal{L}}}{\partial{z^{(i)}}}}
\end{equation}&lt;/script&gt;

&lt;p&gt;&lt;a name=&quot;rd&quot;&gt;&lt;/a&gt;
&lt;strong&gt;Reverse Differentiation&lt;/strong&gt;: a simple procedure summarized for a clearer understanding of backprop from Node A to Node B:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Find one single path of “A$\rightarrow$B”&lt;/li&gt;
  &lt;li&gt;Multiply all edge derivatives&lt;/li&gt;
  &lt;li&gt;Add the multiple to the overall derivative&lt;/li&gt;
  &lt;li&gt;Repeat 1-3&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;e.g.&lt;br /&gt;
Path 1:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/rd1.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Path 2:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/rd2.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Path 3:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/rd3.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;p&gt;And so on ……&lt;/p&gt;
&lt;center&gt;&lt;strong&gt;&lt;i&gt;Reverse Differentiation $\times$ Backward Step = Backward Propagation&lt;/i&gt;&lt;/strong&gt;&lt;/center&gt;

&lt;p&gt; &lt;a name=&quot;gd&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
W := W-\alpha\frac{\partial\mathcal{L}}{\partial W}
\end{equation}&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stochastic GD&lt;/strong&gt; (using 1 training example for each GD step)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}(\hat{Y_i}-Y_i)^2 \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mini-batch GD&lt;/strong&gt; (using mini-batches of size $m’\ (\text{s.t.}\ m=km’, k\in Z)$ for each GD step)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}\sum_{i=1}^{m'}{(\hat{Y_i}-Y_i)^2} \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Batch GD&lt;/strong&gt; (using the whole training set for each GD step)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathcal{L}(\hat{Y},Y)&amp;=\frac{1}{2}\sum_{i=1}^{m}{(\hat{Y_i}-Y_i)^2} \\
 W&amp;=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
 \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Claim: The dimensions used in this notebook are of my personal preference. They can be modified in whatever way you want as long as they are in consistency.</summary></entry><entry><title type="html">Classification</title><link href="http://localhost:4000/ML/class/" rel="alternate" type="text/html" title="Classification" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/ML/ML-class</id><content type="html" xml:base="http://localhost:4000/ML/class/">&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#logreg&quot;&gt;Logistic Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#learn&quot;&gt;Learning&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#gd&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#newton&quot;&gt;Newton’s Method&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#normal&quot;&gt;Normal Equation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#code&quot;&gt;Code Template&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#knn&quot;&gt;k-Nearest Neighbors&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#statsclass&quot;&gt;Statistical Setting for Classification&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gda&quot;&gt;Gaussian Discriminant Analysis&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes&quot;&gt;2 Types of Learning Algorithms &amp;amp; Bayes Theorem&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nb&quot;&gt;Naive Bayes Classifier&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#laplace&quot;&gt;Laplace Smoothing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;logreg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Problem Setting&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Observed pairs $(x,y)$, where $x\in\mathcal{X}$ &amp;amp; $y\in\mathcal{Y}$
        &lt;ul&gt;
          &lt;li&gt;$\mathcal{Y}=\{-1,+1\}\lor\{0,1\}$: binary classification&lt;/li&gt;
          &lt;li&gt;$\mathcal{Y}=\{1,…,K\}$: multiclass classification&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Find a classifier $f$ that can map input $x$ to class $y$: $y=f(x):\ “x\in\mathcal{X}”\rightarrow\ “y\in\mathcal{Y}”$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \hat{y}=g(w^Tx)
  \end{equation}&lt;/script&gt;

    &lt;p&gt;$g(z)$: a function that converts $w^Tx$ to binary value&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sigmoid Function (see Deep Learning for more funcs)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  g(z)=\sigma(z)=\frac{1}{1+e^{-z}}
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Derivative (you will know why we need this in Deep Learning)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  g'(z)&amp;=\frac{d}{dz}\frac{1}{1+e^{-z}} \\
  &amp;=\frac{e^{-z}(+1-1)}{(1+e^{-z})^2} \\
  &amp;=g(z)(1-g(z))
  \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cost Function&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;single training example (derivation later)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \mathcal{L}(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log{(1-\hat{y})})
 \end{equation}&lt;/script&gt;

        &lt;p&gt;If $y=1\rightarrow\mathcal{L}(\hat{y},y)=-\log{\hat{y}}\rightarrow$ want “$\mathcal{L}\downarrow\leftrightarrow\hat{y}\uparrow$”$\rightarrow\hat{y}=1$ &lt;br /&gt;
 If $y=0\rightarrow\mathcal{L}(\hat{y},y)=-\log{(1-\hat{y})}\rightarrow$ want “$\mathcal{L}\downarrow\leftrightarrow\hat{y}\downarrow$”$\rightarrow\hat{y}=0$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;entire training set&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \mathcal{J}(w)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)},y^{(i)})=\text{mean}(\mathcal{L})
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probabilistic Interpretation&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;Assumptions&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 P(y=1|x,w)&amp;=\hat{y} \\
 P(y=0|x,w)&amp;=1-\hat{y}
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Probabilistic Model of LogReg&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 p(y|x,w)=\hat{y}^y(1-\hat{y})^{1-y}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Likelihood Function&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 L(w)=\prod_{i=1}^{m}(\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Log Likelihood&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 l(w)&amp;=\sum_{i=1}^{m}(y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}) \\
 l(w)&amp;=-\sum_{i=1}^{m}\mathcal{L}(\hat{y},y)
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;MLE&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \frac{\partial l(w)}{\partial w_j}&amp;=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})\frac{\partial g(w^Tx)}{\partial w_j} \\
 &amp;=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})g(w^Tx)(1-g(w^Tx))\frac{\partial(w^Tx)}{\partial w_j} \\
 &amp;=(y(1-g(w^Tx))-(1-y)g(w^Tx))x_j \\
 &amp;=(y-\hat{y})x_j
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient Descent&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  w_j &amp;:= w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j} \\
  &amp;=w_j+\alpha(y-\hat{y})x_j
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;Why is it also called “Gradient Ascent”?&lt;br /&gt;
  $\because$ we are trying to minimize the loss function $\Leftrightarrow$ maximize the likelihood function&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;knn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;k-nearest-neighbors&quot;&gt;k-Nearest Neighbors&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;knnalg&quot;&gt;&lt;/a&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;For a new input $x$,&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Return the $k$ points &lt;strong&gt;closest&lt;/strong&gt; to $x$, indexed as $x_{i_1},…,x_{i_k}$.&lt;/li&gt;
      &lt;li&gt;Return the majority votes of $y_{i_1},…,y_{i_k}$.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Distances&lt;/strong&gt; (how to measure “closest”)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Euclidean distance&lt;/strong&gt;: default measurement&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \|u-v\|_ 2=\Big(\sum_{i=1}^n(u_i-v_i)^2\Big)^{\frac{1}{2}}
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;$l_p$&lt;/strong&gt;: variation on Euclidean&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \|u-v\|_ p=\Big(\sum_{i=1}^n|u_i-v_i|^p\Big)^{\frac{1}{p}}\ \ \ |\ p\in[1,\infty]
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Edit distance&lt;/strong&gt;: for strings&lt;/p&gt;

        &lt;center&gt;#modifications required to transform one string to the other&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Correlation distance&lt;/strong&gt;: for signals&lt;/p&gt;

        &lt;center&gt;how correlated 2 vectors are for signal detection&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;$k$&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Smaller $k$ $\Rightarrow$ smaller training error but could lead to overfitting&lt;/li&gt;
      &lt;li&gt;Larger $k$ $\Rightarrow$ more stable predictions due to voting&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Statistical Setting for Classification&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Prediction accuracy: $P(f(x)=y)$&lt;/li&gt;
          &lt;li&gt;Prediction error: $P(f(x)\neq y)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Key Assumption for Supervised Learning&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  (x_i,y_i)\sim\mathcal{P}\ \ \ |\ \ \ i=1,\cdots,n
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;i.i.d. (independent &amp;amp; identically distributed)&lt;/li&gt;
          &lt;li&gt;We assume that the future should look like the past.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;gda&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;gaussian-discriminant-analysis&quot;&gt;Gaussian Discriminant Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;bayes&quot;&gt;&lt;/a&gt;Learning Algorithms&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Discriminative Learning Algorithms&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{model }p(y|x)\text{ directly}\ \ \ (X \Rightarrow Y)
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;Generative Learning Algorithms&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
  \text{model }p(x|y)\ \&amp;\ p(y)\Rightarrow\text{ use Bayes Theorem to get }p(y|x) 
  \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bayes Theorem&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(y|x)=\frac{p(x|y)p(y)}{p(x)}
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt;:   $p(y)$&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Posterior&lt;/strong&gt;: $p(y|x)$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Simplification:&lt;/p&gt;

        &lt;p&gt;$\because$ we are trying to find the output $y$ with the highest probability given $x$&lt;br /&gt;
  $\therefore$ we can simplify Bayes Theorem for our purpose:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathop{\arg\max}_ {y}{p(y|x)}&amp;=\mathop{\arg\max}_ {y}{\frac{p(x|y)p(y)}{p(x)}} \\
  &amp;=\mathop{\arg\max}_ {y}{p(x|y)p(y)}
  \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;Bayes Theorem = the core of Generative Learning Algorithms&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assumption: Multivariate Gaussian Distribution&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\big)}
  \end{equation}&lt;/script&gt;

    &lt;p&gt;It is literally the same as Gaussian Distribution but with vector parameters:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;mean vector:    $\mu\in\mathbb{R}^n$&lt;/li&gt;
      &lt;li&gt;covariance matrix: $\Sigma\in\mathbb{R}^{n\times n}$&lt;br /&gt;
   &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;As a reminder and a comparison, here is the univariate version:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  y&amp;\sim \text{Bernoulli}{(\phi)} \\
  x|y=0&amp;\sim N(\mu_0,\Sigma) \\
  x|y=1&amp;\sim N(\mu_1,\Sigma) \\
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probabilistic Interpretation&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y)&amp;=\phi^y(1-\phi)^{1-y} \\
  p(x|y=0)&amp;=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\big)} \\
  p(x|y=1)&amp;=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\big)}
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;log likelihood&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  l(\phi,\mu_0,\mu_1,\Sigma)=\log{\prod_{i=1}^{m}{p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)}}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MLE&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \phi &amp;= \frac{1}{m}\sum_{i=1}^m{\text{I}\{ y^{(i)}=l \}} \\
  \mu_0 &amp;= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}}} \\
  \mu_1 &amp;= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}}} \\
  \Sigma &amp;= \frac{1}{m}\sum_{i=1}^m{(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T}
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GDA vs LogReg&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;GDA
        &lt;ul&gt;
          &lt;li&gt;makes &lt;strong&gt;stronger&lt;/strong&gt; modeling assumptions about data&lt;/li&gt;
          &lt;li&gt;data efficient when assumptions (Gaussian distributions) are approximately correct&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;LogReg
        &lt;ul&gt;
          &lt;li&gt;makes &lt;strong&gt;weaker&lt;/strong&gt; modeling assumptions about data&lt;/li&gt;
          &lt;li&gt;data efficient when assumptions (Gaussian distributions) are not necessarily correct (e.g. $x|y\sim \text{Poisson}(\lambda_1)$ instead of $N(\mu_0,\Sigma)$)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;nb&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;naive-bayes-classifier&quot;&gt;Naive Bayes Classifier&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GDA vs NB&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;GDA: $x$ = continuous, real-valued vectors&lt;/li&gt;
      &lt;li&gt;NB:   $x$ = discrete-valued vectors (e.g. text classification)&lt;br /&gt;
   &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Text Encoding (more in DL/RNN)&lt;/p&gt;

    &lt;p&gt;We encode a text sentence into a vector of the same length as our &lt;strong&gt;dictionary&lt;/strong&gt; (like a Python dictionary with vocabulary and their indices as key-value pairs):&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  x=\begin{bmatrix}
  0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 1 \\ 1 \\ \vdots \\ 0
  \end{bmatrix}
  \begin{matrix}
  \text{a} \\ \text{abandon} \\ \vdots \\ \text{pewdiepie} \\ \vdots \\ \text{subscribe} \\ \text{to} \\ \vdots \\ \text{zuck}
  \end{matrix}
  \end{equation}&lt;/script&gt;

    &lt;p&gt;The original sentence was “Subscribe to Pewdiepie!”, and this text encoding method uses lowercases, throws punctuations and ignores the order of the sentence. This is convenient in some cases (e.g. spam email classification) but awful in the other cases (e.g. news/report-writer bots)&lt;/p&gt;

    &lt;p&gt;Notice that $x\in \{0,1\}^{\text{len(dict)}}$. Why notice this? Because we now have $2^\text{len(dict)}$ possible outcomes for $x$. When we have a dictionary of over 20000 words, we have a $(2^{20000}-1)$-dimensional parameter vector. Have fun with that, laptop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assumption: Conditional Independence&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(x_i|y)=p(x_i|y,x_j)\ \ \ \forall j\neq i
  \end{equation}&lt;/script&gt;

    &lt;p&gt;meaning: Given $y$ as the condition, $x_i$ is independent of $x_j$.&lt;/p&gt;

    &lt;p&gt;In the case of spam email classification, if we know that the email is spam, then whether or not “pewdiepie” is in the sentence does not change our belief of whether or not “subscribe” is in the sentence.&lt;/p&gt;

    &lt;p&gt;Therefore, we can simplify our $p(x|y)$ into:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  p(x_1,...,x_{\text{len(dict)}}|y)=\prod_{i=1}^{n}{p(x_i|y)}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \phi_{i|y=1}&amp;=p(x_i=1|y=1) \\
  \phi_{i|y=0}&amp;=p(x_i=1|y=0) \\
  \phi_y&amp;=p(y=1)
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Joint Likelihood&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{L}(\phi_y,\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^{m}{p(x^{(i)},y^{(i)})}
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MLE&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \phi_{j|y=1}&amp;=\frac{\sum_{i=1}^m{I\{x_j^{(i)}=1\land y^{(i)}=1\}}}{\sum_{i=1}^m{I\{y^{(i)}=1\}}} \\
  \phi_{j|y=0}&amp;=\frac{\sum_{i=1}^m{I\{x_j^{(i)}=1\land y^{(i)}=0\}}}{\sum_{i=1}^m{I\{y^{(i)}=0\}}} \\
  \phi_y&amp;=\frac{\sum_{i=1}^m{I\{y^{(i)}=1\}}}{m}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;Quite intuitive. For example, $\phi_{j|y=0}$ = the fraction of non-spam emails with the word $j$ in it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prediction&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  p(y=1|x_\text{new})&amp;=\frac{p(x_\text{new}|y=1)p(y=1)}{p(x_\text{new})} \\
  &amp;=\frac{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)}{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)+\prod_{i=1}^n{p(x_i|y=0)}\cdot p(y=0)}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;Again, the formula is tedious but very intuitive. The $y$ with the higher posterior probability will be chosen as the final prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Apply NB in GDA cases?&lt;/p&gt;

    &lt;p&gt;Discretize: Just cut the continuous, real-valued $x$ into small intervals and label them with a discrete-valued scale.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;laplace&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;laplace-smoothing&quot;&gt;&lt;strong&gt;Laplace Smoothing&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: What if there is a new word “mrbeast” in the email for prediction that our NB classifier has never learnt ever since it was born?&lt;/p&gt;

&lt;p&gt;A human would look it up on a dictionary, and so would our NB classifier.&lt;/p&gt;

&lt;p&gt;Assume the word “mrbeast” is the 1234th word in the dictionary, then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\phi_{1234|y=1}&amp;=\frac{\sum_{i=1}^m{I\{x_{1234}^{(i)}=1\land y^{(i)}=1\}}}{\sum_{i=1}^m{I\{y^{(i)}=1\}}}=0 \\
\phi_{1234|y=0}&amp;=\frac{\sum_{i=1}^m{I\{x_{1234}^{(i)}=1\land y^{(i)}=0\}}}{\sum_{i=1}^m{I\{y^{(i)}=0\}}}=0 \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Yes. NB thinks that the probability of seeing this word in either spam or non-spam email is $0$, and therefore it would predict that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(y=1|x_\text{new})&amp;=\frac{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)}{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)+\prod_{i=1}^n{p(x_i|y=0)}\cdot p(y=0)} \\
&amp;=\frac{0}{0}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because both numerator and denominator contains $p(x_{1234|y})=0$.&lt;/p&gt;

&lt;p&gt;In summary, during prediction, if NB has never learnt a word $j$, there will always $\phi_j=0$ ruining the entire prediction. How do we estimate the unknown?&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\phi_j=\frac{\sum_{i=1}^m{I\{z^{(i)}=j\}}+1}{m+k}
\end{equation}&lt;/script&gt;

&lt;p&gt;where $k=\text{#features}$ if you forget.&lt;/p&gt;

&lt;p&gt;Let’s check if it still satisfies our condition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\sum_{j=1}^k{\phi_j}=\sum_{j=1}^k{\frac{\sum_{i=1}^m{I\{z^{(i)}=j\}}+1}{m+k}}=\frac{m+k}{m+k}=1
\end{equation}&lt;/script&gt;

&lt;p&gt;Nice. It still satisfies the basic sum rule. The estimates in NB will now become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\phi_{j|y=1}&amp;=\frac{\sum_{i=1}^m{I\{x_{j}^{(i)}=1\land y^{(i)}=1\}}+1}{\sum_{i=1}^m{I\{y^{(i)}=1\}}+2} \\
\phi_{j|y=0}&amp;=\frac{\sum_{i=1}^m{I\{x_{j}^{(i)}=1\land y^{(i)}=0\}}+1}{\sum_{i=1}^m{I\{y^{(i)}=0\}}+2} \\
\end{align} %]]&gt;&lt;/script&gt;</content><author><name></name></author><summary type="html">Roadmap: Logistic Regression Learning Gradient Descent Newton’s Method Normal Equation Code Template k-Nearest Neighbors Statistical Setting for Classification Gaussian Discriminant Analysis 2 Types of Learning Algorithms &amp;amp; Bayes Theorem Naive Bayes Classifier Laplace Smoothing</summary></entry><entry><title type="html">Regression</title><link href="http://localhost:4000/ML/reg/" rel="alternate" type="text/html" title="Regression" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/ML/ML-reg</id><content type="html" xml:base="http://localhost:4000/ML/reg/">&lt;p&gt;Claim: some of the images in this session are cited from ColumbiaX’s &lt;a href=&quot;https://www.edx.org/micromasters/columbiax-artificial-intelligence&quot; target=&quot;__blank&quot;&gt;Artificial Intelligence MicroMasters Program&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#linreg&quot;&gt;Linear Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#prob&quot;&gt;Problem Setting &amp;amp; Model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#learn&quot;&gt;Learning&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#gd&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#newton&quot;&gt;Newton’s Method&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#normal&quot;&gt;Normal Equation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#probabi&quot;&gt;Probabilistic Interpretation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regular&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#code&quot;&gt;Code Template&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#poly&quot;&gt;Polynomial Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#polyprep&quot;&gt;Different Preprocessing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#polycode&quot;&gt;Code Template&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#polyex&quot;&gt;Further Extensions&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lwr&quot;&gt;Locally Weighted Linear Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lwrprob&quot;&gt;Problem Setting &amp;amp; Intuition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lwrmodel&quot;&gt;Model: Weighted LS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lwrcode&quot;&gt;Code Template&lt;/a&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ridge&quot;&gt;Ridge Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ridgeprob&quot;&gt;Problem Setting, Model &amp;amp; Preprocessing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#svd&quot;&gt;Singular Value Decomposition&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#svdcalc&quot;&gt;Calculation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ridgevsls&quot;&gt;Ridge Regression vs Least Squares LinReg&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ridgecode&quot;&gt;Code Template&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lasso&quot;&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;linreg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;prob&quot;&gt;&lt;/a&gt;&lt;strong&gt;Problem Setting&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Observed pairs $(x,y)$, where $x\in\mathbb{R}^{n+1}$ (&lt;strong&gt;input&lt;/strong&gt;) &amp;amp; $y\in\mathbb{R}$ (&lt;strong&gt;output&lt;/strong&gt;)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Find a linear function of the unknown $w$s: $f:\mathbb{R}^n\rightarrow\mathbb{R}\ \ \text{s.t.}\ \ \forall\ (x,y): y\approx f(x,w)$&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \hat{y}_ i&amp;=\sum_{j=0}^{n}w_jx_{ij} \\ \\
  \hat{y}&amp;=Xw \\ \\
  \begin{bmatrix} \hat{y}_ 1 \\ \vdots \\ \hat{y}_ m \end{bmatrix}&amp;=
  \begin{bmatrix}
  1 &amp; x_{11} &amp; \cdots &amp; x_{1n} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  1 &amp; x_{m1} &amp; \cdots &amp; x_{mn} \\
  \end{bmatrix}\begin{bmatrix} w_0 \\ \vdots \\ w_n \end{bmatrix}
  \end{align} %]]&gt;&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$x_{ij}$: the $j$th feature in the $i$th observation&lt;/li&gt;
      &lt;li&gt;$\hat{y}_ i$: the model prediction for the $i$th observation&lt;/li&gt;
      &lt;li&gt;$w_j$: the parameter for the $j$th feature&lt;/li&gt;
      &lt;li&gt;$m$: #observations&lt;/li&gt;
      &lt;li&gt;$n$: #features&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;learning&quot;&gt;&lt;/a&gt;&lt;strong&gt;Learning&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Aim&lt;/strong&gt;: find the optimal $w$ that minimizes a loss function (i.e. cost function)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Loss Function: OLS [Ordinary Least Squares]&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
  \mathcal{L}(w)=\sum_{i=1}^{m}(\hat{y}_ i-y_i)^2
  \end{equation*}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;Assumption (i.e. requirement): $m &amp;gt; &amp;gt; n$&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;gd&quot;&gt;&lt;/a&gt;&lt;strong&gt;Minimization Method 1: Gradient Descent&lt;/strong&gt; (the practical solution)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_j := w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j}
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$\alpha$: learning rate&lt;/li&gt;
          &lt;li&gt;$\frac{\partial\mathcal{L}(w)}{\partial w_j}$: gradient&lt;/li&gt;
        &lt;/ul&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Stochastic GD&lt;/strong&gt; (using 1 training observation for each GD step)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 w_j := w_j-\alpha(\hat{y}_ i-y_i)x_{ij}
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Mini-batch GD&lt;/strong&gt; (using mini-batches of size $m’$ for each GD step)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
 w_j := w_j-\alpha\sum_{i=1}^{m'}(\hat{y}_ i-y_i)x_{ij}
 \end{equation*}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Batch GD&lt;/strong&gt; (LMS) (using the whole training set for each GD step)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 w_j := w_j-\alpha\sum_{i=1}^{m}(\hat{y}_ i-y_i)x_{ij}
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
        &lt;/ol&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Extra: &lt;a name=&quot;newton&quot;&gt;&lt;/a&gt;&lt;strong&gt;Newton’s Method&lt;/strong&gt;&lt;/p&gt;

            &lt;ol&gt;
              &lt;li&gt;
                &lt;p&gt;Newton’s formula&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 w := w-\frac{f(w)}{f'(w)}
 \end{equation}&lt;/script&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Newton’s Method in GD&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 w := w-H^{-1}\nabla_w\mathcal{L}(w)
 \end{equation}&lt;/script&gt;

                &lt;p&gt;where $H$ is Hessian Matrix:&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 H_{ij}=\frac{\partial^2\mathcal{L}(w)}{\partial w_i \partial w_j}
 \end{equation}&lt;/script&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Newton vs normal GD&lt;/p&gt;
                &lt;ul&gt;
                  &lt;li&gt;YES: faster convergence, fewer iterations&lt;/li&gt;
                  &lt;li&gt;NO:  expensive computing (inverse of a matrix)&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;normal&quot;&gt;&lt;/a&gt;&lt;strong&gt;Minimization Method 2: Normal Equation&lt;/strong&gt; (the exact solution)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
  w_{\text{LS}}=(X^TX)^{-1}X^Ty\ \Longleftrightarrow\  w_{\text{LS}}=\Big(\sum_{i=1}^m{x_ix_i^T}\Big)^{-1}\Big(\sum_{i=1}^m{y_ix_i}\Big)
  \end{equation*}&lt;/script&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;Derivation (matrix)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \DeclareMathOperator{\Tr}{tr}
 \nabla_w\mathcal{L}(w)&amp;=\nabla_w(Xw-y)^T(Xw-y) \\
 &amp;=\nabla_w\Tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) \\
 &amp;=\nabla_w(\Tr(w^TX^TXw)-2\Tr(y^TXw)) \\
 &amp;=2X^TXw-2X^Ty \\
 &amp;\Rightarrow X^TXw-X^Ty=0
 \end{align} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Derivation (vector)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \nabla_w\mathcal{L}(w)&amp;=\sum_{i=1}^m{\nabla_w(w^Tx_ix_i^Tw-2w^Tx_iy_i+y_i^2)} \\
 &amp;=-\sum_{i=1}^m{2y_ix_i}+\Big(\sum_{i=1}^m{2x_ix_i^T}\Big)w \\
 &amp;\Rightarrow \Big(\sum_{i=1}^m{x_ix_i^T}\Big)w-\sum_{i=1}^m{y_ix_i}=0
 \end{align} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;GD vs Normal Equation&lt;/strong&gt;&lt;/p&gt;

        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;GD&lt;/th&gt;
              &lt;th style=&quot;text-align: center&quot;&gt;Normal Equation&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Advantage&lt;/strong&gt;&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;faster computing&lt;br /&gt;less computing power required&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;the exact solution&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Disadvantage&lt;/strong&gt;&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;hard to reach the exact solution&lt;/td&gt;
              &lt;td style=&quot;text-align: center&quot;&gt;$(X^TX)^{-1}$ must exist&lt;br /&gt;(i.e. $(X^TX)^{-1}$ must be full rank)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;u&gt;Full rank&lt;/u&gt;: when the $m\times n$ matrix $X$ has $\geq n$ linearly independent rows (i.e. any point in $\mathbb{R}^n$ can be reached by a weighted combination of $n$ rows of $X$)&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;probabi&quot;&gt;&lt;/a&gt;&lt;strong&gt;Probabilistic Interpretation&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Probabilistic Model: Gaussian&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 p(y_i|x_i,w)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
 \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$y_i=w^Tx_i+\epsilon_i$&lt;/li&gt;
          &lt;li&gt;$\epsilon_i\sim N(0,\sigma)$&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Likelihood Function&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 L(w)=\prod_{i=1}^{m}p(y_i|x_i,w)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
 \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Log Likelihoood&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathcal{l}(w)&amp;=\log{L(w)} \\
 &amp;=\log{\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
 &amp;=\sum_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
 &amp;=m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-w^Tx_i)^2
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;&lt;u&gt;Why log?&lt;/u&gt;&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;log = monotonic &amp;amp; increasing on $[0,1]\rightarrow$&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathop{\arg\max}_ {w}L(w)=\mathop{\arg\max}_ {w}\log{L(w)}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;log simplifies calculation (especially &amp;amp; obviously for $\prod$)&lt;br /&gt;
  &lt;br /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;MLE (Maximum Likelihood Estimation)&lt;/strong&gt;&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathop{\arg\max}_ {w}\mathcal{l}(w)&amp;=\mathop{\arg\max}_ {w}(m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\
 &amp;=\mathop{\arg\max}_ {w}(-\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\
 &amp;=\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y_i-w^Tx_i)^2 \\
 &amp;=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;$\Rightarrow$ Least Squares &amp;amp; Maximum Likelihood share the exact same solution.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Expected Value&lt;/strong&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \mathbb{E}[w_{ML}]&amp;=\mathbb{E}[(X^TX)^{-1}X^Ty] \\
 &amp;=(X^TX)^{-1}X^TXw \\
 &amp;=w
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \text{Var}[w_{ML}]&amp;=\mathbb{E}[(w_{ML}-\mathbb{E}[w_{ML}])(w_{ML}-\mathbb{E}[w_{ML}])^T] \\
 &amp;=\mathbb{E}[w_{ML}w_{ML}^T]-\mathbb{E}[w_{ML}]\mathbb{E}[w_{ML}]^T \\
 &amp;=(X^TX)^{-1}X^T\mathbb{E}[yy^T]X(X^TX)^{-1}-ww^T \\
 &amp;=(X^TX)^{-1}X^T(\sigma^2I+Xww^TX^T)X(X^TX)^{-1}-ww^T\ \ (1) \\
 &amp;=\sigma^2(X^TX)^{-1} \\
 \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;$(1)$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \sigma=\text{Var}[y]&amp;=\mathbb{E}[(y-\mu)(y-\mu)^T] \\
 &amp;=\mathbb{E}[yy^T]-2\mu\mu^T+\mu\mu^T \\
 \Rightarrow \mathbb{E}[yy^T]&amp;=\sigma+\mu\mu^T \\
 \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Assumption: Gaussian - $y\ ~\ N(Xw, \sigma^2I)$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Expected Value: $\mathbb{E}[w_{ML}]=w$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Variance: $\text{Var}[w_{ML}]=\sigma^2(X^TX)^{-1}$&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Problem: Notice how $w_{ML}$ becomes huge when our variance $\sigma^2(X^TX)^{-1}$ is too large.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;regular&quot;&gt;&lt;/a&gt;&lt;strong&gt;Regularization&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;: in order to prevent the problem above, we want to constrain our model parameters $w$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_{op}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda g(w)
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$\lambda&amp;gt;0$: regularization parameter&lt;/li&gt;
          &lt;li&gt;$g(w)&amp;gt;0$: penalty function&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Sample Regularizations&lt;/u&gt;: Ridge Regression, LASSO Regression, …&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;code&quot;&gt;&lt;/a&gt;&lt;strong&gt;Code Template&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Python&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;R&lt;/p&gt;

        &lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newdata&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;poly&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;polynomial-regression&quot;&gt;Polynomial Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Polynomial Regression $\in$ Linear Regression ($f$ = a linear function of unknown parameters $w$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;polyprep&quot;&gt;&lt;/a&gt;&lt;strong&gt;Different Preprocessing&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
  X=\begin{bmatrix}
  1 &amp; x_{11} &amp; \cdots &amp; x_{1n} &amp; x_{11}^2 &amp; \cdots &amp; x_{1n}^p \\
  \vdots &amp;  &amp; \vdots &amp;  &amp;  &amp; \vdots &amp;  \\
  1 &amp; x_{m1} &amp; \cdots &amp; x_{mn} &amp; x_{m1}^2 &amp; \cdots &amp; x_{mn}^p \\
  \end{bmatrix}
  \end{equation} %]]&gt;&lt;/script&gt;

    &lt;p&gt;with the width of $p\times n+1$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Everything else is exactly the same as &lt;a href=&quot;#linreg&quot;&gt;linear regression&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Sample models:
    &lt;ul&gt;
      &lt;li&gt;3rd order with 1 feature: $y_i=w_0+w_1x_i+w_2x_i^2+w_3x_i^3$&lt;/li&gt;
      &lt;li&gt;2nd order with 2 features: $y_i=w_0+w_1x_{i1}+w_2x_{i2}+w_3x_{i1}^2+w_4x_{i2}^2$&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;polycode&quot;&gt;&lt;/a&gt;&lt;strong&gt;Code Template&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Python&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PolynomialFeatures&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;poly_reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PolynomialFeatures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;degree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_poly&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;poly_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;poly_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_poly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;R&lt;/p&gt;

        &lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Sample preprocessing of dataset&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;......&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poly_reg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poly_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newdata&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;polyex&quot;&gt;&lt;/a&gt;&lt;strong&gt;Further Extensions&lt;/strong&gt;: we can generalize our linear regression model as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \hat{y}_ i\approx f(x_i,w)=\sum_{s=1}^S{g_s(x_i)w_s}
  \end{equation}&lt;/script&gt;

    &lt;p&gt;where $g_s(x_i)$ can be any function of $x_i$, such as $e^{x_{ij}},\ \log{x_{ij}},\ …$.&lt;/p&gt;

    &lt;p&gt;Everything else is still the same as &lt;a href=&quot;#linreg&quot;&gt;linear regression&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;lwr&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;locally-weighted-linear-regression&quot;&gt;Locally Weighted Linear Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;lwrprob&quot;&gt;&lt;/a&gt;&lt;strong&gt;Problem Setting&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Underfitting&lt;/strong&gt;: the model barely fits the data points.&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/ML/underfit.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

        &lt;p&gt;One single line is usually not enough to capture the pattern of $x\ \&amp;amp;\ y$. In order to get a better fit, we add more polynomial features ($x^j$) to the original model:&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;: the model fits the given data points too well that it cannot be used on other data points.&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/ML/overfit.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

        &lt;p&gt;When we add too much (e.g. $y=\sum_{j=0}^{9}w_jx^j$), the model captures the pattern of the given data points $(x_i,y_i)$ too much that it cannot perform well on new data points.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: When we would like to estimate $y$ at a certain $x$, instead of applying the original LinReg, we take a subset of data points $(x_i,y_i)$ around $x$ and try to do LinReg on that subset only so that we can get a more accurate estimation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;lwrmodel&quot;&gt;&lt;/a&gt;&lt;strong&gt;Model: Weighted LS&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Original LinReg&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y_i-w^Tx_i)^2
   \end{equation}&lt;/script&gt;

        &lt;p&gt;We find the $w$ that minimizes the cost function (maximizes the likelihood function) so that our model is optimized to fit the data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;LWR&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}e^{-\frac{(x_i-x)^2}{2\tau^2}}\cdot(y_i-w^Tx_i)^2
  \end{equation}&lt;/script&gt;

        &lt;p&gt;We add the weight function $\mathcal{W}_ i=e^{-\frac{(x_i-x)^2}{2\tau^2}}$ to the OLS, where&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Numerator&lt;/strong&gt;:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;\text{If}\ |x_i-x|=\text{small} \longrightarrow W_i\approx 1 \\
  &amp;\text{If}\ |x_i-x|=\text{large} \longrightarrow W_i\approx 0
  \end{align} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Bandwidth Parameter&lt;/strong&gt;: $\tau$ (how fast the weight of $x_i$ falls off the query point $x$)&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;\text{When}\ \tau &gt; &gt; 1, \text{LWR} \approx \text{LinReg} \\
  &amp;\text{When}\ \tau &lt; &lt; 1, \text{LWR} \rightarrow \text{overfitting}
  \end{align} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Exact Solution&lt;/strong&gt;:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \DeclareMathOperator{\Tr}{tr}
  \nabla_w\mathcal{L}(w)&amp;=\nabla_w \mathcal{W}(Xw-y)^T(Xw-y) \\
  &amp;\Rightarrow X^T\mathcal{W}Xw-X^T\mathcal{W}y=0 \\
  \Rightarrow w&amp;=(X^T\mathcal{W}X)^{-1}X^T\mathcal{W}y
  \end{align} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;lwrcode&quot;&gt;&lt;/a&gt;&lt;strong&gt;Code Template&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Python: for simple linreg ($n=1$)&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;
            
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lwr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;y_est&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y_est&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Change this line when n&amp;gt;1
&lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_est&lt;/span&gt;
        
  &lt;span class=&quot;o&quot;&gt;......&lt;/span&gt;
        
  &lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The optimal is 1 imo but change whenever necessary
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;y_lwr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lwr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;ridge&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;ridge-regression&quot;&gt;Ridge Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;ridgeprob&quot;&gt;&lt;/a&gt;&lt;strong&gt;Problem Setting&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The OLS LinReg method gives us an accurate expected value: $\mathbb{E}[w_{ML}]=w$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;However, the &lt;strong&gt;variance&lt;/strong&gt; $\text{Var}[w_{ML}]=\sigma^2(X^TX)^{-1}$ could be &lt;strong&gt;too large&lt;/strong&gt; that it ruins our model parameters.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Ridge Regression $\in$ &lt;a href=&quot;#regular&quot;&gt;regularization&lt;/a&gt; methods&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_{RR}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda\|w\|^2_2
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$\lambda$: regularization parameter:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  &amp;\text{If}\ \lambda\rightarrow0\ \ \Longrightarrow w_{RR}\rightarrow w_{LS} \\
  &amp;\text{If}\ \lambda\rightarrow\infty \Longrightarrow w_{RR}\rightarrow \bf{0}
  \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$g(w)=\|w\|^2_2=w^Tw$: L2 penalty function&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \mathcal{L}&amp;=(y-Xw)^T(y-Xw)+\lambda w^Tw \\
  \nabla_w\mathcal{L}&amp;=-2X^Ty+2X^TXw+2\lambda w=0 \\
  \Rightarrow w_{RR}&amp;=(X^TX+\lambda I)^{-1}X^Ty
  \end{align} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Preprocessing: Standardization&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$y$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  y\leftarrow y-\frac{1}{m}\sum_{i=1}^m{y_i}
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$x$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  x_{ij}\leftarrow \frac{x_{ij}-\bar{x}_ j}{\sqrt{\frac{1}{m}\sum_{i=1}^m{(x_{ij}-\bar{x}_ j)^2}}}
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;svd&quot;&gt;&lt;/a&gt;&lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Definition&lt;/u&gt;: We can write any $n\times d\ (n&amp;gt;d)$ matrix $X$ as $X=USV^T$.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$U$: left singular vectors $(m\times r)$
            &lt;ul&gt;
              &lt;li&gt;orthonormal in cols (i.e. $U^TU=I$)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$S$: singular values $(r\times r)$
            &lt;ul&gt;
              &lt;li&gt;non-negative diagonal (i.e. $S_{ii}\geq0, S_{ij}=0\ \forall i\neq j$)&lt;/li&gt;
              &lt;li&gt;sorted in decreasing order (i.e. $\sigma_1\geq\sigma_2\geq\cdots\geq0$)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$V$: right singular vectors $(n\times r)$
            &lt;ul&gt;
              &lt;li&gt;orthonormal (i.e. $V^TV=VV^T=I$)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$m$: #samples&lt;/li&gt;
          &lt;li&gt;$n$: #features&lt;/li&gt;
          &lt;li&gt;$r$: #concepts $(r=\text{rank}(X))$&lt;/li&gt;
          &lt;li&gt;$\sigma_i$: the strength of the $i$th concept&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Properties&lt;/u&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  X^TX&amp;=VS^2V^T \\
  XX^T&amp;=US^2U^T \\
  \text{If}\ \forall i: S_{ii}\neq0 &amp;\Rightarrow (X^TX)^{-1}=VS^{-2}V^T 
  \end{align} %]]&gt;&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  X=USV^T=\sum{\sigma_i\bf{u_i\times v_i^T}}
  \end{equation}&lt;/script&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/ML/svd.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

        &lt;p&gt;Why do we need this? What’s the practical use of this??&lt;/p&gt;

        &lt;p&gt;As an example, suppose we would like to analyze a dataset of the relationship between &lt;u&gt;Users &amp;amp; Movies&lt;/u&gt;, in which:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Each row = a user&lt;/li&gt;
          &lt;li&gt;Each col = a movie&lt;/li&gt;
          &lt;li&gt;Each entry $X_{ij}$ = the rating of movie $j$ from user $i$ (0=unwatched, 1=hate, 5=love)&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;And here is the situation:&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/ML/lagunita.jpg&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
        &lt;center&gt;cited from Stanford's &lt;a href=&quot;https://lagunita.stanford.edu/courses/course-v1:ComputerScience+MMDS+SelfPaced/about&quot;&gt;Mining Massive Datasets&lt;/a&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;$U=$ “User-to-Concept” similarity matrix
            &lt;ul&gt;
              &lt;li&gt;$U[:,1]=$ Sci-fi concept of users&lt;/li&gt;
              &lt;li&gt;$U[:,2]=$ Romance concept of users&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$S=$ “Strength of Concept” matrix
            &lt;ul&gt;
              &lt;li&gt;$S[1,1]=$ Strength of Sci-fi concept&lt;/li&gt;
              &lt;li&gt;$S[2,2]=$ Strength of Romance concept&lt;/li&gt;
              &lt;li&gt;$\because S[3,3]$ is very small $\therefore$ we can ignore this concept and also ignore $U[:,3]$ and $V^T[3,:]$.&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$V^T=$ “Movie-to-Concept” similarity matrix
            &lt;ul&gt;
              &lt;li&gt;$V^T[1,1:3]=$ Sci-fi concept of the Sci-fi movies&lt;/li&gt;
              &lt;li&gt;$V^T[2,4:5]=$ Romance concept of the Romance movies&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;svdcalc&quot;&gt;&lt;/a&gt;&lt;u&gt;Calculation of SVD&lt;/u&gt;:&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;$X^TX=VS^2V^T\Rightarrow$ calculate $V,S^2$
            &lt;ul&gt;
              &lt;li&gt;$S^2\ni$ eigenvalues&lt;/li&gt;
              &lt;li&gt;$V\ni$ eigenvectors&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$XV=US^2\Rightarrow$ calculate $U$&lt;/li&gt;
          &lt;li&gt;GG.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;ridgevsls&quot;&gt;&lt;/a&gt;&lt;strong&gt;Ridge Regression vs Least Squares LinReg&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_{\text{LS}}=(X^TX)^{-1}X^Ty\ \Leftrightarrow\ w_{\text{RR}}=(\lambda I+X^TX)^{-1}X^Ty
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Problems with LS&lt;/u&gt;:&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;$\text{Var}[w_{ML}]=\sigma^2(X^TX)^{-1}=\sigma^2VS^{-2}V^T$&lt;/p&gt;

            &lt;p&gt;When $S_{ii}$ is very small for some values of $i$, $\text{Var}[w_{ML}]$ is very large.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;$y_{\text{new}}=x_{\text{new}}^Tw_{LS}=x_{\text{new}}^T(X^TX)^{-1}X^Ty=x_{\text{new}}^TVS^{-1}U^Ty$&lt;/p&gt;

            &lt;p&gt;When $S^{-1}$ has very large values, our prediction will be very unstable.
  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;LS = a special case of RR&lt;/u&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  w_{\text{RR}}&amp;=(\lambda I+X^TX)^{-1}X^Ty \\
  &amp;=(\lambda I+X^TX)^{-1}(X^TX)(X^TX)^{-1}X^Ty \\
  &amp;=[(X^TX)(\lambda(X^TX)^{-1}+I)]^{-1}(X^TX)w_{\text{LS}} \\
  &amp;=(\lambda(X^TX)^{-1}+I)^{-1}w_{\text{LS}} \\
  &amp;=(\lambda VS^{-2}V^T+I)^{-1}w_{\text{LS}} \\
  &amp;=V(\lambda S^{-2}+I)^{-1}V^Tw_{\text{LS}}\ \ \ \ \ \ \ \ \ |\ \ \ VV^T=I\\
  &amp;:=VMV^Tw_{\text{LS}}
  \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;where $M=(\lambda S^{-2}+I)^{-1}$ is a diagonal matrix with $M_{ii}=\frac{S_{ii}^2}{\lambda+S_{ii}^2}$,&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  w_{\text{RR}}&amp;:=VMV^Tw_{\text{LS}} \\
  &amp;=V(\lambda S^{-2}+I)^{-1}V^T(VS^{-1}U^Ty) \\
  &amp;=VS^{-1}_ \lambda U^Ty \\
  \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;where $S_\lambda^{-1}$ is a diagonal matrix with $S_{ii}=\frac{S_{ii}}{\lambda+S_{ii}^2}$.&lt;/p&gt;

        &lt;p&gt;Therefore, we get another clearer expression of the relationship between RR and LS:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_{\text{LS}}=VS^{-1}U^Ty\ \Leftrightarrow\ w_{\text{RR}}=VS_\lambda^{-1}U^Ty
  \end{equation}&lt;/script&gt;

        &lt;p&gt;And $w_{LS}$ is simply a special case of $w_{RR}$ where $\lambda=0$.
  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;RR = a special case of LS&lt;/u&gt;:&lt;/p&gt;

        &lt;p&gt;If we do some preprocessing to our model $\hat{y}\approx\hat{X}w$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}\begin{bmatrix}
  y \\ 0 \\ \vdots \\ 0
  \end{bmatrix}\approx\begin{bmatrix}
  - &amp; X &amp; - \\ \sqrt{\lambda} &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; \sqrt{\lambda}
  \end{bmatrix}\begin{bmatrix}
  w_1 \\ \vdots \\ w_n
  \end{bmatrix}\end{equation} %]]&gt;&lt;/script&gt;

        &lt;p&gt;Now we have the exact same loss function:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  (\hat{y}-\hat{X}w)^T(\hat{y}-\hat{X}w)=\|y-Xw\|^2+\lambda\|w\|^2
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Probabilistic Interpretation&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Expected Value&lt;/strong&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathbb{E}[w_{\text{RR}}]=(\lambda I+X^TX)^{-1}X^TXw
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
  \text{Var}[w_{\text{RR}}]&amp;=\mathbb{E}[w_{\text{RR}}w_{\text{RR}}^T]-\mathbb{E}[w_{\text{RR}}]\mathbb{E}[w_{\text{RR}}]^T \\
  &amp;=(\lambda I+X^TX)^{-1}X^T\mathbb{E}[yy^T]X(\lambda I+X^TX)^{-1^T} \\
  &amp;\ \ \ \ \ -(\lambda I+X^TX)^{-1}X^TXww^TX^TX(\lambda I+X^TX)^{-1^T} \\
  &amp;=(\lambda I+X^TX)^{-1}X^T(\sigma^2I)X(\lambda I+X^TX)^{-1^T}\ \ (1) \\
  &amp;=\sigma^2Z(X^TX)^{-1}Z^T \\
  \end{align} %]]&gt;&lt;/script&gt;

        &lt;p&gt;where $Z=(I+\lambda(X^TX)^{-1})^{-1}$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;#bvto&quot;&gt;See more info&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;code&quot;&gt;&lt;/a&gt;&lt;strong&gt;Code Template&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Python&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RidgeCV&lt;/span&gt;
        
  &lt;span class=&quot;c1&quot;&gt;# find optimal lambda through cross validation
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;Lambdas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ridge_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RidgeCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'neg_mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ridge_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
  &lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
  &lt;span class=&quot;n&quot;&gt;ridge_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;R&lt;/p&gt;

        &lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# find optimal lambda through cross validation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda_seq&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv.glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda.min&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge_reg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# summary(fit)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
 &lt;a name=&quot;bvto&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;bias-variance-trade-off&quot;&gt;&lt;strong&gt;Bias-Variance Trade-off&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Ridge vs LS&lt;/strong&gt;:&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;LS&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;Ridge&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Expected value&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\mathbb{E}[w_{\text{LS}}]=w$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\mathbb{E}[w_{\text{RR}}]=(\lambda I+X^TX)^{-1}X^TXw$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Variance&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\text{Var}[w_{\text{LS}}]=\sigma^2(X^TX)^{-1}$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\text{Var}[w_{\text{LS}}]=\sigma^2Z(X^TX)^{-1}Z^T$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;The distribution of $w_{\text{RR}}$ is not centered at $w$, but the variance gets much smaller.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;lasso&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;lasso-regression&quot;&gt;Lasso Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Everything is the same as &lt;a href=&quot;#ridge&quot;&gt;Ridge Regression&lt;/a&gt; except the &lt;strong&gt;model&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  w_{\text{lasso}}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda\|w\|_ 1
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$g(w)=\|w\|_ 1=|w|$: L1 penalty function&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: we are yet able to find a solution to the Multivariate LASSO because of the absolute value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Code Template&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Python&lt;/p&gt;

        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LassoCV&lt;/span&gt;
        
  &lt;span class=&quot;c1&quot;&gt;# find optimal lambda through cross validation
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;Lambdas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lasso_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LassoCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'neg_mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lasso_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
  &lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso_cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
  &lt;span class=&quot;n&quot;&gt;lasso_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;R&lt;/p&gt;

        &lt;div class=&quot;language-R highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# find optimal lambda through cross validation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda_seq&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv.glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda.min&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso_reg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glmnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# summary(fit)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Claim: some of the images in this session are cited from ColumbiaX’s Artificial Intelligence MicroMasters Program.</summary></entry><entry><title type="html">Recurrent Neural Networks</title><link href="http://localhost:4000/DL/RNN/" rel="alternate" type="text/html" title="Recurrent Neural Networks" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/DL/DL-RNN</id><content type="html" xml:base="http://localhost:4000/DL/RNN/">&lt;p&gt;Claim: some of the images in this session are cited from Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot; target=&quot;_blank&quot;&gt;Deep Learning&lt;/a&gt; specialization, but most are created by myself.&lt;/p&gt;

&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Basics of RNN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#intsm&quot;&gt;Intuition of Sequence Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#intrnn&quot;&gt;Intuition of RNN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#layers&quot;&gt;RNN Types&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lm&quot;&gt;Language Model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#var&quot;&gt;RNN Variations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gru&quot;&gt;GRU&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lstm&quot;&gt;LSTM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#birnn&quot;&gt;Bidirectional RNN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#drnn&quot;&gt;Deep RNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Word Embeddings
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ohr&quot;&gt;One-hot representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#fr&quot;&gt;Featurized representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#word2vec&quot;&gt;Learning 1: Word2Vec&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#negsam&quot;&gt;Learning 2: Negative Sampling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#glove&quot;&gt;Learning 3: GloVe&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequence Modeling
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sent&quot;&gt;Sentiment Classification&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#seq2seq&quot;&gt;Sequence to Sequence&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#beam&quot;&gt;Beam Search&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bleu&quot;&gt;Bleu Score&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#attention&quot;&gt;Attention Model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;basics-of-rnn&quot;&gt;Basics of RNN&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;intsm&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;intuition-of-sequence-models&quot;&gt;&lt;strong&gt;Intuition of Sequence Models&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;These are called sequence modeling:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Speech recognition&lt;/li&gt;
  &lt;li&gt;Music generation&lt;/li&gt;
  &lt;li&gt;Sentiment classification&lt;/li&gt;
  &lt;li&gt;DNA sequence analysis&lt;/li&gt;
  &lt;li&gt;Machine translation&lt;/li&gt;
  &lt;li&gt;Video activity recognition&lt;/li&gt;
  &lt;li&gt;Name entity recognition&lt;/li&gt;
  &lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Forget about the tedious definitions. As a basic intuition of what we are doing in sequence modeling, here is a very simple example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have a sentence: “Pewdiepie and MrBeast are two of the greatest youtubers in human history.”&lt;/li&gt;
  &lt;li&gt;We want to know: where are the “names” in this sentence? (i.e. name entity recognition)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We convert the input sentence into $X$: $x^{\langle 1 \rangle}x^{\langle 2 \rangle}…x^{\langle t \rangle}…x^{\langle 12 \rangle}$&lt;/p&gt;

    &lt;p&gt;where $x^{\langle t \rangle}$ represents each word in the sentence.&lt;/p&gt;

    &lt;p&gt;But how does it represent a word? Notice that we used the capitalized $X$ for a single sentence. Actually, $X.\text{shape}=5000\times12$, and $x.\text{shape}=5000\times1$. Why?&lt;/p&gt;

    &lt;p&gt;We first make a vocabulary list like $\text{list}=[\text{a; and; …; history; …; MrBeast; …}]$.&lt;/p&gt;

    &lt;p&gt;Then, we convert each word into a one-hot vector representing the index of the word in the dictionary, e.g.:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  x^{\langle 1 \rangle}=\begin{bmatrix}
  0 \\ \vdots \\ 1 \\ \vdots \\ 0
  \end{bmatrix}\longleftarrow 425,\ 
  x^{\langle 2 \rangle}=\begin{bmatrix}
  0 \\ \vdots \\ 1 \\ \vdots \\ 0
  \end{bmatrix}\longleftarrow 3578,\ \cdots\cdots
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We then label the output as $y: 1\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0$ and train our NN on this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Accordingly, we can use most of the sequences in our daily life as datasets and build our NN models on them to solve such ML problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;intrnn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;intuition-of-rnn&quot;&gt;&lt;strong&gt;Intuition of RNN&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We have very briefly mentioned that Conv1D can be used to scan through a sequence, extract features and make predictions. Then why don’t we just stick to Conv1D or use normal ANNs?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The scope of sequence modeling is not necessarily recognition or classification, meaning that our inputs &amp;amp; outputs can be in very diff lengths for diff examples.&lt;/li&gt;
  &lt;li&gt;Neither ANNs nor CNNs share features learned across diff positions of a text or a sequence, whereas context matters quite a lot in most sequence modeling problems.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, we need to define a brand new NN structure that can perfectly align with sequence modeling - RNN:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/rnn.jpg&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Forward propagation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$a^{\langle 0 \rangle}=\textbf{0}$&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$a^{\langle t \rangle}=g(W_{a}[a^{\langle t-1 \rangle}; x^{\langle t \rangle}]+b_a)\ \ \ \ |\ g:\ \text{tanh/ReLU}$&lt;/p&gt;

    &lt;p&gt;where $W_a=[W_{aa}\ W_{ax}]$ with a shape of $(100,10100)$ if we assume a dictionary of 10000 words (i.e. $x^{\langle t \rangle}.\text{shape}=(10000,100)$) and the activation length of 100.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;$\hat{y}^{\langle t \rangle}=g(W_{y}a^{\langle t \rangle}+b_y)\ \ \ \ |\ g:\ \text{sigmoid}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Backward propagation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{L}^{\langle t \rangle}(\hat{y}^{\langle t \rangle},y^{\langle t \rangle})=-\sum_i{y_i^{\langle t \rangle}\log{\hat{y}_ i^{\langle t \rangle}}}\ \ \ \ |\ $Same loss function as LogReg&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;layers&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;rnn-types&quot;&gt;&lt;strong&gt;RNN Types&lt;/strong&gt;&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/rnntypes.png&quot; width=&quot;550&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
There is nothing much to explain here. The images are pretty clear.&lt;br /&gt;
&lt;br /&gt;&lt;a name=&quot;lm&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;language-model&quot;&gt;&lt;strong&gt;Language Model&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Intuition of Softmax &amp;amp; Conditional Probability&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;The core of RNN is to calculate the likelihood of a sequence: $P(y^{\langle 1 \rangle},y^{\langle 2 \rangle},…,y^{\langle t \rangle})$ and output the one with the highest probability.&lt;/p&gt;

    &lt;p&gt;For example, the sequence “&lt;u&gt;the apple and pair salad&lt;/u&gt;” has a much smaller possibility to occur than the sequence “&lt;u&gt;the apple and pear salad&lt;/u&gt;”. Therefore, RNN will output the latter. This seems much like &lt;strong&gt;Softmax&lt;/strong&gt;, and indeed it is.&lt;/p&gt;

    &lt;p&gt;Recall from the formula of conditional probability, we can separate the likelihood into:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  P\big(y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t \rangle}\big)=P\big(y^{\langle 1 \rangle}\big)P\big(y^{\langle 2 \rangle}|y^{\langle 1 \rangle}\big)...P\big(y^{\langle t \rangle}|y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t-1 \rangle}\big)
  \end{equation}&lt;/script&gt;

    &lt;p&gt;For example, to generate the sentence “I like cats.”, we calculate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  P\big(\text{&quot;I like cats&quot;}\big)=P\big(\text{&quot;I&quot;}\big)P\big(\text{&quot;like&quot;}|\text{&quot;I&quot;}\big)P\big(\text{&quot;cats&quot;}|\text{&quot;I like&quot;}\big)
  \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;u&gt;Language Modeling Procedure&lt;/u&gt;
    &lt;ol&gt;
      &lt;li&gt;Data Preparation
        &lt;ul&gt;
          &lt;li&gt;Training set: large corpus of English text (or other languages)&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Tokenize&lt;/strong&gt;: mark every word into a token
            &lt;ul&gt;
              &lt;li&gt;&amp;lt;EOS&amp;gt;: End of Sentence token&lt;/li&gt;
              &lt;li&gt;&amp;lt;UNK&amp;gt;: Unknown word token&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;e.g. “I hate Minecraft and kids.” $\Rightarrow$ “I hate &amp;lt;UNK&amp;gt; and kids. &amp;lt;EOS&amp;gt;”&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Training
        &lt;center&gt;&lt;img src=&quot;../../images/DL/rnnlm.png&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;
 We use the sentence “I hate Minecraft and kids. &amp;lt;EOS&amp;gt;” as one training example.&lt;br /&gt;
 At the beginning, we initialize $a^{&amp;lt;0&amp;gt;}$ and $x^{&amp;lt;1&amp;gt;}$ as $\vec{0}$ and let the RNN try to guess the first word.&lt;br /&gt;
 At each step, we use the original word at the same index $y^{&amp;lt;i-1&amp;gt;}$ and the previous activation $a^{&amp;lt;i-1&amp;gt;}$ to let the RNN try to guess the next word $\hat{y}^{&amp;lt;i&amp;gt;}$ from Softmax regression.&lt;br /&gt;
 During the training process, we try to minimize the loss function $\mathcal{L}(\hat{y},y)$ to ensure the training is effective to predict the sentence correctly.
  &lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Sequence Sampling
        &lt;center&gt;&lt;img src=&quot;../../images/DL/rnnsample.png&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;
 After the RNN is trained, we can use it to generate a sentence by itself. In each step, the RNN will take the previous word it generated $\hat{y}^{&amp;lt;i-1&amp;gt;}$ as $x^{&amp;lt;i&amp;gt;}$ to generate the next word $\hat{y}^{&amp;lt;i&amp;gt;}$.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;u&gt;Character-level LM&lt;/u&gt;
    &lt;ul&gt;
      &lt;li&gt;Dictionary
        &lt;ul&gt;
          &lt;li&gt;Normal LM: [a, abandon, …, zoo, &amp;lt;UNK&amp;gt;]&lt;/li&gt;
          &lt;li&gt;Char-lv LM: [a, b, c, …, z]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pros &amp;amp; Cons
        &lt;ul&gt;
          &lt;li&gt;Pros: never need to worry about unknown words &amp;lt;UNK&amp;gt;&lt;/li&gt;
          &lt;li&gt;Cons: sequence becomes much much longer; the RNN doesn’t really learn anything about the words.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Problems with current RNN&lt;/u&gt;&lt;br /&gt;
  One of the most significant problems with our current simple RNN is &lt;strong&gt;vanishing gradients&lt;/strong&gt;. As shown in the figures above, the next word always has a very strong dependency on the previous word, and the dependency between two words weakens as the distance between them gets longer. In other words, the current RNN are very bad at catching long-line dependencies, for example,&lt;/p&gt;

    &lt;center&gt;the &lt;strong&gt;cat&lt;/strong&gt;, which already ......, &lt;strong&gt;was&lt;/strong&gt; full.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/center&gt;
    &lt;center&gt;the &lt;strong&gt;cats&lt;/strong&gt;, which already ......, &lt;strong&gt;were&lt;/strong&gt; full.&lt;/center&gt;
    &lt;p&gt;&lt;br /&gt;
  “be” verbs have high dependencies on the “subject”, but RNN doesn’t know that. Since the distance between these two words are too long, the gradient on the “subject” nouns would barely affect the training on the “be” verbs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;a name=&quot;var&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;rnn-variations&quot;&gt;RNN Variations&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;RNN&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;GRU&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;LSTM&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;../../images/DL/rnnblock.png&quot; width=&quot;330&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;../../images/DL/gru.png&quot; width=&quot;330&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;../../images/DL/lstm.png&quot; width=&quot;330&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As shown above, there are currently 3 most used RNN blocks. The original RNN block activates the linear combination of $a^{&amp;lt;t-1&amp;gt;}$ and $x^{&amp;lt;t&amp;gt;}$ with a $\text{tanh}$ function and then passes the output value onto the next block.&lt;/p&gt;

&lt;p&gt;However, because of the previously mentioned problem with the original RNN, scholars have created some variations, such as GRU &amp;amp; LSTM.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;gru&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;gru-gated-recurrent-unit&quot;&gt;&lt;strong&gt;GRU&lt;/strong&gt; (Gated Recurrent Unit)&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/gru.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
As the name implies, GRU is an advancement of normal RNN block with “gates”. There are 2 gates in GRU:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;R gate&lt;/strong&gt;: (Remember) determine whether to remember the previous cell&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;U gate&lt;/strong&gt;: (Update) determine whether to update the computation with the candidate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Computing process of GRU:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Compute R gate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \Gamma_r=\sigma\big(w_r\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_r\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute U gate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \Gamma_u=\sigma\big(w_u\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_u\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Candidate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \tilde{c}^{&lt;t&gt;}=\tanh{\big(w_c\big[\Gamma_r * a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_c\big)}
 \end{equation} %]]&gt;&lt;/script&gt;

    &lt;p&gt;When $\Gamma_r=0$, $\tilde{c}^{&amp;lt;t&amp;gt;}=\tanh{\big(w_cx^{&amp;lt;t&amp;gt;}+b_c\big)}$, the previous word has no effect on the word choice of this cell.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Memory Cell:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 c^{&lt;t&gt;}=\Gamma_u \cdot \tilde{c}^{&lt;t&gt;} + (1-\Gamma_u) \cdot c^{&lt;t-1&gt;}
 \end{equation} %]]&gt;&lt;/script&gt;

    &lt;p&gt;When $\Gamma_u=1$,  $c^{&amp;lt;t&amp;gt;}=\tilde{c}^{&amp;lt;t&amp;gt;}$. The candidate updates.&lt;br /&gt;
 When $\Gamma_u=0$,  $c^{&amp;lt;t&amp;gt;}=c^{&amp;lt;t-1&amp;gt;}$. The candidate does not update.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 a^{&lt;t&gt;}=c^{&lt;t&gt;}
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;lstm&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;lstm-long-short-term-memory&quot;&gt;&lt;strong&gt;LSTM&lt;/strong&gt; (Long Short-Term Memory)&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/lstm.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
LSTM is an advancement of GRU. While GRU relatively saves more computing power, LSTM is more powerful. There are 3 gates in LSTM:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;F gate&lt;/strong&gt;: (Forget) determine whether to forget the previous cell&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;U gate&lt;/strong&gt;: (Update) determine whether to update the computation with the candidate&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;O gate&lt;/strong&gt;: (Update) Compute the normal activation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Computing process of GRU:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Compute F gate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \Gamma_f=\sigma\big(w_f\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_f\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute U gate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \Gamma_u=\sigma\big(w_u\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_u\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute O gate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \Gamma_o=\sigma\big(w_o\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_o\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Candidate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \tilde{c}^{&lt;t&gt;}=\tanh{\big(w_c\big[a^{&lt;t-1&gt;};x^{&lt;t&gt;}\big]+b_c\big)}
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Memory Cell:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 c^{&lt;t&gt;}=\Gamma_u \cdot \tilde{c}^{&lt;t&gt;} + \Gamma_f \cdot c^{&lt;t-1&gt;}
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 a^{&lt;t&gt;}=\Gamma_o \cdot \tanh{c^{&lt;t&gt;}}
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Peephole Connection&lt;/strong&gt;: as shown in the formulae, the gate values $\Gamma \propto c^{&amp;lt;t-1&amp;gt;}$, therefore, we can always include $c^{&amp;lt;t-1&amp;gt;}$ into gate calculations to simplify the computing.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;birnn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;bidirectional-rnn&quot;&gt;&lt;strong&gt;Bidirectional RNN&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: Sometimes, our choices of previous words are dependent on the latter words. For example,&lt;/p&gt;

&lt;center&gt;&lt;strong&gt;Teddy&lt;/strong&gt; Roosevelt was a nice president.&lt;/center&gt;
&lt;center&gt;&lt;strong&gt;Teddy&lt;/strong&gt; bears are now on sale!!!&amp;emsp;&amp;emsp;&amp;emsp;&amp;nbsp;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
The word “Teddy” represents two completely different things, but without the context from the latter part, we cannot determine what the “Teddy” stands for. (This example is cited from Andrew Ng’s Coursera Specialization)&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Solution&lt;/u&gt;: We make the RNN bidirectional:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/birnn.png&quot; height=&quot;250&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Each output is calculated as: $\hat{y}^{&amp;lt;t&amp;gt;}=g\Big(W_y\Big[\overrightarrow{a}^{&amp;lt;t&amp;gt;};\overleftarrow{a}^{&amp;lt;t&amp;gt;}\Big]+b_y\Big)$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;drnn&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;deep-rnn&quot;&gt;&lt;strong&gt;Deep RNN&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Don’t be fascinated by the name. It’s just stacks of RNN layers:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/drnn.png&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;word-embeddings&quot;&gt;Word Embeddings&lt;/h2&gt;

&lt;p&gt;Word embedding is a vectorized representation of a word. Because our PC cannot directly understand the meaning of words, we need to convert these words into numerical values first. So far, we have been using &lt;a name=&quot;ohr&quot;&gt;&lt;/a&gt;&lt;strong&gt;One-hot Encoding&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
x^{&lt;1&gt;}=\begin{bmatrix}
0 \\ \vdots \\ 1 \\ \vdots \\ 0
\end{bmatrix}\longleftarrow 425,\ 
x^{&lt;2&gt;}=\begin{bmatrix}
0 \\ \vdots \\ 1 \\ \vdots \\ 0
\end{bmatrix}\longleftarrow 3578,\ \cdots\cdots
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: our RNN doesn’t really learn anything about these words from one-hot representation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;fr&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;featurized-representation&quot;&gt;&lt;strong&gt;Featurized Representation&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;&lt;strong&gt;Intuition:&lt;/strong&gt;&lt;/u&gt; Suppose we have an online shopping review: “Love this dress! Sexy and comfy!”, we can represent this sentence as:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/fr.png&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
We predefine a certain number of features (e.g. gender, royalty, food, size, cost, etc.).&lt;/p&gt;

&lt;p&gt;Then, we give each word (column categories) their relevance to each feature (row categories). As shown in the picture for example, “dress” is very closely related to the feature “gender”, therefore given the value “1”. Meanwhile, “love” is very closely related to the feature “positive”, therefore given the value “0.99”.&lt;/p&gt;

&lt;p&gt;After we define all the featurized values for the words, we get a vectorized representation of each word:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\text{love}=e_ {1479}=\begin{bmatrix}
0.03 \\ 0.01 \\ 0.99 \\ 1.00 \\ \vdots
\end{bmatrix},\text{comfy}=e_ {987}=\begin{bmatrix}
0.01 \\ 0.56 \\ 0.98 \\ 0.00 \\ \vdots
\end{bmatrix},\cdots\cdots\end{equation}&lt;/script&gt;

&lt;p&gt;This way, our RNN will get to know the rough meanings of these words.&lt;/p&gt;

&lt;p&gt;For example, when it needs to generate the next word of this sentence: &lt;strong&gt;“I want a glass of orange _____.”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since it knows that &lt;strong&gt;“orange”&lt;/strong&gt; is a &lt;strong&gt;fruit&lt;/strong&gt; and that &lt;strong&gt;“glass”&lt;/strong&gt; is closely related to &lt;strong&gt;liquid&lt;/strong&gt;, there is a much higher possibility that our RNN will choose &lt;strong&gt;“juice”&lt;/strong&gt; to fill in the blank.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;&lt;strong&gt;Embedding matrix:&lt;/strong&gt;&lt;/u&gt; To acquire the word embeddings such as $\vec{e}_ {1479}$ and $\vec{e}_ {987}$ above, we can multiply our embedding matrix with the one-hot encoding:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
E\times \vec{o}_ j=\vec{e}_ j
\end{equation}&lt;/script&gt;

&lt;p&gt;where $E$ is our featurized representation (i.e. embedding matrix) and $\vec{o}_ j$ is the one-hot encoding of the word (i.e. the index of the word).&lt;/p&gt;

&lt;p&gt;In practice, this is too troublesome since the dimensions of our $E$ tend to be huge (e.g. $(500,10000)$). Thus, we use specialized function to look up an embedding directly from the embedding matrix.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;&lt;strong&gt;Analogies:&lt;/strong&gt;&lt;/u&gt; One of the most useful properties of word embeddings is analogies. For example, &lt;strong&gt;“man $\rightarrow$ woman”=”king $\rightarrow$ ?”&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Suppose we have the following featurized representation:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;man&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;woman&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;king&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;queen&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gender&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.99&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;royal&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.01&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.02&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;age&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.01&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.01&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.77&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;food&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.03&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.04&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.04&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In order to learn the analogy, our RNN will have the following thinking process:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;\text{Goal: look for}\ w: \mathop{\arg\max}_ w{sim(e_w, e_{\text{king}}-(e_{\text{man}}-e_{\text{woman}}))} \\
&amp;\because e_{\text{man}}-e_{\text{woman}}\approx\begin{bmatrix}-2 \\ 0 \\ 0 \\ 0\end{bmatrix}, e_{\text{king}}-e_{\text{queen}}\approx\begin{bmatrix}-2 \\ 0 \\ 0 \\ 0\end{bmatrix} \\
&amp;\therefore e_{\text{man}}-e_{\text{woman}}\approx e_{\text{king}}-e_{\text{queen}} \\
&amp;\text{Calculate cosine similarity: } sim(\vec{u},\vec{v})=\cos{\phi}=\frac{\vec{u}^T\vec{v}}{\|\vec{u}\|_ 2\|\vec{v}\|_ 2} \\
&amp;\text{Confirm: }e_w\approx e_{queen}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;word2vec&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;learning-1-word2vec&quot;&gt;Learning 1: &lt;strong&gt;Word2Vec&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: We definitely do not want to write the embedding matrix by ourselves. Instead, we train a NN model to learn the word embeddings.&lt;/p&gt;

&lt;p&gt;Suppose we have a sentence “Pewdiepie and MrBeast are two of the greatest youtubers in human history”. Before Word2Vec, let’s define context &amp;amp; target:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;: words around target word
    &lt;ul&gt;
      &lt;li&gt;last 4 words: “two of the greatest &lt;strong&gt;__&lt;/strong&gt;”&lt;/li&gt;
      &lt;li&gt;4 words on both sides: “two of the greatest &lt;strong&gt;__&lt;/strong&gt; in human history.”&lt;/li&gt;
      &lt;li&gt;last 1 word: “greatest &lt;strong&gt;__&lt;/strong&gt;”&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;skip-gram&lt;/strong&gt;: (any nearby word) “… MrBeast … &lt;strong&gt;__&lt;/strong&gt; …”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: the word we want our NN to generate
    &lt;ul&gt;
      &lt;li&gt;“youtubers”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Randomly&lt;/strong&gt; choose context &amp;amp; target words with &lt;strong&gt;skip-gram&lt;/strong&gt;. (e.g. context “MrBeast” &amp;amp; target “youtubers”)&lt;/li&gt;
  &lt;li&gt;Learn &lt;strong&gt;mapping&lt;/strong&gt; of “$c\ (\text{“mrbeast”}[1234])\rightarrow t\ (\text{“youtubers”}[765])$”&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;strong&gt;softmax&lt;/strong&gt; to calculate the probability of appearance of target given context:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \hat{y}=P(t|c)=\frac{e^{\theta_t^Te_c}}{\sum_{j=1}^{n}{e^{\theta_j^Te_c}}}
 \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize the &lt;strong&gt;loss&lt;/strong&gt; function:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \mathcal{L}(\hat{y},y)=-\sum_{i=1}^{n}{y_i\log{\hat{y}_ i}}
 \end{equation}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Computation of softmax is very slow: Hierarchical Softmax (i.e. Huffman Tree + LogReg) can solve this problem - with common words at the top and useless words at the bottom.&lt;/li&gt;
  &lt;li&gt;$c\ \&amp;amp;\ t$ should not be entirely random: words like “the/at/on/it/…” should not be chosen.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;negsam&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;learning-2-negative-sampling&quot;&gt;Learning 2: &lt;strong&gt;Negative Sampling&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: Given a pair of words, predict whether it’s a context-target pair.&lt;/p&gt;

&lt;p&gt;For example, given the word “orange” as the context, we want our model to know that “orange &amp;amp; juice” is a context-target pair but “orange &amp;amp; king” is not.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pick a context-target pair $(c,t)$ (the target should be near the context) from the text corpus as a &lt;strong&gt;positive example&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pick random words $\{t_1,\cdots,t_k\}$ from the dictionary and form word pairs $\{(c,t_1),\cdots,(c,t_k)\}$ as &lt;strong&gt;negative examples&lt;/strong&gt; based on the following probability that the creator recommended:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 P(w_i)=\frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=1}^{n}{f(w_i)^{\frac{3}{4}}}}
 \end{equation}&lt;/script&gt;

    &lt;p&gt;where $w_i$ is the $i$th word in the dictionary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train a &lt;strong&gt;binary classifier&lt;/strong&gt; based on the training examples from previous steps:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \hat{y}_ i=P(y=1|c,t_i)=\sigma(\theta_{t_i}^Te_c)
 \end{equation}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Repeat Step 1-3 till we form our final embedding matrix $E$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Negative Sampling is relatively faster and less costly compared to Word2Vec, since it replaces softmax with binary classification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;glove&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;learning-3-glove-global-vectors&quot;&gt;Learning 3: &lt;strong&gt;GloVe&lt;/strong&gt; (Global Vectors)&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: Learn word embeddings based on how many times target $i$ appears in context of word $j$.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 \sum_{i=1}^{n}{\sum_{j=1}^{n}{f\big(X_{ij}\big)\big(\theta_i^Te_j+b_i+b'_ j-\log{X_{ij}}\big)^2}}
 \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$X_{ij}$: #times $i$ appears in context of $j$&lt;/li&gt;
      &lt;li&gt;$f\big(X_{ij}\big)$: weighing term
        &lt;ul&gt;
          &lt;li&gt;$f\big(X_{ij}\big)=0$ if $X_{ij}=0$&lt;/li&gt;
          &lt;li&gt;$f\big(X_{ij}\big)$ high for uncommon words&lt;/li&gt;
          &lt;li&gt;$f\big(X_{ij}\big)$ low for too-common words&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$b_i:t$, $b’_ j:c$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the final embedding of word $w$:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 e_w^{\text{final}}=\frac{e_w+\theta_w}{2}
 \end{equation}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;sequence-modeling&quot;&gt;Sequence Modeling&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;sent&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sentiment-classification&quot;&gt;&lt;strong&gt;Sentiment Classification&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem Setting&lt;/u&gt;: (many-to-one) given text, predict sentiment.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/sent.png&quot; width=&quot;550&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
&lt;u&gt;Model&lt;/u&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/sentrnn.png&quot; width=&quot;550&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;seq2seq&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;seq2seq&quot;&gt;&lt;strong&gt;Seq2Seq&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem Setting&lt;/u&gt;: (many-to-many) given an entire sequence, generate a new sequence.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Example 1: Machine Translation&lt;/u&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/seq2seq.png&quot; width=&quot;550&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Machine Translation vs Language Model:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Language Model: maximize $P(y^{&amp;lt;1&amp;gt;},\cdots,y^{&amp;lt;T_y&amp;gt;})$&lt;/li&gt;
  &lt;li&gt;Machine Translation: maximize $P(y^{&amp;lt;1&amp;gt;},\cdots,y^{&amp;lt;T_y&amp;gt;} | \vec{x})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;Example 2: Image Captioning&lt;/u&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/seq2seqic.png&quot; width=&quot;550&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;beam&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;beam-search&quot;&gt;&lt;strong&gt;Beam Search&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: So far, when we choose a word from softmax for each RNN block, we are doing &lt;strong&gt;greedy search&lt;/strong&gt;, that we only look for &lt;strong&gt;local optimum&lt;/strong&gt; instead of &lt;strong&gt;global optimum&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That is, we only choose the word with the highest $P(y^{&amp;lt;1&amp;gt;}|\vec{x})$ and then the word with the highest $P(y^{&amp;lt;2&amp;gt;}|\vec{x})$ and then …&lt;/p&gt;

&lt;p&gt;As we already know, local optimum does not necessarily represent global optimum. In the world of NLP, the word &lt;strong&gt;“going”&lt;/strong&gt; always has a much higher probability to appear than the word &lt;strong&gt;“visiting”&lt;/strong&gt;, but in certain situations when we need to use “visiting”, the algorithm will still choose “going”, therefore generating a weird sequence as a whole.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Beam Search Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Define a beam size of $B$ (usually $B\in\{1\times10^n,3\times10^n\},\ n\in\mathbb{Z}^+$).&lt;/li&gt;
  &lt;li&gt;Look at the top $B$ words with the highest $P$s for the first word. (i.e. look for $P(\vec{y}^{&amp;lt;1&amp;gt;}|\vec{x})$)&lt;/li&gt;
  &lt;li&gt;Repeat till &amp;lt;EOS&amp;gt;. Choose the sequence with the highest combined probability.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;u&gt;Improvement&lt;/u&gt;: The original Beam Search is very costly in computing, therefore it is necessary to refine it:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;\because P(y^{&lt;1&gt;},\cdots,y^{&lt;T_y&gt;}|x)=\prod_{t=1}^{T_y}{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\cdots,y^{&lt;t-1&gt;})} \\
&amp;\therefore \text{goal}=\mathop{\arg\max}_ y{\prod_{t=1}^{T_y}{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\cdots,y^{&lt;t-1&gt;})}} \\
&amp;\Rightarrow \mathop{\arg\max}_ y{\sum_{t=1}^{T_y}{\log{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\cdots,y^{&lt;t-1&gt;})}}} \\
&amp;\Rightarrow \mathop{\arg\max}_ y{\frac{1}{T_y^{\alpha}}\sum_{t=1}^{T_y}{\log{P(y^{&lt;t&gt;}|x,y^{&lt;1&gt;},\cdots,y^{&lt;t-1&gt;})}}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\prod\rightarrow\sum{\log}$: log scaling&lt;/li&gt;
  &lt;li&gt;$\frac{1}{T_y^{\alpha}}$: length normalization (when you add more negative values ($\log{(P&amp;lt;1)}&amp;lt;0$), the sum becomes more negative)&lt;/li&gt;
  &lt;li&gt;$\alpha$: &lt;strike&gt;learning rate&lt;/strike&gt; just a coefficient&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;Error Analysis&lt;/u&gt;: Suppose we want to analyze the following error:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Human: Jimmy visits Africa in September. ($y^*$)&lt;/li&gt;
  &lt;li&gt;Algorithm: Jimmy visited Africa last September. ($\hat{y}$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If $P(y^*|x)&amp;gt;P(\hat{y}|x)$, Beam search is at fault $\rightarrow$ increase $B$&lt;br /&gt;
If $P(y^*|x)\leq P(\hat{y}|x)$, RNN is at fault $\rightarrow$ improve RNN (data augmentation, regularization, architecture, etc.)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;bleu&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;bleu-score&quot;&gt;&lt;strong&gt;Bleu Score&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: For many sequence modeling problems (especially seq2seq), there is no fixed correct answer. For example, there are many different Chinese translated versions of the same fiction Sherlock Holmes, and they are all correct. In this case, how do we define “correctness” for machine translation?&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Bilingual Evaluation Understudy&lt;/u&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
p_n=\frac{\sum_{\text{n-gram}\in\hat{y}}{\text{count}_ {clip}(\text{n-gram})}}{\sum_{\text{n-gram}\in\hat{y}}{\text{count}(\text{n-gram})}}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{n-gram}$: $n$ consecutive words (e.g. bigram: “I have a pen.” $\rightarrow$ “I have”, “have a”, “a pen”)&lt;/li&gt;
  &lt;li&gt;$\text{count}_ {clip}(\text{n-gram})$: maximal #times an n-gram appears in one of the reference sequences&lt;/li&gt;
  &lt;li&gt;$\text{count}(\text{n-gram})$: #times an n-gram appears in $\hat{y}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input: “Le chat est sur le tapis.”&lt;/li&gt;
  &lt;li&gt;Reference 1: “The cat is on the mat.”&lt;/li&gt;
  &lt;li&gt;Reference 2: “There is a cat on the mat.”&lt;/li&gt;
  &lt;li&gt;MT output: “the cat the cat on the mat.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The unigrams here are: “the”, “cat”, “on”, “mat”. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
p_1=\frac{2+1+1+1}{3+2+1+1}=\frac{5}{7}
\end{equation}&lt;/script&gt;

&lt;p&gt;The bigrams here are: “the cat”, “cat the”, “cat on”, “on the”, “the mat”. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
p_2=\frac{1+0+1+1+1}{2+1+1+1+1}=\frac{2}{3}
\end{equation}&lt;/script&gt;

&lt;p&gt;The final Bleu score will be calculated as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\text{BLEU}=BP\times e^{\frac{1}{4}\sum_{n=1}^{4}{p_n}}
\end{equation}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;usually we take $n=4$ as the upper limit for n-grams.&lt;/li&gt;
  &lt;li&gt;$BP$: param to penalize short outputs ($\because$ short outputs tend to have high BLEU scores.)&lt;/li&gt;
  &lt;li&gt;$BP=1$ if $\text{len}(\hat{y})&amp;gt;\text{len}(\text{ref})$&lt;/li&gt;
  &lt;li&gt;$BP=e^{\frac{1-\text{len}(\hat{y})}{\text{len}(\text{ref})}}$ if $\text{len}(\hat{y})\leq\text{len}(\text{ref})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;a name=&quot;attention&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;attention-model&quot;&gt;&lt;strong&gt;Attention Model&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: Our Seq2Seq model memorizes the entire sequence and then start to generate output sequence. However, a better approach to such problems like machine translation is actually to memorize part of the sequence, translate it, then memorize the next part of the sequence, translate it, and then keep going. Memorizing the entire fiction series of Sherlock Holmes and then translate it is just inefficient.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;Model&lt;/u&gt;:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/attention.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
&lt;center&gt;Attention Model = Encoding BRNN + Decoding RNN&lt;/center&gt;

&lt;p&gt;&lt;u&gt;Algorithm&lt;/u&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Combine BRNN activations:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 a^{&lt;t'&gt;}=\Big(\overleftarrow{a}^{&lt;t'&gt;},\overrightarrow{a}^{&lt;t'&gt;}\Big)
 \end{equation} %]]&gt;&lt;/script&gt;

    &lt;p&gt;where $t’$ refers to the index of the encoding BRNN layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate the amount of “attention” that $y^{&amp;lt;t&amp;gt;}$ should pay to $a^{&amp;lt;t’&amp;gt;}$:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \alpha^{&lt;t,t'&gt;}=\frac{e^{(e^{&lt;t,t'&gt;})}}{\sum_{t'=1}^{T_x}{e^{(e^{&lt;t,t'&gt;})}}}
 \end{equation} %]]&gt;&lt;/script&gt;

    &lt;p&gt;where $e^{&amp;lt;t,t’&amp;gt;}=W_e^{&amp;lt;t,t’&amp;gt;}[s^{&amp;lt;t-1&amp;gt;};a^{&amp;lt;t’&amp;gt;}] +b_e^{&amp;lt;t,t’&amp;gt;}$ is a linear combination of both encoding activation $a^{&amp;lt;t’&amp;gt;}$ and decoding activation $s^{&amp;lt;t-1&amp;gt;}$. $t$ refers to the index of the decoding RNN layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate the total attention at $t$:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 c^{&lt;t&gt;}=\sum_{t'}{\alpha^{&lt;t,t'&gt;}a^{&lt;t'&gt;}}
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Include the total attention into the input for output calculation:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
 \hat{y}^{&lt;t&gt;}=s^{&lt;t&gt;}=g\big(W_y[\hat{y}^{&lt;t-1&gt;};c^{&lt;t&gt;}]+b_y\big)
 \end{equation} %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Claim: some of the images in this session are cited from Andrew Ng’s Deep Learning specialization, but most are created by myself.</summary></entry><entry><title type="html">Convolutional Neural Networks</title><link href="http://localhost:4000/DL/CNN/" rel="alternate" type="text/html" title="Convolutional Neural Networks" /><published>2019-12-01T22:29:53+09:00</published><updated>2019-12-01T22:29:53+09:00</updated><id>http://localhost:4000/DL/DL-CNN</id><content type="html" xml:base="http://localhost:4000/DL/CNN/">&lt;p&gt;Claim: some of the images in this session are cited from Andrew Ng’s &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot; target=&quot;_blank&quot;&gt;Deep Learning&lt;/a&gt; specialization.&lt;/p&gt;

&lt;p&gt;Roadmap:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Basics of CNN
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cnn&quot;&gt;Intuition of CNN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#formula&quot;&gt;General Formula of Convolution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#layers&quot;&gt;CNN Layers&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Convolution&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#pool&quot;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#fc&quot;&gt;Fully Connected&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN Examples
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lenet&quot;&gt;LeNet-5&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#alexnet&quot;&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vgg&quot;&gt;VGG&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Inception (the most powerful CNN as far as I know)
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#res&quot;&gt;ResNets&lt;/a&gt; (not CNN but prerequisite for understanding Inception)&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#nin&quot;&gt;1x1 Conv&lt;/a&gt; (i.e. NiN)&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#inception&quot;&gt;The Inception&lt;/a&gt; (We need to go deeper!)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conv1d&quot;&gt;Conv1D &amp;amp; Conv3D&lt;/a&gt; (Yes, they exist.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#od&quot;&gt;Object Detection&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Bounding Box&lt;/li&gt;
      &lt;li&gt;Landmark Detection&lt;/li&gt;
      &lt;li&gt;Sliding Windows&lt;/li&gt;
      &lt;li&gt;Intersection over Union&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#yolo&quot;&gt;YOLO (You Only Look Once)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Non-Max Suppression&lt;/li&gt;
          &lt;li&gt;Anchor Boxes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rcnn&quot;&gt;R-CNN&lt;/a&gt; (will talk about this in the future)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fr&quot;&gt;Face Recognition&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sn&quot;&gt;Siamese Network&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#tl&quot;&gt;Triplet Loss&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bc&quot;&gt;Binary Classification&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nst&quot;&gt;Neural Style Transfer&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;basics-of-cnn&quot;&gt;Basics of CNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;cnn&quot;&gt;&lt;/a&gt;&lt;strong&gt;Intuition of CNN&lt;/strong&gt;&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/cnn.gif&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CNN is mostly used in Computer Vision (image classification, object detection, neural style transfer, etc.)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: images $\rightarrow$ volume of numerical values in the shape of &lt;strong&gt;width $\times$ height $\times$ color-scale&lt;/strong&gt; (color-scale=3 $\rightarrow$ RGB; color-scale=1 $\rightarrow$ BW)&lt;/p&gt;

        &lt;p&gt;In the gif above, the input shape is $5\times5\times3$, meaning that the image is colored and the image size $5\times5$. The “$7\times7\times3$” results from &lt;strong&gt;padding&lt;/strong&gt;, which will be discussed below.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Convolution&lt;/strong&gt;:
        &lt;ol&gt;
          &lt;li&gt;For each color layer of the input image, we apply a 2d &lt;strong&gt;filter&lt;/strong&gt; that &lt;strong&gt;scans&lt;/strong&gt; through the layer in order.&lt;/li&gt;
          &lt;li&gt;For each block that the filter scans, we &lt;strong&gt;multiply&lt;/strong&gt; the corresponding filter value and the cell value, and we &lt;strong&gt;sum&lt;/strong&gt; them up.&lt;/li&gt;
          &lt;li&gt;We &lt;strong&gt;sum&lt;/strong&gt; up the output values from all layers of the filter (and add a bias value to it) and &lt;strong&gt;output&lt;/strong&gt; this value to the corresponding output cell.&lt;/li&gt;
          &lt;li&gt;(If there are multiple filters, ) After the first filter finishes scanning, the next filter starts scanning and outputs into a new layer.&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;In the gif above,
        &lt;ol&gt;
          &lt;li&gt;Apply 2 filters of the shape $3\times3\times3$.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;1st filter - 1st layer - 1st block:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 0+0+0+0+0+0+0+(1\times-1)+0=-1
 \end{equation}&lt;/script&gt;

            &lt;p&gt;1st filter - 2nd layer - 1st block:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 0+0+0+0+(2\times-1)+(1\times1)+0+(2\times1)+0=1
 \end{equation}&lt;/script&gt;

            &lt;p&gt;1st filter - 3rd layer - 1st block:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 0+0+0+0+(2\times1)+0+0+(1\times-1)+0=1
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Sum up + bias $\rightarrow$ 1st cell of 1st output layer&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 -1+1+1+1=2
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;Repeat till we finish scanning&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Edge Detection &amp;amp; Filter&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sample filters&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/edgedetect.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

        &lt;ul&gt;
          &lt;li&gt;Gray Scale: 1 = lighter, 0 = gray, -1 = darker&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Notice that we don’t really need to define any filter values. Instead, we are supposed to train the filter values.&lt;br /&gt;
  All the convolution operations above are just the same as the operations in ANN. Filters here correspond to $W$ in ANN.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Padding&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Problem: corner cells &amp;amp; edge cells are detected much fewer times than the middle cells $\rightarrow$ info loss of corner &amp;amp; edge&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Solution: pad the edges of the image with “0” cells (as shown in the gif above)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stride&lt;/strong&gt;: the step size the filter takes ($s=2$ in the gif above)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;formula&quot;&gt;&lt;/a&gt;&lt;strong&gt;General Formula of Convolution&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{Output Size}=\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor\times\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor
  \end{equation}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$n\times n$: image size&lt;/li&gt;
      &lt;li&gt;$f\times f$: filter size&lt;/li&gt;
      &lt;li&gt;$p$: padding&lt;/li&gt;
      &lt;li&gt;$s$: stride&lt;/li&gt;
      &lt;li&gt;Floor: ignore the computation when the filter sweeps the region outside the image matrix&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;layers&quot;&gt;&lt;/a&gt;&lt;strong&gt;CNN Layers&lt;/strong&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Convolution&lt;/strong&gt; (CONV): as described above&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;pool&quot;&gt;&lt;/a&gt;&lt;strong&gt;Pooling&lt;/strong&gt; (POOL): to reduce #params &amp;amp; computations (most common pooling size = $2\times2$)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Max Pooling&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/maxpool.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

            &lt;ol&gt;
              &lt;li&gt;Divide the matrix evenly into regions&lt;/li&gt;
              &lt;li&gt;Take the max value in that region as output value&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Average Pooling&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/avgpool.png&quot; height=&quot;190&quot; /&gt;&lt;/center&gt;

            &lt;ol&gt;
              &lt;li&gt;Divide the matrix evenly into regions&lt;/li&gt;
              &lt;li&gt;Take the average value of the cells in that region as output value&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Stochastic Pooling&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/stochasticpool.png&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;

            &lt;ol&gt;
              &lt;li&gt;Divide the matrix evenly into regions&lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Normalize each cell based on the regional sum:&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 p_i=\frac{a_i}{\sum_{k\in R_j}{a_k}}
 \end{equation}&lt;/script&gt;
              &lt;/li&gt;
              &lt;li&gt;Take a random cell based on multinomial distribution as output value&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a name=&quot;fc&quot;&gt;&lt;/a&gt;&lt;strong&gt;Fully Connected&lt;/strong&gt; (FC): to flatten the 2D/3D matrices into a single vector (each neuron is connected with all input values)&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/fullyconnected.png&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;cnn-examples&quot;&gt;CNN Examples&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;lenet&quot;&gt;&lt;/a&gt;&lt;strong&gt;LeNet-5&lt;/strong&gt;: LeNet-5 Digit Recognizer&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/cnneg.png&quot; /&gt;&lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Shape&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Total Size&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#params&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;INPUT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32 x 32 x 3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3072&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV1 (Layer 1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;28 x 28 x 6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4704&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;156&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POOL1 (Layer 1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14 x 14 x 6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1176&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV2 (Layer 2)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10 x 10 x 16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;416&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POOL2 (Layer 2)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5 x 5 x 16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FC3 (Layer 3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;120 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;120&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;48001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FC4 (Layer 4)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;84 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;84&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Softmax&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;841&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Calculation of #params for CONV: $(f\times f+1)\times n_f$
    &lt;ul&gt;
      &lt;li&gt;$f$: filter size&lt;/li&gt;
      &lt;li&gt;$+1$: bias&lt;/li&gt;
      &lt;li&gt;$n_f$: #filter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a name=&quot;alexnet&quot;&gt;&lt;/a&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;: winner of 2012 ImageNet Large Scale Visual Recognition Challenge&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/alexnet.png&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Shape&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Total Size&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#params&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;INPUT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;227 x 227 x 3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;154587&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV1 (Layer 1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;55 x 55 x 96&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;290400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11712&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POOL1 (Layer 1)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;27 x 27 x 96&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;69984&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV2 (Layer 2)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;27 x 27 x 256&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;186624&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6656&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POOL2 (Layer 2)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13 x 13 x 256&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43264&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV3 (Layer 3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13 x 13 x 384&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64896&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3840&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV4 (Layer 3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13 x 13 x 384&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64896&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3840&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONV5 (Layer 3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13 x 13 x 256&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43264&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2560&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POOL5 (Layer 3)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6 x 6 x 256&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9216&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FC5 (Flatten)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9216 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9216&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FC6 (Layer 4)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4096 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4096&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;37748737&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FC7 (Layer 5)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4096 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4096&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16777217&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Softmax&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000 x 1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4096000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Significantly bigger than LeNet-5 (60M params to be trained)&lt;/li&gt;
  &lt;li&gt;Require multiple GPUs to speed the training up&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;vgg&quot;&gt;&lt;/a&gt;&lt;strong&gt;VGG&lt;/strong&gt;: made by Visual Geometry Group from Oxford&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../../images/DL/vgg.png&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Too large: 138M params&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Inception&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;res&quot;&gt;&lt;/a&gt;&lt;strong&gt;ResNets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Residual Block&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/resnet.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
  \end{equation}&lt;/script&gt;

        &lt;p&gt;Intuition: we add activation values from layer $l$ to the activation in layer $l+2$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Why ResNets?&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ResNets allow parametrization for the identity function $f(x)=x$&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ResNets are proven to be more effective than plain networks:&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/resnetperf.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
          &lt;/li&gt;
          &lt;li&gt;ResNets add more complexity to the NN in a very simple way&lt;/li&gt;
          &lt;li&gt;The idea of ResNets further inspired the development of RNN&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;nin&quot;&gt;&lt;/a&gt;&lt;strong&gt;1x1 Conv&lt;/strong&gt; (i.e. Network in Network [NiN])&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;WHY??? This sounds like the stupidest idea ever!!&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Watch this.&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/1x1pt1.png&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

        &lt;center&gt;In a normal CNN layer like this, we need to do in total 210M calculations.&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/1x1pt2.png&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

        &lt;center&gt;However, if we add a 1x1 Conv layer in between, we only need to do in total 17M calculations.&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Therefore, 1x1 Conv is significantly more useful than what newbies expect. When we would like to keep the matrix size but reduce #layers, using 1x1 Conv can significantly reduce #computations needed, thus requiring less computing power.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;inception&quot;&gt;&lt;/a&gt;&lt;strong&gt;The Inception&lt;/strong&gt;: We need to go deeper!&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Inception Module&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/incepm.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Inception Network&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/incep.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;conv1d&quot;&gt;&lt;/a&gt;&lt;strong&gt;Conv1D &amp;amp; Conv3D&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Although CNN (Conv2D) is undoubtedly most useful in Computer Vision, there are also some other forms of CNN used in other fields:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conv1D&lt;/strong&gt;: e.g. text classification, heartbeat detection, …&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/conv1d.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

    &lt;ul&gt;
      &lt;li&gt;use a 1D filter to convolve a 1D input vector&lt;/li&gt;
      &lt;li&gt;e.g. $14\times1\xrightarrow{5\times1,16}10\times16\xrightarrow{5\times16,32}6\times32$&lt;/li&gt;
      &lt;li&gt;However, this is almost never used since we have &lt;strong&gt;RNN&lt;/strong&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conv3D&lt;/strong&gt;: e.g. CT scan, …&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/conv3d.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

    &lt;ul&gt;
      &lt;li&gt;use a 3D filter to convolve a 3D input cube&lt;/li&gt;
      &lt;li&gt;e.g. $14\times14\times14\times1\xrightarrow{5\times5\times5\times1,16}10\times10\times10\times16\xrightarrow{5\times5\times5\times16,32}6\times6\times6\times32$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;object-detection&quot;&gt;&lt;a name=&quot;od&quot;&gt;&lt;/a&gt;Object Detection&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Object Localization $\rightarrow$ 1 obj; Detection $\rightarrow$ multiple objs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bounding Box&lt;/strong&gt;: to capture the obj in the img with a box
    &lt;ul&gt;
      &lt;li&gt;Params:
        &lt;ul&gt;
          &lt;li&gt;$b_x, b_y$ = central point&lt;/li&gt;
          &lt;li&gt;$b_h, b_w$ = full height/width&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;New target label (in place of image classification output):&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  y=\begin{bmatrix}
  p_c \\ b_x \\ b_y \\ b_h \\ b_w \\ c_1 \\ \vdots \\ c_n
  \end{bmatrix}
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$p_c$: “is there any object in this box?”
            &lt;ul&gt;
              &lt;li&gt;if $p_c=0$, we ignore the remaining params&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;$c_i$: class label $i$ (e.g. $c_1$: cat, $c_2$: dog, $c_3$: bird, …)&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Landmark Detection&lt;/strong&gt;: to capture the obj in the img with points
    &lt;ul&gt;
      &lt;li&gt;Params: $(l_{ix},l_{iy})$ = each landmark point&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;New target label:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  y=\begin{bmatrix}
  p_c \\ l_{1x} \\ l_{1y} \\ \vdots \\ l_{nx} \\ l_{ny} \\ c_1 \\ \vdots \\ c_n
  \end{bmatrix}
  \end{equation}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;THE LABELS MUST BE CONSISTENT!
        &lt;ul&gt;
          &lt;li&gt;Always start from the exact same location of the object! (e.g. if you start with the left corner of the left eye for one image, you should always start with the left corner of the left eye for all images.)&lt;/li&gt;
          &lt;li&gt;#landmarks should be the same!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;br /&gt;
  I personally have a very awful experience with Landmark Detection. When the algorithms of object detection were not yet well-known in the IT industry, I worked on a project of digital screen defects detection in a Finnish company. Since digital screen defects are 1) black &amp;amp; white 2) in very simple geometric shapes, the usage of bounding boxes could have significantly reduced the complexity of both data collection and NN model building.&lt;br /&gt;&lt;br /&gt;
  However, the team insisted to use landmark detection. Due to 1) that screen defects are unstructured 2) that the number of landmark points for two different screen defects can hardly be the same, the dataset was basically unusable, and none of the models we built could learn accurate patterns from it, leading to an unfortunate failure.&lt;br /&gt;&lt;br /&gt;
  I personally would argue that bounding box is much better than landmark detection in most practical cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sliding Window&lt;/strong&gt;&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/sliding.gif&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Apply a sliding window with a fixed size to scan every part of the img left-right and top-bottom (just like CONV), and feed each part to CNN&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In order to capture the same type of objects in different sizes and positions in the img, shrink the img (i.e. enlarge the sliding window) and scan again, and repeat.&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Problem: HUGE computational cost!&lt;/li&gt;
      &lt;li&gt;Solution: (contemporary)
        &lt;ol&gt;
          &lt;li&gt;
            &lt;p&gt;Convert FC layer into CONV layer&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/slidingfc.jpg&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;
            &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Share the former FC info with latter convolutions&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/sliding.png&quot; width=&quot;700&quot; /&gt;&lt;/center&gt;
            &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

            &lt;ol&gt;
              &lt;li&gt;First run of the CNN.&lt;/li&gt;
              &lt;li&gt;Second run of the same CNN with a bigger size of the same img (due to sliding window). Notice that the FC info from the first run is shared in the second run.&lt;/li&gt;
              &lt;li&gt;Latter runs of the same CNN with bigger sizes of the same img (due to sliding window). Notice that the FC info from all previous runs is shared in this run, thus saving computation power and memories.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Intersection over Union&lt;/strong&gt;&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/iou.png&quot; width=&quot;200&quot; /&gt;&lt;/center&gt;
    &lt;center&gt;Is the purple box a good prediction of the car location?&lt;/center&gt;

    &lt;p&gt;Intersection over Union is defined as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{IoU}=\frac{\text{area of intersection}}{\text{area of union}}
  \end{equation}&lt;/script&gt;

    &lt;p&gt;In this case, area of intersection is the intersection between the red and purple box, and area of union is the total area covered by the red and purple box.&lt;br /&gt;
  If $\text{IoU}\leq 0.5$, then the prediction box is correct. (Other threshold values are also okay but 0.5 is conventional.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;yolo&quot;&gt;&lt;/a&gt;&lt;strong&gt;YOLO (You Only Look Once)&lt;/strong&gt;&lt;/p&gt;

    &lt;center&gt;&lt;img src=&quot;../../images/DL/yolo.jpg&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Grids&lt;/strong&gt;: divide the image into grids &amp;amp; use each grid as a bounding box
        &lt;ul&gt;
          &lt;li&gt;when $p_c=0$, we ignore the entire grid&lt;/li&gt;
          &lt;li&gt;$p_c=1$ only when the central point of the object $\in$ the grid&lt;/li&gt;
          &lt;li&gt;target output: $Y.\text{shape}=n_{\text{grid}}\times n_{\text{grid}}\times y.\text{length}$&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Non-Max Suppression&lt;/strong&gt;: what happens when the grid is too small to capture the entire object?
        &lt;center&gt;&lt;img src=&quot;../../images/DL/nms.jpg&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

        &lt;ol&gt;
          &lt;li&gt;Discard all boxes with $p_c\leq 0.6$&lt;/li&gt;
          &lt;li&gt;Pick the box with the largest $p_c$ as the prediction&lt;/li&gt;
          &lt;li&gt;Discard any remaining box with $\text{IoU}\geq 0.5$ with the prediction&lt;/li&gt;
          &lt;li&gt;Repeat till there is only one box left.&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Anchor Boxes&lt;/strong&gt;: what happens when two objects overlap? (e.g. a hot girl standing in front of a car)
        &lt;center&gt;&lt;img src=&quot;../../images/DL/anchor.jpg&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;

        &lt;ol&gt;
          &lt;li&gt;Predefine Anchor boxes for different objects&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Redefine the target value as a combination of Anchor 1 + Anchor 2&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
 y=\begin{bmatrix}
 p_{c1} \\
 \vdots \\ 
 p_{c2} \\
 \vdots 
 \end{bmatrix}
 \end{equation}&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;Each object in the image is assigned to grid cell that contains object’s central point &amp;amp; anchor box for the grid cell with the highest $\text{IoU}$&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;General Procedure&lt;/strong&gt;:&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;Divide the images into grids and label the objects&lt;/li&gt;
          &lt;li&gt;Train the CNN&lt;/li&gt;
          &lt;li&gt;Get the prediction for each anchor box in each grid cell&lt;/li&gt;
          &lt;li&gt;Get rid of low probability predictions&lt;/li&gt;
          &lt;li&gt;Get final predictions through non-max suppression for each class&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;rcnn&quot;&gt;&lt;/a&gt;&lt;strong&gt;R-CNN&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;TO BE CONTINUED&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&quot;face-recognition&quot;&gt;&lt;a name=&quot;fr&quot;&gt;&lt;/a&gt;Face Recognition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Face Verification vs Face Recognition
    &lt;ul&gt;
      &lt;li&gt;Verification
        &lt;ul&gt;
          &lt;li&gt;Input image, name/ID&lt;/li&gt;
          &lt;li&gt;Output whether the input image is that of the claimed person (1:1)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Recognition
        &lt;ul&gt;
          &lt;li&gt;Input image&lt;/li&gt;
          &lt;li&gt;Output name/ID if the image is any of the $K$ ppl in the database (1:K)&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a name=&quot;sn&quot;&gt;&lt;/a&gt;&lt;strong&gt;Siamese Network&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;One Shot Learning&lt;/strong&gt;: learn a similarity function&lt;/p&gt;

        &lt;p&gt;The major difference between normal image classification and face recognition is that we don’t have enough training examples. Therefore, rather than learning image classification, we&lt;/p&gt;

        &lt;ol&gt;
          &lt;li&gt;Calculate the degree of diff between the imgs as $d$&lt;/li&gt;
          &lt;li&gt;If $d\leq\tau$: same person; If $d&amp;gt;\tau$: diff person&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Preparation &amp;amp; Objective:
        &lt;ul&gt;
          &lt;li&gt;Encode $x^{(i)}$ as $f(x^{(i)})$ (defined by the params of the NN)&lt;/li&gt;
          &lt;li&gt;Compute $d(x^{(i)},x^{(j)})=\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$
            &lt;ul&gt;
              &lt;li&gt;i.e. distance between the two encoding vectors&lt;/li&gt;
              &lt;li&gt;if $x^{(i)},x^{(j)}$ are the same person, $\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$ is small&lt;/li&gt;
              &lt;li&gt;if $x^{(i)},x^{(j)}$ are different people,  $\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$ is large&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Method 1: &lt;a name=&quot;tl&quot;&gt;&lt;/a&gt;Triplet Loss&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;u&gt;Learning Objective&lt;/u&gt;: distinguish between Anchor image &amp;amp; Positive/Negative images (i.e. &lt;strong&gt;A vs P / A vs N&lt;/strong&gt;)&lt;/p&gt;

            &lt;ol&gt;
              &lt;li&gt;
                &lt;p&gt;&lt;u&gt;Initial Objective&lt;/u&gt;: $\left\lVert{f(A)-f(P)}\right\lVert_ 2^2 \leq \left\lVert{f(A)-f(N)}\right\lVert_ 2^2$&lt;/p&gt;

                &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;: We want to make sure the difference of A vs P is smaller than the difference of A vs N, so that this Anchor image is classified as positive (i.e. recognized)&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;&lt;u&gt;Problem&lt;/u&gt;: $\exists\ “0-0\leq0”$, in which case we can’t tell any difference&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;&lt;u&gt;Final Objective&lt;/u&gt;: $\left\lVert{f(A)-f(P)}\right\lVert_ 2^2-\left\lVert{f(A)-f(N)}\right\lVert_ 2^2+\alpha\leq0$&lt;/p&gt;

                &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;: We apply a margin $\alpha$ to solve the problem and meanwhile make sure “A vs N” is significantly larger than “A vs P”&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;u&gt;Loss Function&lt;/u&gt;:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{L}(A,P,N)=\max{(\left\lVert{f(A)-f(P)}\right\lVert_ 2^2-\left\lVert{f(A)-f(N)}\right\lVert_ 2^2+\alpha, 0)}
  \end{equation}&lt;/script&gt;

            &lt;ul&gt;
              &lt;li&gt;&lt;u&gt;Intuition&lt;/u&gt;: As long as this thing is less than 0, the loss is 0 and that’s a successful recognition!&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;u&gt;Training Process&lt;/u&gt;:&lt;/p&gt;
            &lt;ul&gt;
              &lt;li&gt;Given 10k imgs of 1k ppl: use the 10k images to generate triplets $A^{(i)}, P^{(i)}, N^{(i)}$&lt;/li&gt;
              &lt;li&gt;Make sure to have multiple imgs of the same person in the training set&lt;/li&gt;
              &lt;li&gt;
                &lt;strike&gt;random choosing&lt;/strike&gt;
              &lt;/li&gt;
              &lt;li&gt;Choose triplets that are quite “hard” to train on&lt;/li&gt;
            &lt;/ul&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/andrew.png&quot; width=&quot;300&quot; /&gt;&lt;/center&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Method 2: &lt;a name=&quot;bc&quot;&gt;&lt;/a&gt;Binary Classification&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;u&gt;Learning Objective&lt;/u&gt;: Check if two imgs represent the same person or diff ppl
            &lt;ul&gt;
              &lt;li&gt;$y=1$: same person&lt;/li&gt;
              &lt;li&gt;$y=0$: diff ppl&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;u&gt;Training output&lt;/u&gt;:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \hat{y}=\sigma\Bigg(\sum_{k=1}^{128}{w_i \Big|f(x^{(i)})_ k-f(x^{(j)})_ k\Big|+b}\Bigg)
  \end{equation}&lt;/script&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/binary.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

            &lt;ul&gt;
              &lt;li&gt;Precompute the output vectors $f(x^{(i)})\ \&amp;amp;\ f(x^{(j)})$ so that you don’t have to compute them again during each training process&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;nst&quot;&gt;&lt;/a&gt;&lt;strong&gt;Neural Style Transfer&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Intuition&lt;/u&gt;: &lt;strong&gt;Content(C) + Style(S) = Generated Image(G)&lt;/strong&gt;&lt;/p&gt;

        &lt;center&gt;&lt;img src=&quot;../../images/DL/csg.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
        &lt;center&gt;Combine Content image with Style image to Generate a brand new image&lt;/center&gt;
        &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;u&gt;Cost Function&lt;/u&gt;:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{J}(G)=\alpha\mathcal{J}_ \text{content}(C,G)+\beta\mathcal{J}_ \text{style}(S,G)
  \end{equation}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;$\mathcal{J}$: the diff between C/S and G&lt;/li&gt;
          &lt;li&gt;$\alpha,\beta$: weight params&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Style: correlation between activations across channels&lt;/p&gt;

            &lt;center&gt;&lt;img src=&quot;../../images/DL/corr.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

            &lt;p&gt;When there is some pattern in one patch, and there is another pattern that changes similarly in the other patch, they are &lt;strong&gt;correlated&lt;/strong&gt;.&lt;/p&gt;

            &lt;p&gt;e.g. vertical texture in one patch $\leftrightarrow$ orange color in another patch&lt;/p&gt;

            &lt;p&gt;The more often they occur together, the more correlated they are.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Content Cost Function:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{J}_ \text{content}(C,G)=\frac{1}{2}\left\lVert{a^{[l](C)}-a^{[1](G)}}\right\lVert^2
  \end{equation}&lt;/script&gt;

            &lt;ul&gt;
              &lt;li&gt;Use hidden layer $l$ to compute content cost&lt;/li&gt;
              &lt;li&gt;Use pre-trained CNN (e.g. VGG)&lt;/li&gt;
              &lt;li&gt;If $a^{[l](C)}\ \&amp;amp;\ a^{[l](G)}$ are similar, then both imgs have similar content&lt;br /&gt;
  &lt;br /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Style Cost Function:&lt;/p&gt;

            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{J}_ \text{style}(S,G)=\sum_l{\lambda^{[l]}\mathcal{J}_ \text{style}^{[l]}(S,G)}
  \end{equation}&lt;/script&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Style Cost per layer:&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathcal{J}^{[l]}_ \text{style}(S,G)=\frac{1}{(2n_h^{[l]}n_w^{[l]}n_c^{[l]})^2}\left\lVert{G^{[l](S)}-G^{[1](G)}}\right\lVert^2_F
  \end{equation}&lt;/script&gt;

                &lt;ul&gt;
                  &lt;li&gt;the first term is simply a normalization param&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Style Matrix:&lt;/p&gt;

                &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  G_{kk'}^{[l]}=\sum_{i=1}^{n_H^{[l]}}{\sum_{j=1}^{n_W^{[l]}}{a_{i,j,k}^{[l]}\cdot a_{i,j,k'}^{[l]}}}
  \end{equation}&lt;/script&gt;

                &lt;ul&gt;
                  &lt;li&gt;$a_{i,j,k}^{[l]}$: activation at height $i$, width $j$, channel $k$&lt;/li&gt;
                  &lt;li&gt;$G^{[l]}.\text{shape}=n_c^{[l]}\times n_c^{[l]}$&lt;/li&gt;
                  &lt;li&gt;&lt;u&gt;Intuition&lt;/u&gt;: sum up the multiplication of the two activations on the same cell in two different channels&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Training Process:&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;Intialize $G$ randomly (e.g. 100 x 100 x 3)&lt;/li&gt;
              &lt;li&gt;Use GD to minimize $\mathcal{J}(G)$: $G := G-\frac{\partial{\mathcal{J}(G)}}{\partial{G}}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Claim: some of the images in this session are cited from Andrew Ng’s Deep Learning specialization.</summary></entry></feed>