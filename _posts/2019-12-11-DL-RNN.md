---
layout: post
title: "Recurrent Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/RNN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim: some of the images in this session are cited from Andrew Ng's <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning</a> specialization.

Roadmap:
- Basics of RNN
    - [Intuition of Sequence Models](#intsm)
    - [Intuition of RNN](#intrnn)
    - [RNN Types](#layers)
    - [Language Model](#lm)
- RNN Variations
    - [GRU](#lenet)
    - [LSTM](#alexnet)
    - [Bidirectional RNN](#vgg)
    - [Deep RNN]
- Word Embeddings
    - Word Embeddings
    - Learning 1: Word2Vec
    - Learning 2: Negative Sampling
    - Learning 3: GloVe
    - Sentiment Classification
- Sequence Modeling
    - Sequence to Sequence
    - Beam Search
    - Attention Model
    - Example: Speech Recognition & Trigger Word Detection

&emsp;
## Basics of RNN

<a name="intsm"></a>**Intuition of Sequence Models**  

These are called sequence modeling:

- Speech recognition
- Music generation
- Sentiment classification
- DNA sequence analysis
- Machine translation
- Video activity recognition
- Name entity recognition
- ......

Forget about the tedious definitions. As a basic intuition of what we are doing in sequence modeling, here is a very simple example:

- We have a sentence: "Pewdiepie and MrBeast are two of the greatest youtubers in human history."
- We want to know: where are the "names" in this sentence? (i.e. name entity recognition)
- We convert the input sentence into $X$: $x^{\langle 1 \rangle}x^{\langle 2 \rangle}...x^{\langle t \rangle}...x^{\langle 12 \rangle}$

    where $x^{\langle t \rangle}$ represents each word in the sentence.  
    
    But how does it represent a word? Notice that we used the capitalized $X$ for a single sentence. Actually, $X.\text{shape}=5000\times12$, and $x.\text{shape}=5000\times1$. Why?
    
    We first make a vocabulary list like $\text{list}=\[\text{a; and; ...; history; ...; MrBeast; ...}]$.
    
    Then, we convert each word into a one-hot vector representing the index of the word in the dictionary, e.g.:
    
    $$\begin{equation}
    x^{\langle 1 \rangle}=\begin{bmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{bmatrix}\longleftarrow 425,\ 
    x^{\langle 2 \rangle}=\begin{bmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{bmatrix}\longleftarrow 3578,\ \cdots\cdots
    \end{equation}$$
    
- We then label the output as $y: 1\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0$ and train our NN on this.

- Accordingly, we can use most of the sequences in our daily life as datasets and build our NN models on them to solve such ML problems.

<br/>
<a name="intrnn"></a>**Intuition of RNN**

We have very briefly mentioned that Conv1D can be used to scan through a sequence, extract features and make predictions. Then why don't we just stick to Conv1D or use normal ANNs?

1. The scope of sequence modeling is not necessarily recognition or classification, meaning that our inputs & outputs can be in very diff lengths for diff examples. 
2. Neither ANNs nor CNNs share features learned across diff positions of a text or a sequence, whereas context matters quite a lot in most sequence modeling problems.

Therefore, we need to define a brand new NN structure that can perfectly align with sequence modeling - RNN:

<center><img src="../../images/DL/rnn.jpg" height="200"/></center>
<br/>
Forward propagation:
- $a^{\langle 0 \rangle}=\textbf{0}$
- $a^{\langle t \rangle}=g(W_{a}\[a^{\langle t-1 \rangle}; x^{\langle t \rangle}]+b_a)\ \ \ \ \|\ g:\ \text{tanh/ReLU}$  
    
    where $W_a=\[W_{aa}\ W_{ax}]$ with a shape of $(100,10100)$ if we assume a dictionary of 10000 words (i.e. $x^{\langle t \rangle}.\text{shape}=(10000,100)$) and the activation length of 100.

- $\hat{y}^{\langle t \rangle}=g(W_{y}a^{\langle t \rangle}+b_y)\ \ \ \ \|\ g:\ \text{sigmoid}$  

Backward propagation:
- $\mathcal{L}^{\langle t \rangle}(\hat{y}^{\langle t \rangle},y^{\langle t \rangle})=-\sum_i{y_i^{\langle t \rangle}\log{\hat{y}_ i^{\langle t \rangle}}}\ \ \ \ \|\ $Same loss function as LogReg
    
<br/>
<a name="layers"></a>**RNN Types**

<center><img src="../../images/DL/rnntypes.png" width="550"/></center>  
<br/>
There is nothing much to explain here. The images are pretty clear.  
<br/>
<a name="lm"></a>**Language Model**

The core of RNN is to calculate the likelihood of a sequence: $P(y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t \rangle})$ and output the one with the highest probability.

For example, the sequence "<u>the apple and pair salad</u>" has a much smaller possibility to occur than the sequence "<u>the apple and pear salad</u>". Therefore, RNN will output the latter. This seems much like **Softmax**, and it actually is. 

Recall from the formula of conditional probability, we can separate the likelihood into:

$$\begin{equation}
P\big(y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t \rangle}\big)=P\big(y^{\langle 1 \rangle}\big)P\big(y^{\langle 2 \rangle}|y^{\langle 1 \rangle}\big)...P\big(y^{\langle t \rangle}|y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t-1 \rangle}\big)
\end{equation}$$

For example, to generate the sentence "I like cats.", we calculate:

$$\begin{equation}
P\big(\text{"I like cats"}\big)=P\big(\text{"I"}\big)P\big(\text{"like"}|\text{"I"}\big)P\big(\text{"cats"}|\text{"I like"}\big)
\end{equation}$$

