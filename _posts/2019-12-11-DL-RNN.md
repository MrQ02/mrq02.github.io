---
layout: post
title: "Recurrent Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/RNN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim: some of the images in this session are cited from Andrew Ng's <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning</a> specialization, but most are created by myself.

Roadmap:
- Basics of RNN
    - [Intuition of Sequence Models](#intsm)
    - [Intuition of RNN](#intrnn)
    - [RNN Types](#layers)
    - [Language Model](#lm)
- [RNN Variations](#var)
    - [GRU](#gru)
    - [LSTM](#lstm)
    - [Bidirectional RNN](#birnn)
    - [Deep RNN](#drnn)
- Word Embeddings
    - Word Embeddings
    - Learning 1: Word2Vec
    - Learning 2: Negative Sampling
    - Learning 3: GloVe
    - Sentiment Classification
- Sequence Modeling
    - Sequence to Sequence
    - Beam Search
    - Attention Model
    - Example: Speech Recognition & Trigger Word Detection

&emsp;
## Basics of RNN

<a name="intsm"></a>
### <strong>Intuition of Sequence Models</strong>  

These are called sequence modeling:

- Speech recognition
- Music generation
- Sentiment classification
- DNA sequence analysis
- Machine translation
- Video activity recognition
- Name entity recognition
- ......

Forget about the tedious definitions. As a basic intuition of what we are doing in sequence modeling, here is a very simple example:

- We have a sentence: "Pewdiepie and MrBeast are two of the greatest youtubers in human history."
- We want to know: where are the "names" in this sentence? (i.e. name entity recognition)
- We convert the input sentence into $X$: $x^{\langle 1 \rangle}x^{\langle 2 \rangle}...x^{\langle t \rangle}...x^{\langle 12 \rangle}$

    where $x^{\langle t \rangle}$ represents each word in the sentence.  
    
    But how does it represent a word? Notice that we used the capitalized $X$ for a single sentence. Actually, $X.\text{shape}=5000\times12$, and $x.\text{shape}=5000\times1$. Why?
    
    We first make a vocabulary list like $\text{list}=\[\text{a; and; ...; history; ...; MrBeast; ...}]$.
    
    Then, we convert each word into a one-hot vector representing the index of the word in the dictionary, e.g.:
    
    $$\begin{equation}
    x^{\langle 1 \rangle}=\begin{bmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{bmatrix}\longleftarrow 425,\ 
    x^{\langle 2 \rangle}=\begin{bmatrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{bmatrix}\longleftarrow 3578,\ \cdots\cdots
    \end{equation}$$
    
- We then label the output as $y: 1\ 0\ 1\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0\ 0$ and train our NN on this.

- Accordingly, we can use most of the sequences in our daily life as datasets and build our NN models on them to solve such ML problems.  

<br/><a name="intrnn"></a>
### <strong>Intuition of RNN</strong>

We have very briefly mentioned that Conv1D can be used to scan through a sequence, extract features and make predictions. Then why don't we just stick to Conv1D or use normal ANNs?

1. The scope of sequence modeling is not necessarily recognition or classification, meaning that our inputs & outputs can be in very diff lengths for diff examples. 
2. Neither ANNs nor CNNs share features learned across diff positions of a text or a sequence, whereas context matters quite a lot in most sequence modeling problems.

Therefore, we need to define a brand new NN structure that can perfectly align with sequence modeling - RNN:

<center><img src="../../images/DL/rnn.jpg" height="200"/></center>
<br/>
Forward propagation:
- $a^{\langle 0 \rangle}=\textbf{0}$
- $a^{\langle t \rangle}=g(W_{a}\[a^{\langle t-1 \rangle}; x^{\langle t \rangle}]+b_a)\ \ \ \ \|\ g:\ \text{tanh/ReLU}$  
    
    where $W_a=\[W_{aa}\ W_{ax}]$ with a shape of $(100,10100)$ if we assume a dictionary of 10000 words (i.e. $x^{\langle t \rangle}.\text{shape}=(10000,100)$) and the activation length of 100.

- $\hat{y}^{\langle t \rangle}=g(W_{y}a^{\langle t \rangle}+b_y)\ \ \ \ \|\ g:\ \text{sigmoid}$  

Backward propagation:
- $\mathcal{L}^{\langle t \rangle}(\hat{y}^{\langle t \rangle},y^{\langle t \rangle})=-\sum_i{y_i^{\langle t \rangle}\log{\hat{y}_ i^{\langle t \rangle}}}\ \ \ \ \|\ $Same loss function as LogReg  

<br/><a name="layers"></a>
### <strong>RNN Types</strong>

<center><img src="../../images/DL/rnntypes.png" width="550"/></center>  
<br/>
There is nothing much to explain here. The images are pretty clear.  
<br/><a name="lm"></a>
### <strong>Language Model</strong>

- <u>Intuition of Softmax & Conditional Probability</u>

    The core of RNN is to calculate the likelihood of a sequence: $P(y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t \rangle})$ and output the one with the highest probability.

    For example, the sequence "<u>the apple and pair salad</u>" has a much smaller possibility to occur than the sequence "<u>the apple and pear salad</u>". Therefore, RNN will output the latter. This seems much like **Softmax**, and indeed it is. 

    Recall from the formula of conditional probability, we can separate the likelihood into:

    $$\begin{equation}
    P\big(y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t \rangle}\big)=P\big(y^{\langle 1 \rangle}\big)P\big(y^{\langle 2 \rangle}|y^{\langle 1 \rangle}\big)...P\big(y^{\langle t \rangle}|y^{\langle 1 \rangle},y^{\langle 2 \rangle},...,y^{\langle t-1 \rangle}\big)
    \end{equation}$$

    For example, to generate the sentence "I like cats.", we calculate:

    $$\begin{equation}
    P\big(\text{"I like cats"}\big)=P\big(\text{"I"}\big)P\big(\text{"like"}|\text{"I"}\big)P\big(\text{"cats"}|\text{"I like"}\big)
    \end{equation}$$  

- <u>Language Modeling Procedure</u>  
    1. Data Preparation
        * Training set: large corpus of English text (or other languages)
        * **Tokenize**: mark every word into a token
            * \<EOS>: End of Sentence token
            * \<UNK>: Unknown word token
        * e.g. "I hate Minecraft and kids." $\Rightarrow$ "I hate \<UNK> and kids. \<EOS>"  
    <br/>
    2. Training
        <center><img src="../../images/DL/rnnlm.png" width="700"/></center>  
        <br/>
        We use the sentence "I hate Minecraft and kids. \<EOS>" as one training example.  
        At the beginning, we initialize $a^{<0>}$ and $x^{<1>}$ as $\vec{0}$ and let the RNN try to guess the first word.  
        At each step, we use the original word at the same index $y^{\<i-1>}$ and the previous activation $a^{\<i-1>}$ to let the RNN try to guess the next word $\hat{y}^{\<i>}$ from Softmax regression.  
        During the training process, we try to minimize the loss function $\mathcal{L}(\hat{y},y)$ to ensure the training is effective to predict the sentence correctly.
    <br/>
    3. Sequence Sampling
        <center><img src="../../images/DL/rnnsample.png" width="700"/></center>  
        <br>
        After the RNN is trained, we can use it to generate a sentence by itself. In each step, the RNN will take the previous word it generated $\hat{y}^{\<i-1>}$ as $x^{\<i>}$ to generate the next word $\hat{y}^{\<i>}$.
        
- <u>Character-level LM</u>
    - Dictionary
        - Normal LM: \[a, abandon, ..., zoo, \<UNK>]
        - Char-lv LM: \[a, b, c, ..., z]
    - Pros & Cons
        - Pros: never need to worry about unknown words \<UNK>
        - Cons: sequence becomes much much longer; the RNN doesn't really learn anything about the words.  
<br>
- <u>Problems with current RNN</u>  
    One of the most significant problems with our current simple RNN is **vanishing gradients**. As shown in the figures above, the next word always has a very strong dependency on the previous word, and the dependency between two words weakens as the distance between them gets longer. In other words, the current RNN are very bad at catching long-line dependencies, for example,
    
    <center>the <strong>cat</strong>, which already ......, <strong>was</strong> full.&nbsp;&nbsp;&nbsp;</center>
    <center>the <strong>cats</strong>, which already ......, <strong>were</strong> full.</center>
    <br>
    "be" verbs have high dependencies on the "subject", but RNN doesn't know that. Since the distance between these two words are too long, the gradient on the "subject" nouns would barely affect the training on the "be" verbs.

&emsp;<a name="var"></a>
## RNN Variations

| RNN | GRU | LSTM |
|:---:|:---:|:----:|
| <img src="../../images/DL/rnnblock.png" width="330"/> | <img src="../../images/DL/gru.png" width="330"/> | <img src="../../images/DL/lstm.png" width="330"/> |

As shown above, there are currently 3 most used RNN blocks. The original RNN block activates the linear combination of $a^{\<t-1>}$ and $x^{\<t>}$ with a $\text{tanh}$ function and then passes the output value onto the next block.

However, because of the previously mentioned problem with the original RNN, scholars have created some variations, such as GRU & LSTM.  

<br/><a name="gru"></a>
### <strong>GRU</strong> (Gated Recurrent Unit)

<center><img src="../../images/DL/gru.png" width="400"/></center>  
<br>
As the name implies, GRU is an advancement of normal RNN block with "gates". There are 2 gates in GRU:

- **R gate**: (Remember) determine whether to remember the previous cell
- **U gate**: (Update) determine whether to update the computation with the candidate

Computing process of GRU:

1. Compute R gate:

    $$\begin{equation}
    \Gamma_r=\sigma\big(w_r\big[a^{<t-1>};x^{<t>}\big]+b_r\big)
    \end{equation}$$

2. Compute U gate:

    $$\begin{equation}
    \Gamma_u=\sigma\big(w_u\big[a^{<t-1>};x^{<t>}\big]+b_u\big)
    \end{equation}$$
    
3. Compute Candidate:

    $$\begin{equation}
    \tilde{c}^{<t>}=\tanh{\big(w_c\big[\Gamma_r * a^{<t-1>};x^{<t>}\big]+b_c\big)}
    \end{equation}$$
    
    When $\Gamma_r=0$, $\tilde{c}^{\<t>}=\tanh{\big(w_cx^{\<t>}+b_c\big)}$, the previous word has no effect on the word choice of this cell.
    
4. Compute Memory Cell:

    $$\begin{equation}
    c^{<t>}=\Gamma_u \cdot \tilde{c}^{<t>} + (1-\Gamma_u) \cdot c^{<t-1>}
    \end{equation}$$
    
    When $\Gamma_u=1$, &emsp;$c^{\<t>}=\tilde{c}^{\<t>}$. The candidate updates.  
    When $\Gamma_u=0$, &emsp;$c^{\<t>}=c^{\<t-1>}$. The candidate does not update.
    
5. Output:

    $$\begin{equation}
    a^{<t>}=c^{<t>}
    \end{equation}$$ 

<br/><a name="lstm"></a>
### <strong>LSTM</strong> (Long Short-Term Memory)

<center><img src="../../images/DL/lstm.png" width="400"/></center>  
<br>
LSTM is an advancement of GRU. While GRU relatively saves more computing power, LSTM is more powerful. There are 3 gates in LSTM:

- **F gate**: (Forget) determine whether to forget the previous cell
- **U gate**: (Update) determine whether to update the computation with the candidate
- **O gate**: (Update) Compute the normal activation

Computing process of GRU:

1. Compute F gate:

    $$\begin{equation}
    \Gamma_f=\sigma\big(w_f\big[a^{<t-1>};x^{<t>}\big]+b_f\big)
    \end{equation}$$

2. Compute U gate:

    $$\begin{equation}
    \Gamma_u=\sigma\big(w_u\big[a^{<t-1>};x^{<t>}\big]+b_u\big)
    \end{equation}$$
    
3. Compute O gate:

    $$\begin{equation}
    \Gamma_o=\sigma\big(w_o\big[a^{<t-1>};x^{<t>}\big]+b_o\big)
    \end{equation}$$
    
4. Compute Candidate:

    $$\begin{equation}
    \tilde{c}^{<t>}=\tanh{\big(w_c\big[a^{<t-1>};x^{<t>}\big]+b_c\big)}
    \end{equation}$$
    
5. Compute Memory Cell:

    $$\begin{equation}
    c^{<t>}=\Gamma_u \cdot \tilde{c}^{<t>} + \Gamma_f \cdot c^{<t-1>}
    \end{equation}$$
    
6. Output:

    $$\begin{equation}
    a^{<t>}=\Gamma_o \cdot \tanh{c^{<t>}}
    \end{equation}$$
    
**Peephole Connection**: as shown in the formulae, the gate values $\Gamma \propto c^{\<t-1>}$, therefore, we can always include $c^{\<t-1>}$ into gate calculations to simplify the computing.

<br/><a name="birnn"></a>
### <strong>Bidirectional RNN</strong>

<u>Problem</u>: Sometimes, our choices of previous words are dependent on the latter words. For example,

<center><strong>Teddy</strong> Roosevelt was a nice president.</center>
<center><strong>Teddy</strong> bears are now on sale!!!&emsp;&emsp;&emsp;&nbsp;</center>  
<br>
The word "Teddy" represents two completely different things, but without the context from the latter part, we cannot determine what the "Teddy" stands for. (This example is cited from Andrew Ng's Coursera Specialization)  

<u>Solution</u>: We make the RNN bidirectional:

<center><img src="../../images/DL/birnn.png" height="250"/></center>  
<br>
Each output is calculated as: $\hat{y}^{\<t>}=g\Big(W_y\Big[\overrightarrow{a}^{\<t>};\overleftarrow{a}^{\<t>}\Big]+b_y\Big)$

<br/><a name="drnn"></a>
### <strong>Deep RNN</strong>

Don't be fascinated by the name. It's just stacks of RNN layers:

<center><img src="../../images/DL/drnn.png" width="700"/></center>  