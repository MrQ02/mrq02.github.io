---
layout: post
title: "Convolutional Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/CNN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim: some of the images in this session are cited from Andrew Ng's <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning</a> specialization.

Roadmap:
- Basics of CNN
    - [Intuition of CNN](#cnn)
    - [General Formula of Convolution](#formula)
    - [CNN Layers](#layers)
        - Convolution
        - [Pooling](#pool)
        - [Fully Connected](#fc)
- CNN Examples
    - [LeNet-5](#lenet)
    - [AlexNet](#alexnet)
    - [VGG](#vgg)
    - Inception (the most powerful CNN as far as I know)
        - [ResNets](#res) (not CNN but prerequisite for understanding Inception)
        - [1x1 Conv](#nin) (i.e. NiN)
        - [The Inception](#inception) (We need to go deeper!)
    - [Conv1D & Conv3D](#conv1d) (Yes, they exist.)
- [Object Detection](#od)
    - Bounding Box
    - Landmark Detection
    - Sliding Windows
    - Intersection over Union
    - [YOLO (You Only Look Once)](#yolo)
        - Non-Max Suppression
        - Anchor Boxes
    - [R-CNN](#rcnn) (will talk about this in the future)
- [Face Recognition](#fr)
    - [Siamese Network](#sn)
        - [Triplet Loss](#tl)
        - [Binary Classification](#bc)
    - [Neural Style Transfer](#nst)

&emsp;
## Basics of CNN

- <a name="cnn"></a>**Intuition of CNN**
    
    <center><img src="../../images/DL/cnn.gif" width="500"/></center>
    <br/>
    - CNN is mostly used in Computer Vision (image classification, object detection, neural style transfer, etc.)  
    
    - **Input**: images $\rightarrow$ volume of numerical values in the shape of **width $\times$ height $\times$ color-scale** (color-scale=3 $\rightarrow$ RGB; color-scale=1 $\rightarrow$ BW)  
    
        In the gif above, the input shape is $5\times5\times3$, meaning that the image is colored and the image size $5\times5$. The "$7\times7\times3$" results from **padding**, which will be discussed below.
    
    - **Convolution**: 
        1. For each color layer of the input image, we apply a 2d **filter** that **scans** through the layer in order.
        2. For each block that the filter scans, we **multiply** the corresponding filter value and the cell value, and we **sum** them up.
        3. We **sum** up the output values from all layers of the filter (and add a bias value to it) and **output** this value to the corresponding output cell. 
        4. (If there are multiple filters, ) After the first filter finishes scanning, the next filter starts scanning and outputs into a new layer.  
    <br/>
    - In the gif above, 
        1. Apply 2 filters of the shape $3\times3\times3$.
        2. 1st filter - 1st layer - 1st block: 
        
            $$\begin{equation}
            0+0+0+0+0+0+0+(1\times-1)+0=-1
            \end{equation}$$
            
            1st filter - 2nd layer - 1st block:
            
            $$\begin{equation}
            0+0+0+0+(2\times-1)+(1\times1)+0+(2\times1)+0=1
            \end{equation}$$
            
            1st filter - 3rd layer - 1st block:
            
            $$\begin{equation}
            0+0+0+0+(2\times1)+0+0+(1\times-1)+0=1
            \end{equation}$$
            
        3. Sum up + bias $\rightarrow$ 1st cell of 1st output layer
            
            $$\begin{equation}
            -1+1+1+1=2
            \end{equation}$$
    
        4. Repeat till we finish scanning  
<br/>
- **Edge Detection & Filter**

    - Sample filters
    
        <center><img src="../../images/DL/edgedetect.png" width="500"/></center>
        
        - Gray Scale: 1 = lighter, 0 = gray, -1 = darker  
    <br/>
    - Notice that we don't really need to define any filter values. Instead, we are supposed to train the filter values.  
    All the convolution operations above are just the same as the operations in ANN. Filters here correspond to $W$ in ANN.  
    
- **Padding**

    - Problem: corner cells & edge cells are detected much fewer times than the middle cells $\rightarrow$ info loss of corner & edge
    
    - Solution: pad the edges of the image with "0" cells (as shown in the gif above)
    
- **Stride**: the step size the filter takes ($s=2$ in the gif above)

- <a name="formula"></a>**General Formula of Convolution**: 

    $$\begin{equation}
    \text{Output Size}=\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor\times\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor
    \end{equation}$$
    
    - $n\times n$: image size
    - $f\times f$: filter size
    - $p$: padding
    - $s$: stride
    - Floor: ignore the computation when the filter sweeps the region outside the image matrix  
<br/>
- <a name="layers"></a>**CNN Layers**:

    - **Convolution** (CONV): as described above
    
    - <a name="pool"></a>**Pooling** (POOL): to reduce #params & computations (most common pooling size = $2\times2$)
    
        - Max Pooling
        
            <center><img src="../../images/DL/maxpool.png" height="200"/></center>
            
            1. Divide the matrix evenly into regions
            2. Take the max value in that region as output value  
            <br/>
        - Average Pooling
        
            <center><img src="../../images/DL/avgpool.png" height="190"/></center>
            
            1. Divide the matrix evenly into regions
            2. Take the average value of the cells in that region as output value  
            <br/>
        - Stochastic Pooling
        
            <center><img src="../../images/DL/stochasticpool.png" height="200"/></center>
            
            1. Divide the matrix evenly into regions
            2. Normalize each cell based on the regional sum:
            
                $$\begin{equation}
                p_i=\frac{a_i}{\sum_{k\in R_j}{a_k}}
                \end{equation}$$
                
            3. Take a random cell based on multinomial distribution as output value  
        <br/>
    - <a name="fc"></a>**Fully Connected** (FC): to flatten the 2D/3D matrices into a single vector (each neuron is connected with all input values)
    
        <center><img src="../../images/DL/fullyconnected.png" width="300"/></center>

&emsp;
## CNN Examples

<a name="lenet"></a>**LeNet-5**: LeNet-5 Digit Recognizer
        
<center><img src="../../images/DL/cnneg.png"/></center>  

|  Layer  |  Shape  | Total Size | #params |
| :-----: | :-----: | :--------: | :-----: |
| INPUT | 32 x 32 x 3 | 3072 | 0 |
| CONV1 (Layer 1) | 28 x 28 x 6 | 4704 | 156 |
| POOL1 (Layer 1) | 14 x 14 x 6 | 1176 | 0 |
| CONV2 (Layer 2) | 10 x 10 x 16 | 1600 | 416 |
| POOL2 (Layer 2) | 5 x 5 x 16 | 400 | 0 |
| FC3 (Layer 3) | 120 x 1 | 120 | 48001 |
| FC4 (Layer 4) | 84 x 1 | 84 | 10081 |
| Softmax | 10 x 1 | 10 | 841 |

- Calculation of #params for CONV: $(f\times f+1)\times n_f$
    - $f$: filter size
    - $+1$: bias
    - $n_f$: #filter
 
<br/>
<a name="alexnet"></a>**AlexNet**: winner of 2012 ImageNet Large Scale Visual Recognition Challenge  

<center><img src="../../images/DL/alexnet.png"/></center><br/>  
    
|  Layer  |  Shape  | Total Size | #params |
| :-----: | :-----: | :--------: | :-----: |
| INPUT | 227 x 227 x 3 | 154587 | 0 |
| CONV1 (Layer 1) | 55 x 55 x 96 | 290400 | 11712 |
| POOL1 (Layer 1) | 27 x 27 x 96 | 69984 | 0 |
| CONV2 (Layer 2) | 27 x 27 x 256 | 186624 | 6656 |
| POOL2 (Layer 2) | 13 x 13 x 256 | 43264 | 0 |
| CONV3 (Layer 3) | 13 x 13 x 384 | 64896 | 3840 |
| CONV4 (Layer 3) | 13 x 13 x 384 | 64896 | 3840 |
| CONV5 (Layer 3) | 13 x 13 x 256 | 43264 | 2560 |
| POOL5 (Layer 3) | 6 x 6 x 256 | 9216 | 0 |
| FC5 (Flatten) | 9216 x 1 | 9216 | 0 |
| FC6 (Layer 4) | 4096 x 1 | 4096 | 37748737 |
| FC7 (Layer 5) | 4096 x 1 | 4096 | 16777217 |
| Softmax | 1000 x 1 | 1000 | 4096000 |

- Significantly bigger than LeNet-5 (60M params to be trained)
- Require multiple GPUs to speed the training up<br/><br/>  
    
<a name="vgg"></a>**VGG**: made by Visual Geometry Group from Oxford  

<center><img src="../../images/DL/vgg.png"/></center>  

- Too large: 138M params<br/><br/>  

**Inception**  

- <a name="res"></a>**ResNets** 

    - Residual Block

        <center><img src="../../images/DL/resnet.png" width="500"/></center>

        $$\begin{equation}
        a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
        \end{equation}$$
    
        Intuition: we add activation values from layer $l$ to the activation in layer $l+2$ 

    - Why ResNets?
    
        - ResNets allow parametrization for the identity function $f(x)=x$
        - ResNets are proven to be more effective than plain networks:
        
            <center><img src="../../images/DL/resnetperf.png" width="500"/></center>
            
        - ResNets add more complexity to the NN in a very simple way
        - The idea of ResNets further inspired the development of RNN  
<br/>
- <a name="nin"></a>**1x1 Conv** (i.e. Network in Network [NiN])  

    - WHY??? This sounds like the stupidest idea ever!!
    - Watch this.
        
        <center><img src="../../images/DL/1x1pt1.png" height="300"/></center><br/>
        
        <center>In a normal CNN layer like this, we need to do in total 210M calculations.</center><br/>
        
        <center><img src="../../images/DL/1x1pt2.png" height="300"/></center><br/>
    
        <center>However, if we add a 1x1 Conv layer in between, we only need to do in total 17M calculations.</center><br/>
    
    - Therefore, 1x1 Conv is significantly more useful than what newbies expect. When we would like to keep the matrix size but reduce #layers, using 1x1 Conv can significantly reduce #computations needed, thus requiring less computing power.  
<br/>
- <a name="inception"></a>**The Inception**: We need to go deeper!

    - Inception Module
    
        <center><img src="../../images/DL/incepm.png" width="400"/></center><br/>
        
    - Inception Network
    
        <center><img src="../../images/DL/incep.png" width="500"/></center>     

<a name="conv1d"></a>**Conv1D & Conv3D**:

Although CNN (Conv2D) is undoubtedly most useful in Computer Vision, there are also some other forms of CNN used in other fields:

- **Conv1D**: e.g. text classification, heartbeat detection, ...

    <center><img src="../../images/DL/conv1d.png" width="400"/></center>
    
    - use a 1D filter to convolve a 1D input vector
    - e.g. $14\times1\xrightarrow{5\times1,16}10\times16\xrightarrow{5\times16,32}6\times32$
    - However, this is almost never used since we have **RNN**  
<br/>
- **Conv3D**: e.g. CT scan, ...

    <center><img src="../../images/DL/conv3d.png" width="400"/></center>
    
    - use a 3D filter to convolve a 3D input cube
    - e.g. $14\times14\times14\times1\xrightarrow{5\times5\times5\times1,16}10\times10\times10\times16\xrightarrow{5\times5\times5\times16,32}6\times6\times6\times32$

&emsp;
## <a name="od"></a>Object Detection

- Object Localization $\rightarrow$ 1 obj; Detection $\rightarrow$ multiple objs.

- **Bounding Box**: to capture the obj in the img with a box
    - Params: 
        - $b_x, b_y$ = central point
        - $b_h, b_w$ = full height/width
    - New target label (in place of image classification output):
        
        $$\begin{equation}
        y=\begin{bmatrix}
        p_c \\ b_x \\ b_y \\ b_h \\ b_w \\ c_1 \\ \vdots \\ c_n
        \end{bmatrix}
        \end{equation}$$
    
        - $p_c$: "is there any object in this box?"
            - if $p_c=0$, we ignore the remaining params
        - $c_i$: class label $i$ (e.g. $c_1$: cat, $c_2$: dog, $c_3$: bird, ...)  
<br/>      
- **Landmark Detection**: to capture the obj in the img with points
    - Params: $(l_{ix},l_{iy})$ = each landmark point
    - New target label:
    
        $$\begin{equation}
        y=\begin{bmatrix}
        p_c \\ l_{1x} \\ l_{1y} \\ \vdots \\ l_{nx} \\ l_{ny} \\ c_1 \\ \vdots \\ c_n
        \end{bmatrix}
        \end{equation}$$
        
    - THE LABELS MUST BE CONSISTENT!
        - Always start from the exact same location of the object! (e.g. if you start with the left corner of the left eye for one image, you should always start with the left corner of the left eye for all images.)
        - #landmarks should be the same! 

    <br/>
    I personally have a very awful experience with Landmark Detection. When the algorithms of object detection were not yet well-known in the IT industry, I worked on a project of digital screen defects detection in a Finnish company. Since digital screen defects are 1) black & white 2) in very simple geometric shapes, the usage of bounding boxes could have significantly reduced the complexity of both data collection and NN model building.<br/>  
    However, the team insisted to use landmark detection. Due to 1) that screen defects are unstructured 2) that the number of landmark points for two different screen defects can hardly be the same, the dataset was basically unusable, and none of the models we built could learn accurate patterns from it, leading to an unfortunate failure.<br/>  
    I personally would argue that bounding box is much better than landmark detection in most practical cases.
    
- **Sliding Window**

    <center><img src="../../images/DL/sliding.gif" width="500"/></center><br/>
    
    - Apply a sliding window with a fixed size to scan every part of the img left-right and top-bottom (just like CONV), and feed each part to CNN
    - In order to capture the same type of objects in different sizes and positions in the img, shrink the img (i.e. enlarge the sliding window) and scan again, and repeat.<br/>

    - Problem: HUGE computational cost!
    - Solution: (contemporary)
        1. Convert FC layer into CONV layer
        
            <center><img src="../../images/DL/slidingfc.jpg" width="700"/></center><br/>
        
        2. Share the former FC info with latter convolutions
        
            <center><img src="../../images/DL/sliding.png" width="700"/></center><br/>
            
            1. First run of the CNN.
            2. Second run of the same CNN with a bigger size of the same img (due to sliding window). Notice that the FC info from the first run is shared in the second run.
            3. Latter runs of the same CNN with bigger sizes of the same img (due to sliding window). Notice that the FC info from all previous runs is shared in this run, thus saving computation power and memories.  
<br/>           
- **Intersection over Union**

    <center><img src="../../images/DL/iou.png" width="200"/></center>  
    <center>Is the purple box a good prediction of the car location?</center>
    
    Intersection over Union is defined as:
    
    $$\begin{equation}
    \text{IoU}=\frac{\text{area of intersection}}{\text{area of union}}
    \end{equation}$$
    
    In this case, area of intersection is the intersection between the red and purple box, and area of union is the total area covered by the red and purple box.  
    If $\text{IoU}\leq 0.5$, then the prediction box is correct. (Other threshold values are also okay but 0.5 is conventional.)
    
- <a name="yolo"></a>**YOLO (You Only Look Once)**

    <center><img src="../../images/DL/yolo.jpg" width="300"/></center>

    - **Grids**: divide the image into grids & use each grid as a bounding box
        - when $p_c=0$, we ignore the entire grid
        - $p_c=1$ only when the central point of the object $\in$ the grid
        - target output: $Y.\text{shape}=n_{\text{grid}}\times n_{\text{grid}}\times y.\text{length}$  
    <br/>
    - **Non-Max Suppression**: what happens when the grid is too small to capture the entire object?
        <center><img src="../../images/DL/nms.jpg" width="500"/></center>
        
        1. Discard all boxes with $p_c\leq 0.6$
        2. Pick the box with the largest $p_c$ as the prediction
        3. Discard any remaining box with $\text{IoU}\geq 0.5$ with the prediction
        4. Repeat till there is only one box left.  
    <br/>
    - **Anchor Boxes**: what happens when two objects overlap? (e.g. a hot girl standing in front of a car)
        <center><img src="../../images/DL/anchor.jpg" width="300"/></center>
        
        1. Predefine Anchor boxes for different objects
        2. Redefine the target value as a combination of Anchor 1 + Anchor 2
        
            $$\begin{equation}
            y=\begin{bmatrix}
            p_{c1} \\
            \vdots \\ 
            p_{c2} \\
            \vdots 
            \end{bmatrix}
            \end{equation}$$
        
        3. Each object in the image is assigned to grid cell that contains object's central point & anchor box for the grid cell with the highest $\text{IoU}$  
    <br/>   
    - **General Procedure**:
    
        1. Divide the images into grids and label the objects
        2. Train the CNN
        3. Get the prediction for each anchor box in each grid cell
        4. Get rid of low probability predictions
        5. Get final predictions through non-max suppression for each class  
<br/>
- <a name="rcnn"></a>**R-CNN**

    TO BE CONTINUED

&emsp;
## <a name="fr"></a>Face Recognition

- Face Verification vs Face Recognition
    - Verification
        - Input image, name/ID
        - Output whether the input image is that of the claimed person (1:1)
    - Recognition
        - Input image
        - Output name/ID if the image is any of the $K$ ppl in the database (1:K)  
<br/>
- <a name="sn"></a>**Siamese Network**

    - **One Shot Learning**: learn a similarity function

        The major difference between normal image classification and face recognition is that we don't have enough training examples. Therefore, rather than learning image classification, we

        1. Calculate the degree of diff between the imgs as $d$
        2. If $d\leq\tau$: same person; If $d>\tau$: diff person  
    <br/>
    - Preparation & Objective:
        - Encode $x^{(i)}$ as $f(x^{(i)})$ (defined by the params of the NN)
        - Compute $d(x^{(i)},x^{(j)})=\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$            
            - i.e. distance between the two encoding vectors
            - if $x^{(i)},x^{(j)}$ are the same person, $\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$ is small
            - if $x^{(i)},x^{(j)}$ are different people,&nbsp; $\left\lVert{f(x^{(i)})-f(x^{(j)})}\right\lVert_ 2^2$ is large  
    <br/>
    - **Method 1: <a name="tl"></a>Triplet Loss** 
        - <u>Learning Objective</u>: distinguish between Anchor image & Positive/Negative images (i.e. **A vs P / A vs N**)
        
            1. <u>Initial Objective</u>: $\left\lVert{f(A)-f(P)}\right\lVert_ 2^2 \leq \left\lVert{f(A)-f(N)}\right\lVert_ 2^2$  
            
                <u>Intuition</u>: We want to make sure the difference of A vs P is smaller than the difference of A vs N, so that this Anchor image is classified as positive (i.e. recognized)
                
            2. <u>Problem</u>: $\exists\ "0-0\leq0"$, in which case we can't tell any difference
            
            3. <u>Final Objective</u>: $\left\lVert{f(A)-f(P)}\right\lVert_ 2^2-\left\lVert{f(A)-f(N)}\right\lVert_ 2^2+\alpha\leq0$  
            
                <u>Intuition</u>: We apply a margin $\alpha$ to solve the problem and meanwhile make sure "A vs N" is significantly larger than "A vs P"  
            
        - <u>Loss Function</u>:
        
            $$\begin{equation}
            \mathcal{L}(A,P,N)=\max{(\left\lVert{f(A)-f(P)}\right\lVert_ 2^2-\left\lVert{f(A)-f(N)}\right\lVert_ 2^2+\alpha, 0)}
            \end{equation}$$
        
            - <u>Intuition</u>: As long as this thing is less than 0, the loss is 0 and that's a successful recognition!  
        <br/>
        - <u>Training Process</u>:
            - Given 10k imgs of 1k ppl: use the 10k images to generate triplets $A^{(i)}, P^{(i)}, N^{(i)}$
            - Make sure to have multiple imgs of the same person in the training set
            - <strike>random choosing</strike>
            - Choose triplets that are quite "hard" to train on
        
            <center><img src="../../images/DL/andrew.png" width="300"/></center>
            
    - **Method 2: <a name="bc"></a>Binary Classification**
    
        - <u>Learning Objective</u>: Check if two imgs represent the same person or diff ppl
            - $y=1$: same person
            - $y=0$: diff ppl
            
        - <u>Training output</u>:
        
            $$\begin{equation}
            \hat{y}=\sigma\Bigg(\sum_{k=1}^{128}{w_i \Big|f(x^{(i)})_ k-f(x^{(j)})_ k\Big|+b}\Bigg)
            \end{equation}$$
        
            <center><img src="../../images/DL/binary.png" width="500"/></center>
        
            - Precompute the output vectors $f(x^{(i)})\ \&\ f(x^{(j)})$ so that you don't have to compute them again during each training process  
<br/>
- <a name="nst"></a>**Neural Style Transfer**
    - <u>Intuition</u>: **Content(C) + Style(S) = Generated Image(G)**
    
        <center><img src="../../images/DL/csg.png" width="500"/></center>
        <center>Combine Content image with Style image to Generate a brand new image</center>  
    <br/>
    - <u>Cost Function</u>: 
    
        $$\begin{equation}
        \mathcal{J}(G)=\alpha\mathcal{J}_ \text{content}(C,G)+\beta\mathcal{J}_ \text{style}(S,G)
        \end{equation}$$
        
        - $\mathcal{J}$: the diff between C/S and G
        - $\alpha,\beta$: weight params
        - Style: correlation between activations across channels
            
            <center><img src="../../images/DL/corr.png" width="500"/></center>
            
            When there is some pattern in one patch, and there is another pattern that changes similarly in the other patch, they are **correlated**.  
            
            e.g. vertical texture in one patch $\leftrightarrow$ orange color in another patch  
            
            The more often they occur together, the more correlated they are.
            
        - Content Cost Function:
        
            $$\begin{equation}
            \mathcal{J}_ \text{content}(C,G)=\frac{1}{2}\left\lVert{a^{[l](C)}-a^{[1](G)}}\right\lVert^2
            \end{equation}$$
            
            - Use hidden layer $l$ to compute content cost
            - Use pre-trained CNN (e.g. VGG)
            - If $a^{\[l\](C)}\ \&\ a^{\[l\](G)}$ are similar, then both imgs have similar content  
        <br/>
        - Style Cost Function:
        
            $$\begin{equation}
            \mathcal{J}_ \text{style}(S,G)=\sum_l{\lambda^{[l]}\mathcal{J}_ \text{style}^{[l]}(S,G)}
            \end{equation}$$
            
            - Style Cost per layer:
            
                $$\begin{equation}
                \mathcal{J}^{[l]}_ \text{style}(S,G)=\frac{1}{(2n_h^{[l]}n_w^{[l]}n_c^{[l]})^2}\left\lVert{G^{[l](S)}-G^{[1](G)}}\right\lVert^2_F
                \end{equation}$$
                
                - the first term is simply a normalization param 
            
            - Style Matrix:
            
                $$\begin{equation}
                G_{kk'}^{[l]}=\sum_{i=1}^{n_H^{[l]}}{\sum_{j=1}^{n_W^{[l]}}{a_{i,j,k}^{[l]}\cdot a_{i,j,k'}^{[l]}}}
                \end{equation}$$
            
                - $a_{i,j,k}^{\[l]}$: activation at height $i$, width $j$, channel $k$
                - $G^{\[l]}.\text{shape}=n_c^{\[l]}\times n_c^{\[l]}$
                - <u>Intuition</u>: sum up the multiplication of the two activations on the same cell in two different channels
                
        - Training Process:
        
            - Intialize $G$ randomly (e.g. 100 x 100 x 3)
            - Use GD to minimize $\mathcal{J}(G)$: $G := G-\frac{\partial{\mathcal{J}(G)}}{\partial{G}}$