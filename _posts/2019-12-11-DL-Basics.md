---
layout: post
title: "Basics of Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/ANN/
header-includes:
- \usepackage{amsmath}
---

- **Basics of NN**
    - Neural Network Representation
        - Input Matrix
        - Output Vector
        - Weight Matrix
        - Bias Vector
        - Linear Combination
        - Activation Vector
    - Activation Functions
    - Backward Propagation
    

## Neural Network Representation

<p align=center><img src="../../images/DL/NN.png" width="400"/></p>

**Input Matrix**:

$$\begin{equation}
X=\begin{bmatrix}
x_1^{(1)} & \cdots & x_1^{(m)} \\
\vdots & \ddots & \vdots \\
x_{n_x}^{(1)} & \cdots & x_{n_x}^{(m)}
\end{bmatrix}=\begin{bmatrix}
x^{(1)} & \cdots & x^{(m)}
\end{bmatrix}\quad\quad\quad X\in\mathbb{R}^{n_x\times m}
\end{equation}$$

- $x_j^{(i)}$: the $j$th feature of the $i$th training example
- $m$: # training examples: each column vector of $x$ represents one training example
- $n_x$: # input features: each row vector of $x$ represents one type of input feature

for easier understanding in this session, we use one training example / input vector at each training step:

$$\begin{equation}
x^{(i)}=\begin{bmatrix}
x_1^{(i)} \\ \vdots \\ x_{n_x}^{(i)}
\end{bmatrix}\quad\quad\quad x^{(i)}\in\mathbb{R}^{n_x}
\end{equation}$$

**Output Vector**:


$$\begin{equation}
\hat{Y}=\begin{bmatrix}
\hat{y}^{(1)} & \cdots & \hat{y}^{(m)}
\end{bmatrix}\quad\quad\quad \hat{Y}\in\mathbb{R}^{m}
\end{equation}$$

- $\hat{y}^{(i)}$: the predicted output value of the $i$th training example

for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the "$\hat{}$" symbol.

**Weight Matrix**:

$$\begin{equation}
W^{[k]}=\begin{bmatrix}
w_{1,1}^{[k]} & \cdots & w_{1,n_{k-1}}^{[k]} \\
\vdots & \ddots & \vdots \\
w_{n_k,1}^{[k]} & \cdots & w_{n_k,n_{k-1}}^{[k]}
\end{bmatrix}=\begin{bmatrix}
w_1^{[k]} \\ \vdots \\ w_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad W^{[k]}\in\mathbb{R}^{n_k\times n_{k-1}}
\end{equation}$$

- $w_{j,l}^{\[k\]}$: the weight value for the $l$th input at the $j$th node on the $k$th layer
- $n_k$: # nodes/neurons on the $k$th layer (the current layer)
- $n_{k-1}$: # nodes/neurons on the $k-1$th layer (the previous layer)

**Bias Vector**:

$$\begin{equation}
b^{[k]}=\begin{bmatrix}
b_1^{[k]} \\ \vdots \\ b_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad b^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

**Activation**:

$$\begin{equation}
a^{[k]}=\begin{bmatrix}
a_1^{[k]} \\ \vdots \\ a_{n_k}^{[k]}
\end{bmatrix}=\begin{bmatrix}
g(z_1^{[k]}) \\ \vdots \\ g(z_{n_k}^{[k]})
\end{bmatrix}\quad\quad\quad a^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $g(z)$: Activation function (to add **nonlinearity**)

**Linear Combination**:

$$\begin{equation}
z_j^{[k]}=w_j^{[k]}\cdot a^{[k-1]}+b_j^{[k]} \quad\quad\quad z_j^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $z_j^{\[k\]}$: the unactivated output value from the $j$th node of the $k$th layer

## Activation Functions


| Sigmoid        | Tanh           | ReLU  | Leaky ReLU |
|:-------------:|:-------------:|:-----:| :--------------: |
| $g(z)=\frac{1}{1+e^{-z}}$ | $g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$ | $g(z)=\max{(0,z)}$ | $g(z)=\max{(\varepsilon z,z)\ \|\ \varepsilon << 1}$ |
| col 2 is      | centered      |   $12 | try |
| zebra stripes | are neat      |    $1 | test |

