---
layout: post
title: "Basics of Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/ANN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---

- **Basics of NN**
    - [Neural Network Representation](#nn)
    - [Activation Functions](#af)
    - Training Process
        - [Forward Propagation](#fp)
        - [Backward Propagation](#bp)
    
&emsp;<a name="nn"></a>
## Neural Network Representation

<div align=center><img src="../../images/DL/NN.png" width="400"/></div>

**Input Matrix**:

$$\begin{equation}
X=\begin{bmatrix}
x_1^{(1)} & \cdots & x_1^{(m)} \\
\vdots & \ddots & \vdots \\
x_{n_x}^{(1)} & \cdots & x_{n_x}^{(m)}
\end{bmatrix}=\begin{bmatrix}
x^{(1)} & \cdots & x^{(m)}
\end{bmatrix}\quad\quad\quad X\in\mathbb{R}^{n_x\times m}
\end{equation}$$

- $x_j^{(i)}$: the $j$th feature of the $i$th training example
- $m$: # training examples: each column vector of $x$ represents one training example
- $n_x$: # input features: each row vector of $x$ represents one type of input feature

for easier understanding in this session, we use one training example / input vector at each training step:

$$\begin{equation}
x^{(i)}=\begin{bmatrix}
x_1^{(i)} \\ \vdots \\ x_{n_x}^{(i)}
\end{bmatrix}\quad\quad\quad x^{(i)}\in\mathbb{R}^{n_x}
\end{equation}$$

**Output Vector**:

$$\begin{equation}
\hat{Y}=\begin{bmatrix}
\hat{y}^{(1)} & \cdots & \hat{y}^{(m)}
\end{bmatrix}\quad\quad\quad \hat{Y}\in\mathbb{R}^{m}
\end{equation}$$

- $\hat{y}^{(i)}$: the predicted output value of the $i$th training example

for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the "$\hat{}$" symbol.

**Weight Matrix**:

$$\begin{equation}
W^{[k]}=\begin{bmatrix}
w_{1,1}^{[k]} & \cdots & w_{1,n_{k-1}}^{[k]} \\
\vdots & \ddots & \vdots \\
w_{n_k,1}^{[k]} & \cdots & w_{n_k,n_{k-1}}^{[k]}
\end{bmatrix}=\begin{bmatrix}
w_1^{[k]} \\ \vdots \\ w_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad W^{[k]}\in\mathbb{R}^{n_k\times n_{k-1}}
\end{equation}$$

- $w_{j,l}^{\[k\]}$: the weight value for the $l$th input at the $j$th node on the $k$th layer
- $n_k$: # nodes/neurons on the $k$th layer (the current layer)
- $n_{k-1}$: # nodes/neurons on the $k-1$th layer (the previous layer)

**Bias Vector**:

$$\begin{equation}
b^{[k]}=\begin{bmatrix}
b_1^{[k]} \\ \vdots \\ b_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad b^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

**Activation**:

$$\begin{equation}
a^{[k]}=\begin{bmatrix}
a_1^{[k]} \\ \vdots \\ a_{n_k}^{[k]}
\end{bmatrix}=\begin{bmatrix}
g(z_1^{[k]}) \\ \vdots \\ g(z_{n_k}^{[k]})
\end{bmatrix}\quad\quad\quad a^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $g(z)$: Activation function (to add **nonlinearity**)

**Linear Combination**:

$$\begin{equation}
z_j^{[k]}=w_j^{[k]}\cdot a^{[k-1]}+b_j^{[k]} \quad\quad\quad z_j^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $z_j^{\[k\]}$: the unactivated output value from the $j$th node of the $k$th layer

&emsp;<a name="af"></a>
## Activation Functions

<table>
    <thead>
        <tr align=center>
            <th>Sigmoid</th>
            <th>Tanh</th>
            <th>ReLU</th>
            <th>Leaky ReLU</th>
        </tr>
    </thead>
    <tbody align=center>
        <tr>
            <td>$g(z)=\frac{1}{1+e^{-z}}$</td>
            <td>$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$</td>
            <td>$g(z)=\max{(0,z)}$</td>
            <td>$g(z)=\max{(\varepsilon z,z)}$</td>
        </tr>
        <tr>
            <td><img src="../../images/DL/sigmoid.png" width="100"/></td>
            <td><img src="../../images/DL/tanh.png" width="100"/></td>
            <td><img src="../../images/DL/relu.png" width="100"/></td>
            <td><img src="../../images/DL/leakyrelu.png" width="100"/></td>
        </tr>
        <tr>
            <td><small>$g'(z)=g(z)\cdot (1-g(z))$</small></td>
            <td><small>$g'(z)=1-(g(z))^2$</small></td>
            <td><small>$$g'(z)=\begin{cases} 0&z<0 \\ 1&z>0\end{cases}$$</small></td>
            <td><small>$$g'(z)=\begin{cases} \varepsilon&z<0 \\ 1&z>0\end{cases}$$</small></td>
        </tr>
        <tr>
            <td><small>centered at $y=0.5$<br>$\Rightarrow$only good for binary classification</small></td>
            <td><small>centered at $y=0$<br>$\Rightarrow$better than sigmoid in many cases</small></td>
            <td colspan=2><small>used for cases other than binary classification<br>faster computing<br><strike>vanishing gradient</strike><br>model sparsity (some neurons can be inactivated)</small></td>
        </tr>
        <tr>
            <td colspan=2>$|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0$<br>$\Rightarrow$ vanishing gradient</td>
            <td>too many neurons get inactivated<br>$\Rightarrow$dying ReLU</td>
            <td>$\varepsilon$ usually set to 0.01<br><strike>dying ReLU</strike><br>widely used on Kaggle</td>
        </tr>
    </tbody>
</table>

- Why need activation funcs? To add nonlinearity.
    1. Suppose $g(z)=z$ (i.e. $\nexists g(z)$)
    2. $\Longrightarrow z^{[1]}=w^{[1]}x+b^{[1]}$
    3. $\Longrightarrow z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})=w'x+b'$
    4. This is just linear regression. Hidden layers exist for no reason.

&emsp;
## Training
<a name="fp"></a>
- **Forward Propagation**

$$
\begin{pmatrix} Z^{[1]}=W^{[1]}X+b^{[1]} \\ \cdots \end{pmatrix}
\Rightarrow
\begin{pmatrix} Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\ A^{[l]}=g^{[l]}(Z^{[l]}) \end{pmatrix}
\Rightarrow
\begin{pmatrix} \cdots \\ A^{[L]}=g^{[L]}(Z^{[L]})=\hat{Y} \end{pmatrix}
$$

- **Backward Propagation**

  <small>
$$
\begin{pmatrix} \cdots \\ dW^{[1]}=\frac{1}{m}dZ^{[1]}A^{[1]^T} \\ db^{[1]}=\frac{1}{m}\sum_{j=1}^{n}{dz_j^{[1]}} \end{pmatrix}
\Leftarrow
\begin{pmatrix} dZ^{[l-1]}=dW^{[l]^T}dZ^{[l]}g'^{[l]}(Z^{[l-1]}) \\ \cdots \\ \cdots \end{pmatrix}
\Leftarrow
\begin{pmatrix} dZ^{[L]}=\hat{Y}-Y \\ dW^{[L]}=\frac{1}{m}dZ^{[L]}A^{[L]^T} \\ db^{[L]}=\frac{1}{m}\sum_{j=1}^{n}{dz_j^{[L]}}\end{pmatrix}
$$
</small>
