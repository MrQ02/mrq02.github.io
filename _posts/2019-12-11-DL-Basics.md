---
layout: post
title: "Basics of Artificial Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/ANN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim:  
The dimensions used in this notebook are of my personal preference. They can be modified in whatever way you want as long as they are in consistency.

Roadmap:
- [Neural Network Representation](#nn)
- [Activation Functions](#af)
- Training
    - [Forward Propagation](#fp)
    - [Backward Propagation](#bp)
    - Example: [Forward & Backward Step: Stochastic](#fbss)
    - Example: [Forward & Backward Step: Mini-batch](#fbsb)
    - [Reverse Differentiation](#rd) (for a clearer understanding of backpropagation)
- [Gradient Descent](#gd)
    
&emsp;<a name="nn"></a>
## Neural Network Representation

<center><img src="../../images/DL/NN.png" width="400"/></center>

**Input Matrix**:

$$\begin{equation}
X=\begin{bmatrix}
x_1^{(1)} & \cdots & x_1^{(m)} \\
\vdots & \ddots & \vdots \\
x_{n_x}^{(1)} & \cdots & x_{n_x}^{(m)}
\end{bmatrix}=\begin{bmatrix}
x^{(1)} & \cdots & x^{(m)}
\end{bmatrix}\quad\quad\quad X\in\mathbb{R}^{n_x\times m}
\end{equation}$$

- $x_j^{(i)}$: the $j$th feature of the $i$th training example
- $m$: # training examples: each column vector of $x$ represents one training example
- $n_x$: # input features: each row vector of $x$ represents one type of input feature

for easier understanding in this session, we use one training example / input vector at each training step:

$$\begin{equation}
x^{(i)}=\begin{bmatrix}
x_1^{(i)} \\ \vdots \\ x_{n_x}^{(i)}
\end{bmatrix}\quad\quad\quad x^{(i)}\in\mathbb{R}^{n_x}
\end{equation}$$

**Output Vector**:

$$\begin{equation}
\hat{Y}=\begin{bmatrix}
\hat{y}^{(1)} & \cdots & \hat{y}^{(m)}
\end{bmatrix}\quad\quad\quad \hat{Y}\in\mathbb{R}^{m}
\end{equation}$$

- $\hat{y}^{(i)}$: the predicted output value of the $i$th training example

for easier understanding in this session, we assume that there is only one output value for each training example. The output vector in the training set is denoted without the "$\hat{}$" symbol.

**Weight Matrix**:

$$\begin{equation}
W^{[k]}=\begin{bmatrix}
w_{1,1}^{[k]} & \cdots & w_{1,n_{k-1}}^{[k]} \\
\vdots & \ddots & \vdots \\
w_{n_k,1}^{[k]} & \cdots & w_{n_k,n_{k-1}}^{[k]}
\end{bmatrix}=\begin{bmatrix}
w_1^{[k]} \\ \vdots \\ w_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad W^{[k]}\in\mathbb{R}^{n_k\times n_{k-1}}
\end{equation}$$

- $w_{j,l}^{\[k\]}$: the weight value for the $l$th input at the $j$th node on the $k$th layer
- $n_k$: # nodes/neurons on the $k$th layer (the current layer)
- $n_{k-1}$: # nodes/neurons on the $k-1$th layer (the previous layer)

**Bias Vector**:

$$\begin{equation}
b^{[k]}=\begin{bmatrix}
b_1^{[k]} \\ \vdots \\ b_{n_k}^{[k]}
\end{bmatrix}\quad\quad\quad b^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

**Linear Combination**:

$$\begin{equation}
z_j^{[k]}=w_j^{[k]}\cdot a^{[k-1]}+b_j^{[k]} \quad\quad\quad z_j^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $z_j^{\[k\]}$: the unactivated output value from the $j$th node of the $k$th layer

**Activation**:

$$\begin{equation}
a^{[k]}=\begin{bmatrix}
a_1^{[k]} \\ \vdots \\ a_{n_k}^{[k]}
\end{bmatrix}=\begin{bmatrix}
g(z_1^{[k]}) \\ \vdots \\ g(z_{n_k}^{[k]})
\end{bmatrix}\quad\quad\quad a^{[k]}\in\mathbb{R}^{n_k}
\end{equation}$$

- $g(z)$: Activation function (to add **nonlinearity**)

&emsp;<a name="af"></a>
## Activation Functions

(Blame github pages for not supporting colspan/rowspan)
<table>
    <thead>
        <tr style="text-align: center">
            <th>Sigmoid</th>
            <th>Tanh</th>
            <th>ReLU</th>
            <th>Leaky ReLU</th>
        </tr>
    </thead>
    <tbody style="text-align: center">
        <tr>
            <td>$g(z)=\frac{1}{1+e^{-z}}$</td>
            <td>$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$</td>
            <td>$g(z)=\max{(0,z)}$</td>
            <td>$g(z)=\max{(\varepsilon z,z)}$</td>
        </tr>
        <tr>
            <td><img src="../../images/DL/sigmoid.png" width="100"/></td>
            <td><img src="../../images/DL/tanh.png" width="100"/></td>
            <td><img src="../../images/DL/relu.png" width="100"/></td>
            <td><img src="../../images/DL/leakyrelu.png" width="100"/></td>
        </tr>
        <tr>
            <td><small>$g'(z)=g(z)\cdot (1-g(z))$</small></td>
            <td><small>$g'(z)=1-(g(z))^2$</small></td>
            <td><small>$$g'(z)=\begin{cases} 0&z<0 \\ 1&z>0\end{cases}$$</small></td>
            <td><small>$$g'(z)=\begin{cases} \varepsilon&z<0 \\ 1&z>0\end{cases}$$</small></td>
        </tr>
        <tr>
            <td><small>centered at $y=0.5$<br>$\Rightarrow$only good for binary classification</small></td>
            <td><small>centered at $y=0$<br>$\Rightarrow$better than sigmoid in many cases</small></td>
            <td><small>faster computing<br><strike>vanishing gradient</strike><br>model sparsity (some neurons can be inactivated)</small></td>
            <td><small>faster computing<br><strike>vanishing gradient</strike><br>model sparsity (some neurons can be inactivated)</small></td>
        </tr>
        <tr>
            <td>$|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0$<br>$\Rightarrow$ vanishing gradient</td>
            <td>$|z|\uparrow\uparrow \rightarrow\frac{da}{dz}\approx 0$<br>$\Rightarrow$ vanishing gradient</td>
            <td>too many neurons get inactivated<br>$\Rightarrow$dying ReLU</td>
            <td>$\varepsilon$ usually set to 0.01<br><strike>dying ReLU</strike><br>widely used on Kaggle</td>
        </tr>
    </tbody>
</table>

- Why need activation funcs? To add nonlinearity.
    1. Suppose $g(z)=z$ (i.e. $\nexists g(z)$)
    2. $\Longrightarrow z^{[1]}=w^{[1]}x+b^{[1]}$
    3. $\Longrightarrow z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})=w'x+b'$
    4. This is just linear regression. Hidden layers exist for no reason.

&emsp;
## Training
<a name="fp"></a>
**Forward Propagation**

<center><img src="../../images/DL/fp.png" width="500"/></center>

<a name="bp"></a>
**Backward Propagation**

<center><img src="../../images/DL/bp.png" width="500"/></center>
  
<a name="fbss"></a>
**Example: Forward & Backward Step: Stochastic**: 2 nodes & 3 inputs & no bias

- Forward Step: 

$$\begin{equation}
\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} \\
w_{2,1} & w_{2,2} & w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}=\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix}
\end{equation}$$

- Backward Step: 

$$\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1}}x_1 & \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_2 & \frac{\partial{\mathcal{L}}}{\partial{z_1}}x_3 \\
\frac{\partial{\mathcal{L}}}{\partial{z_2}}x_1 & \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_2 & \frac{\partial{\mathcal{L}}}{\partial{z_2}}x_3
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}x^T
\end{equation}$$
  
<a name="fbsb"></a>
**Example: Forward & Backward Step: Mini-batch**: 2 nodes & 3 inputs & bias & 2 training examples

- Forward Step: 

$$\begin{equation}
\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} \\
w_{2,1} & w_{2,2} & w_{2,3}
\end{bmatrix}\begin{bmatrix}
x_1^{(1)} & x_1^{(2)} \\ 
x_2^{(1)} & x_2^{(2)} \\ 
x_3^{(1)} & x_3^{(2)}
\end{bmatrix}+\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}=\begin{bmatrix}
z_1^{(1)} & z_1^{(2)} \\
z_2^{(1)} & z_2^{(2)}
\end{bmatrix}
\end{equation}$$

- Backward Step: 

<center><small>$$\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{W}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{w_{1,1}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{1,2}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{1,3}}} \\
\frac{\partial{\mathcal{L}}}{\partial{w_{2,1}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{2,2}}} & \frac{\partial{\mathcal{L}}}{\partial{w_{2,3}}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_1^{(2)} & \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_2^{(2)} & \frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}}x_3^{(2)} \\
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_1^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_1^{(2)} & \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_2^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_2^{(2)} & \frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}x_3^{(1)}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}x_3^{(2)} \\
\end{bmatrix}=\frac{\partial{\mathcal{L}}}{\partial{Z}}X^T
\end{equation}$$</small></center>

$$\begin{equation}
\frac{\partial{\mathcal{L}}}{\partial{b}}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{b_1}} \\ \frac{\partial{\mathcal{L}}}{\partial{b_2}}
\end{bmatrix}=\begin{bmatrix}
\frac{\partial{\mathcal{L}}}{\partial{z_1^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_1^{(2)}}} \\ 
\frac{\partial{\mathcal{L}}}{\partial{z_2^{(1)}}}+\frac{\partial{\mathcal{L}}}{\partial{z_2^{(2)}}}
\end{bmatrix}=\sum_{i=1}^{2}{\frac{\partial{\mathcal{L}}}{\partial{z^{(i)}}}}
\end{equation}$$

<a name="rd"></a>
**Reverse Differentiation**: a simple procedure summarized for a clearer understanding of backprop from Node A to Node B:
1. Find one single path of "A$\rightarrow$B"
2. Multiply all edge derivatives
3. Add the multiple to the overall derivative
4. Repeat 1-3  
  
e.g.  
Path 1:

<center><img src="../../images/DL/rd1.png" width="500"/></center>

Path 2:

<center><img src="../../images/DL/rd2.png" width="500"/></center>

Path 3:

<center><img src="../../images/DL/rd3.png" width="500"/></center>

And so on ......  
<center><strong><i>Reverse Differentiation $\times$ Backward Step = Backward Propagation</i></strong></center>

&emsp;<a name="gd"></a>
## Gradient Descent

$$\begin{equation}
W := W-\alpha\frac{\partial\mathcal{L}}{\partial W}
\end{equation}$$

1. **Stochastic GD** (using 1 training example for each GD step)

    $$\begin{align}
    \mathcal{L}(\hat{Y},Y)&=\frac{1}{2}(\hat{Y_i}-Y_i)^2 \\
    W&=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
    \end{align}$$

2. **Mini-batch GD** (using mini-batches of size $m'\ (\text{s.t.}\ m=km', k\in Z)$ for each GD step)

    $$\begin{align}
    \mathcal{L}(\hat{Y},Y)&=\frac{1}{2}\sum_{i=1}^{m'}{(\hat{Y_i}-Y_i)^2} \\
    W&=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
    \end{align}$$

3. **Batch GD** (using the whole training set for each GD step)

    $$\begin{align}
    \mathcal{L}(\hat{Y},Y)&=\frac{1}{2}\sum_{i=1}^{m}{(\hat{Y_i}-Y_i)^2} \\
    W&=W-\alpha\frac{\partial\mathcal{L}}{\partial W}
    \end{align}$$
