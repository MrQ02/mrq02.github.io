---
layout: post
title: "GLM"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/GLM/
header-includes:
- \usepackage{amsmath}
---

## Intro

This covers the basics of Generalized Linear Models together with Softmax Regression.
- [GLM](#glm)
- [Softmax Regression](#softmax)

## <a name="glm"></a>Generalized Linear Models (GLM)

- What are **GLM**s?  


    Remember the two models we had in the last post?  
    Regression:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p(y|x,w)\sim N(\mu,\sigma^2)$  
    Classification: &nbsp;&nbsp;$p(y|x,w)\sim \text{Bernoulli}(\phi)$  
    
    They belong to GLM, a collection of models that can be applied to Supervised Learning problems. We will show more examples of GLMs in this markdown.

- **Exponential Family**

    $$\begin{equation}
    p(y,\eta)=b(y)\cdot e^{\eta^TT(y)-a(\eta)}
    \end{equation}$$
    
    1. $\eta$: natural parameter (i.e. canonical parameter)  
        
        &emsp;different $\eta \rightarrow$ different distributions within the family
        
    2. $T(y)$: sufficient statistic (usually, $T(y)=y$)  
        
    3. $a(\eta)$: log partition function  
        
    4. $e^{-a(\eta)}$: normalization constant (to ensure that $\int{p(y,\eta)dy}=1$) 
        
    5. $T,a,b$: fixed choice that defines a family of distributions parametrized by $\eta$
    
- Example 1: **Bernoulli Distribution (Classification)**

    $$\begin{align}
    p(y|\phi)&=\phi^y(1-\phi)^{1-y} \\
    &=e^{y\log{\phi}+(1-y)\log{(1-\phi)}} \\
    &=e^{y\log{\frac{\phi}{1-\phi}}+\log{(1-\phi)}} \\
    \end{align}$$
    
    1. $\eta=\log{\frac{\phi}{1-\phi}}\Leftrightarrow \phi=\frac{1}{1+e^{-\eta}}$
        
    2. $T(y)=y$
        
    3. $a(\eta)=\log{(1+e^\eta)}$
        
    4. $b(y)=1$  
    &nbsp;
    
- Example 2: **Gaussian Distribution (Regression)**

    $$\begin{align}
    p(y|\mu,\sigma^2)&=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(y-\mu)^2} \\
    &=\frac{1}{\sqrt{2\pi}}e^{\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-\frac{1}{2\sigma^2}\mu^2-\log{\sigma}}
    \end{align}$$
    
    1. $\eta=\begin{bmatrix}
           \frac{\mu}{\sigma^2} ;
           \frac{-1}{2\sigma^2}
          \end{bmatrix}$
              
    2. $T(y)=\begin{bmatrix}
           y;
           y^2
          \end{bmatrix}$
              
    3. $a(\eta)=\frac{1}{2\sigma^2}\mu^2-\log{\sigma}=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log{(-2\eta_2)}$
        
    4. $b(y)=\frac{1}{\sqrt{2\pi}}$
    &nbsp;
    
- Example 3: **Poisson Distribution** (count-data)

    $$\begin{align}
    p(y|\lambda)&=\frac{\lambda^ye^{-\lambda}}{y!}\\
    &=\frac{1}{y!}e^{y\log{\lambda}-\lambda}
    \end{align}$$
    
    1. $\eta=\log{\lambda}$
        
    2. $T(y)=y$
        
    3. $a(\eta)=e^\eta$
        
    4. $b(y)=\frac{1}{y!}$  
    &nbsp;
    
- Example 4: **Gamma Distribution** (continuous non-negative random variables)

    $$\begin{align}
    p(y|\lambda,a)&=\frac{\lambda^ay^{a-1}e^{-\lambda y}}{\Gamma(a)}\\
    &=\frac{y^{a-1}}{\Gamma(a)}e^{-\lambda y+a\log{\lambda}}
    \end{align}$$
    
    1. $\eta=-\lambda$
        
    2. $T(y)=y$
        
    3. $a(\eta)=-a\log{(-\eta)}$
        
    4. $b(y)=\frac{y^{a-1}}{\Gamma(a)}$  
    &nbsp;
    
    
- Example 5: **Beta Distribution** (distribution of probabilities)

    $$\begin{align}
    p(y|\alpha,\beta)&=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}y^{\alpha-1}(1-y)^{\beta-1} \\
    &=\frac{(1-y)^\beta}{y(1-y)\Gamma(\beta)}e^{\alpha\log{y}- \log{\frac{\Gamma(\alpha)}{\Gamma(\alpha+\beta)}}} \\
    &=\frac{y^\alpha}{y(1-y)\Gamma(\alpha)}e^{\beta\log{(1-y)}- \log{\frac{\Gamma(\beta)}{\Gamma(\alpha+\beta)}}}
    \end{align}$$
    
    1. $\eta=\alpha\ \text{or}\ \beta$
        
    2. $T(y)=\log{y}\ \text{or}\ \log{(1-y)}$
        
    3. $a(\eta)=\log{\frac{\Gamma(\eta)}{\Gamma(\alpha+\beta)}}$
        
    4. $b(y)=\frac{(1-y)^\beta}{y(1-y)\Gamma(\beta)}\ \text{or}\ \frac{y^\alpha}{y(1-y)\Gamma(\alpha)}$  
    &nbsp;
    
- Example 6: **Dirichlet Distribution** (multivariate beta)

    $$\begin{align}
    p(y|\alpha)&=\frac{\Gamma(\sum_k\alpha_k)}{\prod_k\Gamma(\alpha_k)}\prod_k{y_k^{\alpha_k-1}} \\
    &=\exp{\big(\sum_k{(\alpha_k-1)\log{y_k}}-\big[\sum_k{\log{\Gamma(\alpha_k)}}-\log{\Gamma(\sum_k{\alpha_k})}\big]\big)}
    \end{align}$$
    
    1. $\eta=\alpha-1$
        
    2. $T(y)=\log{y}$
        
    3. $a(\eta)=\sum_k{\log{\Gamma(\alpha_k)}}-\log{\Gamma(\sum_k{\alpha_k})}$
        
    4. $b(y)=1$  
    &nbsp;
    
## Method of Constructing GLMs

- 3 Assumptions
    
    1. $y\|x,w \sim \text{ExponentialFamily}(\eta)$
    
        interpretation: $y$ given $x\&w$ follows some exponential family distribution with natural parameter $\eta$
        
    2. $h(x)=\text{E}[y\|x]$
    
        interpretation: our hypothetical model $h(x)$ should predict the expected value of $y$ given $x$
    
    3. $\eta=w^Tx$
    
        interpretation: $\eta$ is linearly related to $x$
    
- Example 1: OLS (Ordinary Least Squares) (i.e. LinReg)

    $$\begin{align}
    h(x)&=\text{E}[y\|x,w]\ \ \ \ \ \ &\text{(Assumption 2)} \\
       &=\mu \\
       &=\eta\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &\text{(Assumption 1)} \\
       &=w^Tx\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &\text{(Assumption 3)}
    \end{align}$$
    
- Example 2: Logistic Regression

    $$\begin{align}
    h(x)&=\text{E}[y\|x,w]\ \ \ \ \ \ &\text{(Assumption 2)} \\
       &=\phi \\
       &=\frac{1}{1+e^{-\eta}}\ \ \ \ \ \ &\text{(Assumption 1)} \\
       &=\frac{1}{1+e^{-w^Tx}}\ \ \ \ \ \ &\text{(Assumption 3)}
    \end{align}$$
    
- Example 3: <a name="softmax"></a>**Softmax Regression**
    
    1. What is it?
    
        a method used in **multiclass classification** to select one output value $\phi_i$ of the highest probability among all the output values.
        
        $$\begin{equation}
        \hat{y}=\begin{bmatrix}
        \phi_1 \\
        \vdots \\
        \phi_{k-1}
        \end{bmatrix}
        \end{equation}$$
        
    2. **One-hot Encoding**
        
        $$\begin{equation}
        y\in \{ 1,\cdots,k \} \Rightarrow T(y)\in \mathbb{R}^{k}
        \end{equation}$$  
        
        where
        
        $$\begin{equation}
        T(1)=\begin{bmatrix}
        1 \\ 0 \\ \vdots \\ 0
        \end{bmatrix},
        T(2)=\begin{bmatrix}
        0 \\ 1 \\ \vdots \\ 0
        \end{bmatrix},\cdots,
        T(k)=\begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 1
        \end{bmatrix}
        \end{equation}$$
    
    3. **Dummy Encoding**

        
        $$\begin{equation}
        y\in \{ 1,\cdots,k \} \Rightarrow T(y)\in \mathbb{R}^{k-1}
        \end{equation}$$  
        
        where
        
        $$\begin{equation}
        T(1)=\begin{bmatrix}
        1 \\ 0 \\ \vdots \\ 0
        \end{bmatrix},
        T(2)=\begin{bmatrix}
        0 \\ 1 \\ \vdots \\ 0
        \end{bmatrix},\cdots,
        T(k-1)=\begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 1
        \end{bmatrix},
        T(k)=\begin{bmatrix}
        0 \\ 0 \\ \vdots \\ 0
        \end{bmatrix}
        \end{equation}$$
        
        Why Dummy Encoding > One-hot Encoding? It reduces 1 entire column!
    
    4. Indicator Function
    
        $$\begin{equation}
        \text{I}\{ \text{True} \}=1,\ \text{I}\{ \text{False} \}=0
        \end{equation}$$
        
        Therefore,
        
        $$\begin{equation}
        T(y)_i =\text{I}\{ y=i \}
        \end{equation}$$
        
        Therefore,
        
        $$\begin{equation}
        \text{E}[T(y)_i] =P(y=i)=\phi_i
        \end{equation}$$
        
    5. Exponential Family form
    
        $$\begin{align}
        p(y|\phi)&=\prod_{i=1}^{k}{\phi_i^{\text{I}\{ y=i \}}} \\
        &=\prod_{i=1}^{k-1}{\phi_i^{T(y)_i}} \cdot \phi_k^{1-\sum_{i=1}^{k-1}{T(y)_i}} \\
        &=\exp{\big(\sum_{i=1}^{k-1}{T(y)_i\log{(\phi_i)}-\sum_{i=1}^{k-1}{T(y)_i}\log{(\phi_k)}}+\log{(\phi_k)}\big)} \\
        &=\exp{\big(\sum_{i=1}^{k-1}{T(y)_i\log{\big(\frac{\phi_i}{\phi_k}\big)}+\log{(\phi_k)}\big)}} \\
        \end{align}$$

        1. $\eta=\begin{bmatrix}\log{\big(\frac{\phi_1}{\phi_k}\big)}\ ;\ \cdots\ ;\ \log{\big(\frac{\phi_{k-1}}{\phi_k}\big)}\end{bmatrix}$
              
        2. $T(y)=\begin{bmatrix}T(y)_1\ ;\ \cdots\ ;\ T(y)_k-1\end{bmatrix}$

        3. $a(\eta)=-\log{(\phi_k)}$

        4. $b(y)=1$  
        &nbsp;
    
    6. **Softmax Function** (derived from $\eta_i=\log{\big(\frac{\phi_i}{\phi_k}\big)}$)
    
        $$\begin{equation}
        \phi_i=\frac{e^{\eta_i}}{\sum_{j=1}^k{e^{\eta_j}}}
        \end{equation}$$
        
    7. Probabilistic Interpretation of Softmax Regression
    
        $$\begin{equation}
        p(y=i|x,w)=\frac{e^{w_i^Tx}}{\sum_{j=1}^k{e^{w_i^Tx}}}
        \end{equation}$$
    
    8. Log Likelihood
    
        $$\begin{align}
        l(w)&=\sum_{i=1}^m{\log{p(y^{(i)}|x^{(i)},w)}} \\
        &=\sum_{i=1}^m{\log{\prod_{i=1}^{k}{\Bigg(\frac{e^{w_l^Tx^{(i)}}}{\sum_{j=1}^k{e^{w_l^Tx^{(i)}}}}\Bigg)^{\text{I}\{ y^{(i)}=l \}}}}}
        \end{align}$$
        