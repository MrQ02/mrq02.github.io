---
layout: post
title: "SVM"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/SVM/
header-includes:
- \usepackage{amsmath}
---

## Intro

- Problem with Classification:
    
    <img src="../../images/ML/SVM1.png" width="400"/>
    
    This is a binary classification. The circles & crosses are training examples with two different labels. The black line is the classifier, and it is able to classify "circle" and "cross". For points like $\text{A}$ that are distant from the classifier, we are quite confident that they belong to "cross".
    
    However, what about $\text{B}$ and $\text{C}$ that are super close to the decision boundary? Based on this classifier, $\text{B}$ belongs to "cross" and $\text{C}$ belongs to "circle", but how confident are we about our classifier? What if our classifier is just slightly off and $\text{C}$ was actually "cross"?
    
    <img src="../../images/ML/SVM2.png" width="400"/>
    
    This, is SVM in a nutshell.
    
    &emsp;
    &emsp;
    
## Margins
    
- **Functional Margin**

    $$\begin{equation}
    \hat{\gamma}^{(i)}=y^{(i)}(w^Tx+b)\ \ \ \ \ \ \|\ y\in\{-1,1\}
    \end{equation}$$

    Intuition: $\hat{\gamma}^{(i)}\uparrow\uparrow\ \rightarrow\text{confidence}\uparrow\uparrow$
    
    When $y=1\ \rightarrow w^Tx+b \>\> 0$.  
    When $y=-1\\rightarrow w^Tx+b \<\< 0$.
    
    $$\begin{align}
    \text{if}\ y=1\ \rightarrow 
    \hat{\gamma}^{(i)}=y^{(i)}(w^Tx+b)\ \ \ \ \ \ \|\ y\in\{-1,1\}
    \end{align}$$

    - Problem with functional margin:
    
      if $w\rightarrow kw$ and $b\rightarrow kb$ (where $k>0$), then $g(w^Tx+b)=g(k(w^Tx+b))$
    
      but our $g(z)$ here follows:
      
      $$g(z)=\begin{cases}
      -1& \text{if $z<0$} \\
      1& \text{if $z>0$} \\
      \end{cases}$$
      
      that is, $z$ and $kz$ makes no difference for $g(z)$.
      
      HOWEVER, the functional margin does change by a factor of $k$ here, meaning that a large functional margin does not necessarily represent a confident prediction in this case.
    
    &emsp;
      
- **Geometric Margin**

    Refer back to the figure above. If we want to find the distance between point $A$ and the decision boundary, which is $AA'=\gamma^{(i)}$, what should we do?
        
    <img src="../../images/ML/SVM3.png" width="400"/>
    
    We normalize $w$ to find the unit vector $\frac{w}{\lVert w \rVert}$, and we also have $A=x^{(i)}$. Because $AA'\parallel \overrightarrow{w}$, we can find $A'$ by:
    
    $$\begin{equation}
    A'=x^{(i)}-\gamma^{(i)}\frac{w}{\lVert w \rVert}
    \end{equation}$$
    
    and because $A'$ is on the decision boundary $w^Tx+b=0$, we get
    
    $$\begin{align}
    &w^TA'+b=0 \\
    \Longrightarrow\ &w^Tx^{(i)}+b=w^T\frac{w}{\lVert w \rVert}\gamma^{(i)} \ \ \ \ \ \ \ \ \ \bigg(w^T\frac{w}{\lVert w \rVert}=\frac{\lVert w \rVert^2}{\lVert w \rVert}\bigg) \\
    \Longrightarrow\ &\gamma^{(i)}=\bigg(\frac{w}{\lVert w \rVert}\bigg)^Tx^{(i)}+\frac{b}{\lVert w \rVert}
    \end{align}$$
    
    and if we generalize it with both classes of $y^{(i)}$:
    
    $$\begin{equation}
    \gamma^{(i)}=y^{(i)}\Bigg(\bigg(\frac{w}{\lVert w \rVert}\bigg)^Tx^{(i)}+\frac{b}{\lVert w \rVert}\Bigg)
    \end{equation}$$
    
    &emsp;

## Optimization: Lagrange Duality

- Optimization problem:
    
    $$\begin{equation}
    \mathop{\min}_ {w} f(w)\ \ \text{s.t.}\ h_i(w)=0\ \ \forall i\in\{1,...,m\}
    \end{equation}$$
    
- Lagrangian

    $$\begin{equation}
    \mathcal{L}(w,\beta)=f(w)+\sum_{i=1}^{m}{\beta_ih_i(w)}
    \end{equation}$$
    
    where $\beta_i=$ Lagrange multipliers, and then we solve it by $\frac{\partial{\mathcal{L}}}{\partial{w_i}}=0$ and $\frac{\partial{\mathcal{L}}}{\partial{\beta_i}}=0$
    
- Primal optimization problem:

    $$\begin{align}
    \mathop{\min}_ {w} f(w)\ \ \text{s.t.}\ h_i(w)=0\ \ &\forall i\in\{1,...,m\} \\
    g_i(w)\leq 0\ \ &\forall i\in\{1,...,n\}
    \end{align}$$
    
- Generalized Lagrangian

    $$\begin{equation}
    \mathcal{L}(w,\alpha,\beta)=f(w)+\sum_{i=1}^{m}{\beta_ih_i(w)}+\sum_{i=1}^{n}{\alpha_ig_i(w)}
    \end{equation}$$
    
    

