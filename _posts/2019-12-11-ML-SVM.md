---
layout: post
title: "SVM"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/SVM/
header-includes:
- \usepackage{amsmath}
---

## Intro

- Problem with Classification:

    ![](../../images/ML/SVM1.png)
    
    This is a binary classification. The circles & crosses are training examples with two different labels. The black line is the classifier, and it is able to classify "circle" and "cross". For points like $\text{A}$ that are distant from the classifier, we are quite confident that they belong to "cross".
    
    However, what about $\text{B}$ and $\text{C}$ that are super close to the decision boundary? Based on this classifier, $\text{B}$ belongs to "cross" and $\text{C}$ belongs to "circle", but how confident are we about our classifier? What if our classifier is just slightly off and $\text{C}$ was actually "cross"?
    
    ![](../../images/ML/SVM2.png)
    
    This, is SVM in a nutshell.
    &nbsp;
    
## Margins
    
- Functional Margin

    $$\begin{equation}
    \hat{\gamma}^{(i)}=y^{(i)}(w^Tx+b)\ \ \ \ \ \ \|\ y\in\{-1,1\}
    \end{equation}$$

    Intuition: $\hat{\gamma}^{(i)}\uparrow\uparrow\ \rightarrow\text{confidence}\uparrow\uparrow$
    
    When $y=1\ \rightarrow w^Tx+b \>\> 0$.  
    When $y=-1\\rightarrow w^Tx+b \<\< 0$.
    
    $$\begin{align}
    \text{if}\ y=1\ \rightarrow 
    \hat{\gamma}^{(i)}=y^{(i)}(w^Tx+b)\ \ \ \ \ \ \|\ y\in\{-1,1\}
    \end{align}$$


        