---
layout: post
title: "Regression"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/linreg/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Roadmap:
- [Linear Regression](#linreg)
    - [Problem Setting](#lwr)
    - [Newton's Method](#newton)
- [Logistic Regression](#logreg)
- [Generalized Linear Models](#glm)
    - [Bernoulli Distribution](#bernoulli)
    - [Gaussian Distribution](#gaussian)
    - [Poisson Distribution](#poisson)
    - [Gamma Distribution](#gamma)
    - [Beta Distribution](#beta)
    - [Dirichlet Distribution](#dirichlet)
    - [Method of Constructing GLMs](#construct)
        - [Softmax Regression](#softmax)

&emsp;<a name="linreg"></a>
## Linear Regression

- **Problem Setting**

    - **Input**: $x\in\mathbb{R}^n$ (i.e. measurements, features, independent variables, whatever you call it.)
    - **Output**: $y\in\mathbb{R}$ (i.e. response, dependent variable, whatever you call it.)
    - **Goal**: Find a linear function $f:\mathbb{R}^n\rightarrow\mathbb{R}\ \ \text{s.t.}\ \ \forall\ (x,y): y\approx f(x;w)$  
<br>
- **Model**

    $$\begin{align}
    \hat{y}_ i&=\sum_{j=0}^{n}w_jx_{ij} \\ \\
    \hat{y}&=Xw \\ \\
    \begin{bmatrix} \hat{y}_ 1 \\ \vdots \\ \hat{y}_ m \end{bmatrix}&=
    \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1n} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{m1} & \cdots & x_{mn} \\
    \end{bmatrix}\begin{bmatrix} w_0 \\ \vdots \\ w_n \end{bmatrix}
    \end{align}$$

    - $x_{ij}$: the $j$th feature in the $i$th observation
    - $\hat{y}_ i$: the model prediction for the $i$th observation
    - $w_j$: the parameter for the $j$th feature
    - $m$: #observations
    - $n$: #features  
<br>
- **Learning**
    
    - **Aim**: find the optimal $w$ that minimizes a loss function (i.e. cost function)

    - **Loss Function: OLS \[Ordinary Least Squares]**

        $$\begin{equation*}
        \mathcal{L}(w)=\sum_{i=1}^{m}(\hat{y}_ i-y_i)^2
        \end{equation*}$$
        
        - Assumption (i.e. requirement): $m > > n$  
    <br>
    - **Minimization Method 1: Gradient Descent** (the practical solution)

        $$\begin{equation}
        w_j := w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j}
        \end{equation}$$

        - $\alpha$: learning rate
        - $\frac{\partial\mathcal{L}(w)}{\partial w_j}$: gradient  
        
        1. **Stochastic GD** (using 1 training observation for each GD step)

            $$\begin{equation}
            w_j = w_j-\alpha(\hat{y}_ i-y_i)x_{ij}
            \end{equation}$$

        2. **Mini-batch GD** (using mini-batches of size $m'$ for each GD step)

            $$\begin{equation*}
            w_j = w_j-\alpha\sum_{i=1}^{m'}(\hat{y}_ i-y_i)x_{ij}
            \end{equation*}$$

        3. **Batch GD** (LMS) (using the whole training set for each GD step)

            $$\begin{equation}
            w_j = w_j-\alpha\sum_{i=1}^{m}(\hat{y}_ i-y_i)x_{ij}
            \end{equation}$$
    
    <br>
    - **Minimization Method 2: Normal Equation** (the exact solution)

        $$\begin{equation*}
        w_{\text{LS}}=(X^TX)^{-1}X^Ty\ \Longleftrightarrow\  w_{\text{LS}}=\Big(\sum_{i=1}^m{x_ix_i^T}\Big)^{-1}\Big(\sum_{i=1}^m{y_ix_i}\Big)
        \end{equation*}$$

        1. Derivation (matrix)

            $$\begin{align}
            \DeclareMathOperator{\Tr}{tr}
            \nabla_w\mathcal{L}(w)&=\nabla_w(Xw-y)^T(Xw-y) \\
            &=\nabla_w\Tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) \\
            &=\nabla_w(\Tr(w^TX^TXw)-2\Tr(y^TXw)) \\
            &=2X^TXw-2X^Ty \\
            &\Rightarrow X^TXw-X^Ty=0
            \end{align}$$
            
        2. Derivation (vector)
            
            $$\begin{align}
            \nabla_w\mathcal{L}(w)&=\sum_{i=1}^m{\nabla_w(w^Tx_ix_i^Tw-2w^Tx_iy_i+y_i^2)} \\
            &=-\sum_{i=1}^m{2y_ix_i}+\Big(\sum_{i=1}^m{2x_ix_i^T}\Big)w \\
            &\Rightarrow \Big(\sum_{i=1}^m{x_ix_i^T}\Big)w-\sum_{i=1}^m{y_ix_i}=0
            \end{align}$$
    
    <br>
    - **Comparison: GD vs Normal Equation** 
        
        |           | GD | Normal Equation |
        |:---------:|:------:|:---------------:|
        | **Advantage** | faster computing<br>less computing power required | the exact solution |
        | **Disadvantage** | hard to reach the exact solution | $(X^TX)^{-1}$ must exist<br>(i.e. $(X^TX)^{-1}$ must be full rank) |
        
        - <u>Full rank</u>: when the $m\times n$ matrix $X$ has $\geq n$ linearly independent rows (i.e. any point in $\mathbb{R}^n$ can be reached by a weighted combination of $n$ rows of $X$)  
<br>
- **Probabilistic Interpretation**
    
    1. **Probabilistic Model**
    
        $$\begin{equation}
        p(y_i|x_i,w)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
        \end{equation}$$
        
        - $y_i=w^Tx_i+\epsilon_i$
        - $\epsilon_i\sim N(0,\sigma)$  
    <br>
    2. **Likelihood Function**
    
        $$\begin{equation}
        L(w)=\prod_{i=1}^{m}p(y_i|x_i,w)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
        \end{equation}$$
    
    3. **Log Likelihoood**
      
        $$\begin{align}
        \mathcal{l}(w)&=\log{L(w)} \\
        &=\log{\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
        &=\sum_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
        &=m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-w^Tx_i)^2
        \end{align}$$
    
        <u>Why log?</u>
        
        1. log = monotonic & increasing on $[0,1]\rightarrow$  
        
            $$\mathop{\arg\max}_ {w}L(w)=\mathop{\arg\max}_ {w}\log{L(w)}$$
            
        2. log simplifies calculation (especially & obviously for $\prod$)  
    <br>
    4. **MLE (Maximum Likelihood Estimation)**
    
        $$\begin{align}
        \mathop{\arg\max}_ {w}\mathcal{l}(w)&=\mathop{\arg\max}_ {w}(m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-w^Tx^{(i)})^2) \\
        &=\mathop{\arg\max}_ {w}(-\sum_{i=1}^{m}(y^{(i)}-w^Tx^{(i)})^2) \\
        &=\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y^{(i)}-w^Tx^{(i)})^2
        \end{align}$$

<br/><a name="lwr"></a>
### <strong>Locally Weighted Linear Regression (LWR)</strong>
1. Original LinReg

    $$\begin{equation}
    w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y^{(i)}-w^Tx^{(i)})^2
    \end{equation}$$

    We find the $w$ that minimizes the cost function that in turn maximizes the likelihood function so that our linear regression model is optimized to fit the data.

2. LWR

    $$\begin{equation}
    w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}\cdot(y^{(i)}-w^Tx^{(i)})^2
    \end{equation}$$

    We add the weight function $W^{(i)}=e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}$ to change the game.

    What game?
    - **Underfitting**: the model barely fits the data points.

        <center><img src="../../images/ML/underfit.png" height="200"/></center>

        One single line is usually not enough to capture the pattern of $x\ \&\ y$. In order to get a better fit, we add more polynomial features ($x^j$) to the original model:

    - **Overfitting**: the model fits the given data points too well that it cannot be used on other data points

        <center><img src="../../images/ML/overfit.png" height="200"/></center>

        When we add too much (e.g. $y=\sum_{j=0}^{6}w_jx^j$), the model captures the pattern of the given data points $(x^{(i)},y^{(i)})$ too much that it cannot perform well on new data points.

    How can LWR change the game?
    - When we would like to estimate $y$ at a certain $x$, instead of applying the original LinReg, we take a subset of data points $(x^{(i)},y^{(i)})$ around $x$ and try to do LinReg on that subset only so that we can get a more accurate estimation. 
    - numerator

    $$\begin{align}
    &\text{If}\ |x^{(i)}-x|=\text{small} \longrightarrow W^{(i)}\approx 1 \\
    &\text{If}\ |x^{(i)}-x|=\text{large} \longrightarrow W^{(i)}\approx 0
    \end{align}$$

    - bandwidth parameter: $\tau$ (how fast the weight of $x^{(i)}$ falls off the query point $x$)

<br/><a name="newton"></a>
### Gradient Descent: <strong>Newton's Method</strong>
1. Newton's formula

    $$\begin{equation}
    w := w-\frac{f(w)}{f'(w)}
    \end{equation}$$

2. Newton-Raphson Method in GD

    $$\begin{equation}
    w := w-H^{-1}\nabla_wl(w)
    \end{equation}$$

    $H$: Hessian Matrix

    $$\begin{equation}
    H_{ij}=\frac{\partial^2l(w)}{\partial w_i \partial w_j}
    \end{equation}$$

3. Newton vs normal GD
    - YES: faster convergence, fewer iterations
    - NO:  expensive computing (inverse of a matrix)

&emsp;<a name="logreg"></a>
## Logistic Regression (Classification)

- Model

    $$\begin{equation}
    \hat{y}=g(w^Tx)
    \end{equation}$$

    $g(z)$: a function that converts $w^Tx$ to binary value

- Sigmoid Function (see Deep Learning for more funcs)

    $$\begin{equation}
    g(z)=\sigma(z)=\frac{1}{1+e^{-z}}
    \end{equation}$$
    
    - Derivative (you will know why we need this in Deep Learning)
    
        $$\begin{align}
        g'(z)&=\frac{d}{dz}\frac{1}{1+e^{-z}} \\
        &=\frac{e^{-z}(+1-1)}{(1+e^{-z})^2} \\
        &=g(z)(1-g(z))
        \end{align}$$

- Cost Function


    1. single training example (derivation later)
    
        $$\begin{equation}
        \mathcal{L}(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log{(1-\hat{y})})
        \end{equation}$$
        
        If $y=1\rightarrow\mathcal{L}(\hat{y},y)=-\log{\hat{y}}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\uparrow$"$\rightarrow\hat{y}=1$   
        If $y=0\rightarrow\mathcal{L}(\hat{y},y)=-\log{(1-\hat{y})}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\downarrow$"$\rightarrow\hat{y}=0$ 
        
    2. entire training set
    
        $$\begin{equation}
        \mathcal{J}(w)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)},y^{(i)})=\text{mean}(\mathcal{L})
        \end{equation}$$

- Probabilistic Interpretation

    1. Assumptions
    
        $$\begin{align}
        P(y=1|x,w)&=\hat{y} \\
        P(y=0|x,w)&=1-\hat{y}
        \end{align}$$

    2. Probabilistic Model of LogReg
    
        $$\begin{equation}
        p(y|x,w)=\hat{y}^y(1-\hat{y})^{1-y}
        \end{equation}$$
        
    3. Likelihood Function
    
        $$\begin{equation}
        L(w)=\prod_{i=1}^{m}(\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}
        \end{equation}$$
        
    4. Log Likelihood
    
        $$\begin{align}
        l(w)&=\sum_{i=1}^{m}(y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}) \\
        l(w)&=-\sum_{i=1}^{m}\mathcal{L}(\hat{y},y)
        \end{align}$$
        
    5. MLE
        
        $$\begin{align}
        \frac{\partial l(w)}{\partial w_j}&=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})\frac{\partial g(w^Tx)}{\partial w_j} \\
        &=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})g(w^Tx)(1-g(w^Tx))\frac{\partial(w^Tx)}{\partial w_j} \\
        &=(y(1-g(w^Tx))-(1-y)g(w^Tx))x_j \\
        &=(y-\hat{y})x_j
        \end{align}$$

- Gradient Descent

    $$\begin{align}
    w_j &:= w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j} \\
    &=w_j+\alpha(y-\hat{y})x_j
    \end{align}$$
    
    Why is it also called "Gradient Ascent"?  
    $\because$ we are trying to minimize the loss function $\Leftrightarrow$ maximize the likelihood function