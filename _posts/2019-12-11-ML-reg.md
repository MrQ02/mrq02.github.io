---
layout: post
title: "Regression"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/reg/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim: some of the images in this session are cited from ColumbiaX's <a href="https://www.edx.org/micromasters/columbiax-artificial-intelligence" target="__blank">Artificial Intelligence MicroMasters Program</a>.

Roadmap:
- [Linear Regression](#linreg)
    - [Problem Setting](#prob)
    - [Model](#model)
    - [Learning](#learn)
        - [Gradient Descent](#gd)
        - [Newton's Method](#newton)
        - [Normal Equation](#normal)
    - [Probabilistic Interpretation](#probabi)
    - [Regularization](#regular)
    - [Code Template](#code)  
<br>
- [Polynomial Regression](#poly)
    - [Different Preprocessing](#polyprep)
    - [Code Template](#polycode)
    - [Further Extensions](#polyex)  
<br>
- [Locally Weighted Linear Regression](#lwr)
    - [Problem Setting & Intuition](#lwrprob)
    - [Model: Weighted LS](#lwrmodel)
    - [Code Template](#lwrcode)  
<br>
- [Ridge Regression](#ridge)
    - [Problem Setting, Model & Preprocessing](#ridgeprob)
    - [Singular Value Decomposition](#svd)
        - [Calculation](#svdcalc)
    - [Ridge Regression vs Least Squares LinReg](#ridgevsls)
    
    

&emsp;<a name="linreg"></a>
## Linear Regression

- <a name="prob"></a>**Problem Setting**

    - **Data**: Observed pairs $(x,y)$, where $x\in\mathbb{R}^{n+1}$ (**input**) & $y\in\mathbb{R}$ (**output**)
    - **Goal**: Find a linear function of the unknown $w$s: $f:\mathbb{R}^n\rightarrow\mathbb{R}\ \ \text{s.t.}\ \ \forall\ (x,y): y\approx f(x,w)$  
<br>
- <a name="model"></a>**Model**

    $$\begin{align}
    \hat{y}_ i&=\sum_{j=0}^{n}w_jx_{ij} \\ \\
    \hat{y}&=Xw \\ \\
    \begin{bmatrix} \hat{y}_ 1 \\ \vdots \\ \hat{y}_ m \end{bmatrix}&=
    \begin{bmatrix}
    1 & x_{11} & \cdots & x_{1n} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{m1} & \cdots & x_{mn} \\
    \end{bmatrix}\begin{bmatrix} w_0 \\ \vdots \\ w_n \end{bmatrix}
    \end{align}$$

    - $x_{ij}$: the $j$th feature in the $i$th observation
    - $\hat{y}_ i$: the model prediction for the $i$th observation
    - $w_j$: the parameter for the $j$th feature
    - $m$: #observations
    - $n$: #features  
<br>
- <a name="learning"></a>**Learning**
    
    - **Aim**: find the optimal $w$ that minimizes a loss function (i.e. cost function)

    - **Loss Function: OLS \[Ordinary Least Squares]**

        $$\begin{equation*}
        \mathcal{L}(w)=\sum_{i=1}^{m}(\hat{y}_ i-y_i)^2
        \end{equation*}$$
        
        - Assumption (i.e. requirement): $m > > n$  
    <br>
    - <a name="gd"></a>**Minimization Method 1: Gradient Descent** (the practical solution)

        $$\begin{equation}
        w_j := w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j}
        \end{equation}$$

        - $\alpha$: learning rate
        - $\frac{\partial\mathcal{L}(w)}{\partial w_j}$: gradient  
        
        1. **Stochastic GD** (using 1 training observation for each GD step)

            $$\begin{equation}
            w_j := w_j-\alpha(\hat{y}_ i-y_i)x_{ij}
            \end{equation}$$

        2. **Mini-batch GD** (using mini-batches of size $m'$ for each GD step)

            $$\begin{equation*}
            w_j := w_j-\alpha\sum_{i=1}^{m'}(\hat{y}_ i-y_i)x_{ij}
            \end{equation*}$$

        3. **Batch GD** (LMS) (using the whole training set for each GD step)

            $$\begin{equation}
            w_j := w_j-\alpha\sum_{i=1}^{m}(\hat{y}_ i-y_i)x_{ij}
            \end{equation}$$
    
        - Extra: <a name="newton"></a>**Newton's Method**
    
            1. Newton's formula

                $$\begin{equation}
                w := w-\frac{f(w)}{f'(w)}
                \end{equation}$$

            2. Newton's Method in GD

                $$\begin{equation}
                w := w-H^{-1}\nabla_w\mathcal{L}(w)
                \end{equation}$$

                where $H$ is Hessian Matrix:

                $$\begin{equation}
                H_{ij}=\frac{\partial^2\mathcal{L}(w)}{\partial w_i \partial w_j}
                \end{equation}$$

            3. Newton vs normal GD
                - YES: faster convergence, fewer iterations
                - NO:  expensive computing (inverse of a matrix)  
    <br>
    - <a name="normal"></a>**Minimization Method 2: Normal Equation** (the exact solution)

        $$\begin{equation*}
        w_{\text{LS}}=(X^TX)^{-1}X^Ty\ \Longleftrightarrow\  w_{\text{LS}}=\Big(\sum_{i=1}^m{x_ix_i^T}\Big)^{-1}\Big(\sum_{i=1}^m{y_ix_i}\Big)
        \end{equation*}$$

        1. Derivation (matrix)

            $$\begin{align}
            \DeclareMathOperator{\Tr}{tr}
            \nabla_w\mathcal{L}(w)&=\nabla_w(Xw-y)^T(Xw-y) \\
            &=\nabla_w\Tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) \\
            &=\nabla_w(\Tr(w^TX^TXw)-2\Tr(y^TXw)) \\
            &=2X^TXw-2X^Ty \\
            &\Rightarrow X^TXw-X^Ty=0
            \end{align}$$
            
        2. Derivation (vector)
            
            $$\begin{align}
            \nabla_w\mathcal{L}(w)&=\sum_{i=1}^m{\nabla_w(w^Tx_ix_i^Tw-2w^Tx_iy_i+y_i^2)} \\
            &=-\sum_{i=1}^m{2y_ix_i}+\Big(\sum_{i=1}^m{2x_ix_i^T}\Big)w \\
            &\Rightarrow \Big(\sum_{i=1}^m{x_ix_i^T}\Big)w-\sum_{i=1}^m{y_ix_i}=0
            \end{align}$$
    
    <br>
    - **GD vs Normal Equation** 
        
        |           | GD | Normal Equation |
        |:---------:|:------:|:---------------:|
        | **Advantage** | faster computing<br>less computing power required | the exact solution |
        | **Disadvantage** | hard to reach the exact solution | $(X^TX)^{-1}$ must exist<br>(i.e. $(X^TX)^{-1}$ must be full rank) |
        
        - <u>Full rank</u>: when the $m\times n$ matrix $X$ has $\geq n$ linearly independent rows (i.e. any point in $\mathbb{R}^n$ can be reached by a weighted combination of $n$ rows of $X$)  
<br>
- <a name="probabi"></a>**Probabilistic Interpretation**
    
    1. **Probabilistic Model: Gaussian**
    
        $$\begin{equation}
        p(y_i|x_i,w)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
        \end{equation}$$
        
        - $y_i=w^Tx_i+\epsilon_i$
        - $\epsilon_i\sim N(0,\sigma)$  
    <br>
    2. **Likelihood Function**
    
        $$\begin{equation}
        L(w)=\prod_{i=1}^{m}p(y_i|x_i,w)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}
        \end{equation}$$
    
    3. **Log Likelihoood**
      
        $$\begin{align}
        \mathcal{l}(w)&=\log{L(w)} \\
        &=\log{\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
        &=\sum_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}} \\
        &=m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-w^Tx_i)^2
        \end{align}$$
    
        <u>Why log?</u>
        
        1. log = monotonic & increasing on $[0,1]\rightarrow$  
        
            $$\mathop{\arg\max}_ {w}L(w)=\mathop{\arg\max}_ {w}\log{L(w)}$$
            
        2. log simplifies calculation (especially & obviously for $\prod$)  
    <br>
    4. **MLE (Maximum Likelihood Estimation)**
    
        $$\begin{align}
        \mathop{\arg\max}_ {w}\mathcal{l}(w)&=\mathop{\arg\max}_ {w}(m\log{\frac{1}{\sqrt{2\pi}\sigma}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\
        &=\mathop{\arg\max}_ {w}(-\sum_{i=1}^{m}(y_i-w^Tx_i)^2) \\
        &=\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y_i-w^Tx_i)^2 \\
        &=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2
        \end{align}$$
        
        $\Rightarrow$ Least Squares & Maximum Likelihood share the exact same solution.
        
    5. **Expected Value**:
    
        $$\begin{align}
        \mathbb{E}[w_{ML}]&=\mathbb{E}[(X^TX)^{-1}X^Ty] \\
        &=(X^TX)^{-1}X^TXw \\
        &=w
        \end{align}$$
        
    6. **Variance**:
    
        $$\begin{align}
        \text{Var}[w_{ML}]&=\mathbb{E}[(w_{ML}-\mathbb{E}[w_{ML}])(w_{ML}-\mathbb{E}[w_{ML}])^T] \\
        &=\mathbb{E}[w_{ML}w_{ML}^T]-\mathbb{E}[w_{ML}]\mathbb{E}[w_{ML}]^T \\
        &=(X^TX)^{-1}X^T\mathbb{E}[yy^T]X(X^TX)^{-1}-ww^T \\
        &=(X^TX)^{-1}X^T(\sigma^2I+Xww^TX^T)X(X^TX)^{-1}-ww^T\ \ (1) \\
        &=\sigma^2(X^TX)^{-1} \\
        \end{align}$$
        
        $(1)$:
        
        $$\begin{align}
        \sigma=\text{Var}[y]&=\mathbb{E}[(y-\mu)(y-\mu)^T] \\
        &=\mathbb{E}[yy^T]-2\mu\mu^T+\mu\mu^T \\
        \Rightarrow \mathbb{E}[yy^T]&=\sigma+\mu\mu^T \\
        \end{align}$$
        
    7. **Summary**:
    
        - Assumption: Gaussian - $y\ ~\ N(Xw, \sigma^2I)$
        
        - Expected Value: $\mathbb{E}[w_{ML}]=w$
        
        - Variance: $\text{Var}[w_{ML}]=\sigma^2(X^TX)^{-1}$
        
        - Problem: Notice how $w_{ML}$ becomes huge when our variance $\sigma^2(X^TX)^{-1}$ is too large.
        
- <a name="regular"></a>**Regularization**:
    
    - <u>Intuition</u>: in order to prevent the problem above, we want to constrain our model parameters $w$:

        $$\begin{equation}
        w_{op}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda g(w)
        \end{equation}$$
        
        - $\lambda>0$: regularization parameter
        - $g(w)>0$: penalty function  
    <br>
    - <u>Sample Regularizations</u>: Ridge Regression, LASSO Regression, ...  
<br>
- <a name="code"></a>**Code Template**:

    - Python
    
        ```python
        from sklearn.linear_model import LinearRegression
        lin_reg = LinearRegression()
        lin_reg.fit(X_train, y_train)
        y_pred = lin_reg.predict(X_test, y_test)
        ```
    
    - R
    
        ```R
        lin_reg = lm(formula = y ~ ., data = training_set)
        y_pred = predict(lin_reg, newdata = test_set)
        ```

&emsp;<a name="poly"></a>
## Polynomial Regression

- Polynomial Regression $\in$ Linear Regression ($f$ = a linear function of unknown parameters $w$)

- <a name="polyprep"></a>**Different Preprocessing**:

    $$\begin{equation}
    X=\begin{bmatrix}
    1 & x_{11} & \cdots & x_{1n} & x_{11}^2 & \cdots & x_{1n}^p \\
    \vdots &  & \vdots &  &  & \vdots &  \\
    1 & x_{m1} & \cdots & x_{mn} & x_{m1}^2 & \cdots & x_{mn}^p \\
    \end{bmatrix}
    \end{equation}$$
    
    with the width of $p\times n+1$.

- Everything else is exactly the same as [linear regression](#linreg).

- Sample models:
    - 3rd order with 1 feature: $y_i=w_0+w_1x_i+w_2x_i^2+w_3x_i^3$
    - 2nd order with 2 features: $y_i=w_0+w_1x_{i1}+w_2x_{i2}+w_3x_{i1}^2+w_4x_{i2}^2$  
<br>
- <a name="polycode"></a>**Code Template**:

    - Python
    
        ```python
        from sklearn.preprocessing import PolynomialFeatures
        poly_reg = PolynomialFeatures(degree = 3)
        X_poly = poly_reg.fit_transform(X_train)
        X_pred = poly_reg.fit_transform(X_test)
        
        from sklearn.linear_model import LinearRegression
        lin_reg = LinearRegression()
        lin_reg.fit(X_poly, y_train)
        y_pred = lin_reg.predict(X_pred, y_test)
        ```
    
    - R
    
        ```R
        # Sample preprocessing of dataset
        dataset$col2 = dataset$col^2
        dataset$col3 = dataset$col^3
        ......
        
        poly_reg = lm(formula = y ~ ., data = training_set)
        y_pred = predict(poly_reg, newdata = test_set)
        ```
<br>
- <a name="polyex"></a>**Further Extensions**: we can generalize our linear regression model as:

    $$\begin{equation}
    \hat{y}_ i\approx f(x_i,w)=\sum_{s=1}^S{g_s(x_i)w_s}
    \end{equation}$$
    
    where $g_s(x_i)$ can be any function of $x_i$, such as $e^{x_{ij}},\ \log{x_{ij}},\ ...$.
    
    Everything else is still the same as [linear regression](#linreg).

&emsp;<a name="lwr"></a>
## Locally Weighted Linear Regression

- <a name="lwrprob"></a>**Problem Setting**

    - **Underfitting**: the model barely fits the data points.

        <center><img src="../../images/ML/underfit.png" height="200"/></center>

        One single line is usually not enough to capture the pattern of $x\ \&\ y$. In order to get a better fit, we add more polynomial features ($x^j$) to the original model:

    - **Overfitting**: the model fits the given data points too well that it cannot be used on other data points.

        <center><img src="../../images/ML/overfit.png" height="200"/></center>

        When we add too much (e.g. $y=\sum_{j=0}^{9}w_jx^j$), the model captures the pattern of the given data points $(x_i,y_i)$ too much that it cannot perform well on new data points.
        
- **Intuition**: When we would like to estimate $y$ at a certain $x$, instead of applying the original LinReg, we take a subset of data points $(x_i,y_i)$ around $x$ and try to do LinReg on that subset only so that we can get a more accurate estimation.

- <a name="lwrmodel"></a>**Model: Weighted LS**

   - Original LinReg

        $$\begin{equation}
        w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}(y_i-w^Tx_i)^2
        \end{equation}$$

        We find the $w$ that minimizes the cost function (maximizes the likelihood function) so that our model is optimized to fit the data.

    - LWR

        $$\begin{equation}
        w\leftarrow\mathop{\arg\min}_ {w}\sum_{i=1}^{m}e^{-\frac{(x_i-x)^2}{2\tau^2}}\cdot(y_i-w^Tx_i)^2
        \end{equation}$$

        We add the weight function $\mathcal{W}_ i=e^{-\frac{(x_i-x)^2}{2\tau^2}}$ to the OLS, where

        - **Numerator**:

            $$\begin{align}
            &\text{If}\ |x_i-x|=\text{small} \longrightarrow W_i\approx 1 \\
            &\text{If}\ |x_i-x|=\text{large} \longrightarrow W_i\approx 0
            \end{align}$$

        - **Bandwidth Parameter**: $\tau$ (how fast the weight of $x_i$ falls off the query point $x$)
            
            $$\begin{align}
            &\text{When}\ \tau > > 1, \text{LWR} \approx \text{LinReg} \\
            &\text{When}\ \tau < < 1, \text{LWR} \rightarrow \text{overfitting}
            \end{align}$$
        
        - **Exact Solution**:
        
            $$\begin{align}
            \DeclareMathOperator{\Tr}{tr}
            \nabla_w\mathcal{L}(w)&=\nabla_w \mathcal{W}(Xw-y)^T(Xw-y) \\
            &\Rightarrow X^T\mathcal{W}Xw-X^T\mathcal{W}y=0 \\
            \Rightarrow w&=(X^T\mathcal{W}X)^{-1}X^T\mathcal{W}y
            \end{align}$$

- <a name="lwrcode"></a>**Code Template**

    - Python: for simple linreg ($n=1$)
    
        ```python
        def get_weight(x,x_ref,tau):
            weight = np.zeros([m,m])
            for i in range(m):
                weight[i,i] = np.exp((x[i]-x_ref)**2/(-2*tau**2))
            return weight
            
        def lwr(x,y,tau):
            y_est = np.zeros(m)
            for i in range(m):
                weight = get_weight(x,x[i],tau)
                w = np.linalg.inv(x.T.dot(weight).dot(x)).dot(x.T).dot(w).dot(y)
                y_est[i] = w[0]+w[1]*x[i] # Change this line when n>1
            return y_est
        
        ......
        
        tau = 1 # The optimal is 1 imo but change whenever necessary
        y_lwr = lwr(x,y,tau)
        ```

&emsp;<a name="ridge"></a>
## Ridge Regression

- <a name="ridgeprob"></a>**Problem Setting**

    - The OLS LinReg method gives us an accurate expected value: $\mathbb{E}[w_{ML}]=w$.
    
    - However, the **variance** $\text{Var}[w_{ML}]=\sigma^2(X^TX)^{-1}$ could be **too large** that it ruins our model parameters.
    
    - Ridge Regression $\in$ [regularization](#regular) methods
    
- **Model**

    $$\begin{equation}
    w_{RR}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda\|w\|^2_2
    \end{equation}$$
    
    - $\lambda$: regularization parameter: 
        
        $$\begin{align}
        &\text{If}\ \lambda\rightarrow0\ \ \Longrightarrow w_{RR}\rightarrow w_{LS} \\
        &\text{If}\ \lambda\rightarrow\infty \Longrightarrow w_{RR}\rightarrow \bf{0}
        \end{align}$$
        
    - $g(w)=\\|w\\|^2_2=w^Tw$: L2 penalty function
    
- **Solution**

    $$\begin{align}
    \mathcal{L}&=(y-Xw)^T(y-Xw)+\lambda w^Tw \\
    \nabla_w\mathcal{L}&=-2X^Ty+2X^TXw+2\lambda w=0 \\
    \Rightarrow w_{RR}&=(X^TX+\lambda I)^{-1}X^Ty
    \end{align}$$
    
- **Data Preprocessing: Standardization**

    - $y$:
    
        $$\begin{equation}
        y\leftarrow y-\frac{1}{m}\sum_{i=1}^m{y_i}
        \end{equation}$$

    - $x$:
    
        $$\begin{equation}
        x_{ij}\leftarrow \frac{x_{ij}-\bar{x}_ j}{\sqrt{\frac{1}{m}\sum_{i=1}^m{(x_{ij}-\bar{x}_ j)^2}}}
        \end{equation}$$

<br>
- <a name="svd"></a>**Singular Value Decomposition**

    - <u>Definition</u>: We can write any $n\times d\ (n>d)$ matrix $X$ as $X=USV^T$.
        
        - $U$: left singular vectors $(m\times r)$
            - orthonormal in cols (i.e. $U^TU=I$)
        
        - $S$: singular values $(r\times r)$
            - non-negative diagonal (i.e. $S_{ii}\geq0, S_{ij}=0\ \forall i\neq j$)
            - sorted in decreasing order (i.e. $\sigma_1\geq\sigma_2\geq\cdots\geq0$)
        
        - $V$: right singular vectors $(n\times r)$
            - orthonormal (i.e. $V^TV=VV^T=I$)  
        
        - $m$: #samples
        - $n$: #features
        - $r$: #concepts $(r=\text{rank}(X))$
        - $\sigma_i$: the strength of the $i$th concept  
    <br>
    - <u>Properties</u>:
    
        $$\begin{align}
        X^TX&=VS^2V^T \\
        XX^T&=US^2U^T \\
        \text{If}\ \forall i: S_{ii}\neq0 &\Rightarrow (X^TX)^{-1}=VS^{-2}V^T 
        \end{align}$$
    
    - <u>Intuition</u>:
        
        $$\begin{equation}
        X=USV^T=\sum{\sigma_i\bf{u_i\times v_i^T}}
        \end{equation}$$
        
        <center><img src="../../images/ML/svd.png" height="200"/></center>
        
        Why do we need this? What's the practical use of this??  
        
        As an example, suppose we would like to analyze a dataset of the relationship between <u>Users & Movies</u>, in which:
        
        - Each row = a user
        - Each col = a movie
        - Each entry $X_{ij}$ = the rating of movie $j$ from user $i$ (0=unwatched, 1=hate, 5=love)  
        <br>
        
        And here is the situation: 

        <center><img src="../../images/ML/lagunita.jpg" height="300"/></center>
        <center>cited from Stanford's <a href="https://lagunita.stanford.edu/courses/course-v1:ComputerScience+MMDS+SelfPaced/about">Mining Massive Datasets</a></center>
        <br>
        - $U=$ "User-to-Concept" similarity matrix
            - $U\[:,1]=$ Sci-fi concept of users
            - $U\[:,2]=$ Romance concept of users  
        <br>
        - $S=$ "Strength of Concept" matrix
            - $S\[1,1]=$ Strength of Sci-fi concept
            - $S\[2,2]=$ Strength of Romance concept
            - $\because S\[3,3]$ is very small $\therefore$ we can ignore this concept and also ignore $U\[:,3]$ and $V^T\[3,:]$.  
        <br>
        - $V^T=$ "Movie-to-Concept" similarity matrix
            - $V^T\[1,1:3]=$ Sci-fi concept of the Sci-fi movies
            - $V^T\[2,4:5]=$ Romance concept of the Romance movies  
    <br>
    - <a name="svdcalc"></a><u>Calculation of SVD</u>:
        
        1. $X^TX=VS^2V^T\Rightarrow$ calculate $V,S^2$
            - $S^2\ni$ eigenvalues
            - $V\ni$ eigenvectors
        2. $XV=US^2\Rightarrow$ calculate $U$
        3. GG.  
<br><br>
- <a name="ridgevsls"></a>**Ridge Regression vs Least Squares LinReg**

    $$\begin{equation}
    w_{\text{LS}}=(X^TX)^{-1}X^Ty\ \Leftrightarrow\ w_{\text{RR}}=(\lambda I+X^TX)^{-1}X^Ty
    \end{equation}$$
    
    - <u>Problems with LS</u>:
    
        1. $\text{Var}\[w_{ML}]=\sigma^2(X^TX)^{-1}=\sigma^2VS^{-2}V^T$
        
            When $S_{ii}$ is very small for some values of $i$, $\text{Var}\[w_{ML}]$ is very large.
            
        2. $y_{\text{new}}=x_{\text{new}}^Tw_{LS}=x_{\text{new}}^T(X^TX)^{-1}X^Ty=x_{\text{new}}^TVS^{-1}U^Ty$
        
            When $S^{-1}$ has very large values, our prediction will be very unstable.
    <br><br>
    - <u>LS = a special case of RR</u>:
    
        $$\begin{align}
        w_{\text{RR}}&=(\lambda I+X^TX)^{-1}X^Ty \\
        &=(\lambda I+X^TX)^{-1}(X^TX)(X^TX)^{-1}X^Ty \\
        &=[(X^TX)(\lambda(X^TX)^{-1}+I)]^{-1}(X^TX)w_{\text{LS}} \\
        &=(\lambda(X^TX)^{-1}+I)^{-1}w_{\text{LS}} \\
        &=(\lambda VS^{-2}V^T+I)^{-1}w_{\text{LS}} \\
        &=V(\lambda S^{-2}+I)^{-1}V^Tw_{\text{LS}}\ \ \ \ \ \ \ \ \ |\ \ \ VV^T=I\\
        &:=VMV^Tw_{\text{LS}}
        \end{align}$$
        
        where $M=(\lambda S^{-2}+I)^{-1}$ is a diagonal matrix with $M_{ii}=\frac{S_{ii}^2}{\lambda+S_{ii}^2}$,
        
        $$\begin{align}
        w_{\text{RR}}&:=VMV^Tw_{\text{LS}} \\
        &=V(\lambda S^{-2}+I)^{-1}V^T(VS^{-1}U^Ty) \\
        &=VS^{-1}_ \lambda U^Ty \\
        \end{align}$$
        
        where $S_\lambda^{-1}$ is a diagonal matrix with $S_{ii}=\frac{S_{ii}}{\lambda+S_{ii}^2}$.  
        
        Therefore, we get another clearer expression of the relationship between RR and LS:
        
        $$\begin{equation}
        w_{\text{LS}}=VS^{-1}U^Ty\ \Leftrightarrow\ w_{\text{RR}}=VS_\lambda^{-1}U^Ty
        \end{equation}$$
        
        And $w_{LS}$ is simply a special case of $w_{RR}$ where $\lambda=0$.
    <br><br>
    - <u>RR = a special case of LS</u>:
    
        If we do some preprocessing to our model $\hat{y}\approx\hat{X}w$:
        
        $$\begin{equation}\begin{bmatrix}
        y \\ 0 \\ \vdots \\ 0
        \end{bmatrix}\approx\begin{bmatrix}
        - & X & - \\ \sqrt{\lambda} & & 0 \\ & \ddots & \\ 0 & & \sqrt{\lambda}
        \end{bmatrix}\begin{bmatrix}
        w_1 \\ \vdots \\ w_n
        \end{bmatrix}\end{equation}$$
        
        Now we have the exact same loss function:
        
        $$\begin{equation}
        (\hat{y}-\hat{X}w)^T(\hat{y}-\hat{X}w)=\|y-Xw\|^2+\lambda\|w\|^2
        \end{equation}$$

<br>
- **Probabilistic Interpretation**

    - **Expected Value**:
    
        $$\begin{equation}
        \mathbb{E}[w_{\text{RR}}]=(\lambda I+X^TX)^{-1}X^TXw
        \end{equation}$$
        
    - **Variance**:
    
        $$\begin{align}
        \text{Var}[w_{\text{RR}}]&=\mathbb{E}[w_{\text{RR}}w_{\text{RR}}^T]-\mathbb{E}[w_{\text{RR}}]\mathbb{E}[w_{\text{RR}}]^T \\
        &=(\lambda I+X^TX)^{-1}X^T\mathbb{E}[yy^T]X(\lambda I+X^TX)^{-1^T} \\
        &\ \ \ \ \ -(\lambda I+X^TX)^{-1}X^TXww^TX^TX(\lambda I+X^TX)^{-1^T} \\
        &=(\lambda I+X^TX)^{-1}X^T(\sigma^2I)X(\lambda I+X^TX)^{-1^T}\ \ (1) \\
        &=\sigma^2Z(X^TX)^{-1}Z^T \\
        \end{align}$$
        
        where $Z=(I+\lambda(X^TX)^{-1})^{-1}$.
        
    - [See more info](#bvto)

- <a name="code"></a>**Code Template**:

    - Python
    
        ```python
        from sklearn.linear_model import Ridge,RidgeCV
        
        # find optimal lambda through cross validation
        Lambdas=np.logspace(-5,2,200)
        ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)
        ridge_cv.fit(x_train,y_train)
        
        ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)
        ridge.fit(x_train,y_train)
        
        ridge_pred=ridge.predict(x_test)
        ```
        
    - R
    
        ```R
        library(glmnet)
        
        # find optimal lambda through cross validation
        lambda_seq = 10^seq(2, -2, by=-.1)
        cv_output = cv.glmnet(x_train, y_train, alpha=0, lambda=lambda_seq)
        best_lam = cv_output$lambda.min
        
        ridge_reg = glmnet(x_train, y_train, alpha=0, lambda=best_lam)
        # summary(fit)
        pred = predict(ridge_reg, s=best_lam, newx=x_test)
        ```

<br>
&emsp;<a name="bvto"></a>
### <strong>Bias-Variance Trade-off</strong>

- **Ridge vs LS**:

    |  | LS | Ridge |
    |:-:|:-:|:-:|
    | Expected value | $\mathbb{E}\[w_{\text{LS}}]=w$ | $\mathbb{E}\[w_{\text{RR}}]=(\lambda I+X^TX)^{-1}X^TXw$ |
    | Variance | $\text{Var}\[w_{\text{LS}}]=\sigma^2(X^TX)^{-1}$ | $\text{Var}\[w_{\text{LS}}]=\sigma^2Z(X^TX)^{-1}Z^T$ |

    The distribution of $w_{\text{RR}}$ is not centered at $w$, but the variance gets much smaller.

&emsp;<a name="lasso"></a>
## Lasso Regression

- Everything is the same as [Ridge Regression](#ridge) except the **model**:

    $$\begin{equation}
    w_{\text{lasso}}=\mathop{\arg\min}_ {w}\|{y-Xw}\|^2+\lambda\|w\|_ 1
    \end{equation}$$
        
    - $g(w)=\\|w\\|_ 1=\|w\|$: L1 penalty function  
<br>    
- **Solution**: we are yet able to find a solution to the Multivariate LASSO because of the absolute value.

- **Code Template**:

    - Python
    
        ```python
        from sklearn.linear_model import Lasso,LassoCV
        
        # find optimal lambda through cross validation
        Lambdas=np.logspace(-5,2,200)
        lasso_cv=LassoCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)
        lasso_cv.fit(x_train,y_train)
        
        lasso=Lasso(alpha=lasso_cv.alpha_,normalize=True)
        lasso.fit(x_train,y_train)
        
        lasso_pred=lasso.predict(x_test)
        ```
        
    - R
        
        ```R
        library(glmnet)
        
        # find optimal lambda through cross validation
        lambda_seq = 10^seq(2, -2, by=-.1)
        cv_output = cv.glmnet(x_train, y_train, alpha=1, lambda=lambda_seq)
        best_lam = cv_output$lambda.min
        
        lasso_reg = glmnet(x_train, y_train, alpha=1, lambda=best_lam)
        # summary(fit)
        pred = predict(lasso_reg, s=best_lam, newx=x_test)
        ```