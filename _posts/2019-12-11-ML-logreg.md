---
layout: post
title: "Logistic Regression"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/logreg/
header-includes:
- \usepackage{amsmath}
---
## Intro

This covers the basics of two basic regressions as the very basics of Machine Learning.
- [Linear Regression](#linreg)
- [Logistic Regression](#logreg)


## Notations

- $i$ th training example: $(x^{(i)},y^{(i)}) \| x\in \mathbb{R}^n, y\in \{0,1\}$
- $m$ = # training examples
- $n$ = # features
- $x_j$ = the $j$th feature (assume $x_0=1$)
- $w_j$ = the weight for $j$th feature (assume $w_0=b:\sum_{i=1}^{n}w_i x_i+b = \sum_{i=0}^{n}w_i x_i$)
- $\hat{y}=h(x)$= the hypothetical model

## <a name="logreg"></a>Logistic Regression (Classification)

- Model

    $$\begin{equation}
    \hat{y}=g(w^Tx)
    \end{equation}$$

    $g(z)$: a function that converts $w^Tx$ to binary value

- Sigmoid Function (see Deep Learning for more funcs)

    $$\begin{equation}
    g(z)=\sigma(z)=\frac{1}{1+e^{-z}}
    \end{equation}$$
    
    - Derivative (you will know why we need this in Deep Learning)
    
        $$\begin{align}
        g'(z)&=\frac{d}{dz}\frac{1}{1+e^{-z}} \\
        &=\frac{e^{-z}(+1-1)}{(1+e^{-z})^2} \\
        &=g(z)(1-g(z))
        \end{align}$$

- Cost Function


    1. single training example (derivation later)
    
        $$\begin{equation}
        \mathcal{L}(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log{(1-\hat{y})})
        \end{equation}$$
        
        If $y=1\rightarrow\mathcal{L}(\hat{y},y)=-\log{\hat{y}}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\uparrow$"$\rightarrow\hat{y}=1$   
        If $y=0\rightarrow\mathcal{L}(\hat{y},y)=-\log{(1-\hat{y})}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\downarrow$"$\rightarrow\hat{y}=0$ 
        
    2. entire training set
    
        $$\begin{equation}
        \mathcal{J}(w)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)},y^{(i)})=\text{mean}(\mathcal{L})
        \end{equation}$$

- Probabilistic Interpretation

    1. Assumptions
    
        $$\begin{align}
        P(y=1|x,w)&=\hat{y} \\
        P(y=0|x,w)&=1-\hat{y}
        \end{align}$$

    2. Probabilistic Model of LogReg
    
        $$\begin{equation}
        p(y|x,w)=\hat{y}^y(1-\hat{y})^{1-y}
        \end{equation}$$
        
    3. Likelihood Function
    
        $$\begin{equation}
        L(w)=\prod_{i=1}^{m}(\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}
        \end{equation}$$
        
    4. Log Likelihood
    
        $$\begin{align}
        l(w)&=\sum_{i=1}^{m}(y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}) \\
        l(w)&=-\sum_{i=1}^{m}\mathcal{L}(\hat{y},y)
        \end{align}$$
        
    5. MLE
        
        $$\begin{align}
        \frac{\partial l(w)}{\partial w_j}&=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})\frac{\partial g(w^Tx)}{\partial w_j} \\
        &=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})g(w^Tx)(1-g(w^Tx))\frac{\partial(w^Tx)}{\partial w_j} \\
        &=(y(1-g(w^Tx))-(1-y)g(w^Tx))x_j \\
        &=(y-\hat{y})x_j
        \end{align}$$

- Gradient Descent

    $$\begin{align}
    w_j &:= w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j} \\
    &=w_j+\alpha(y-\hat{y})x_j
    \end{align}$$
    
    Why is it also called "Gradient Ascent"?  
    $\because$ we are trying to minimize the loss function $\Leftrightarrow$ maximize the likelihood function

- Gradient Descent - Newton's Method
    1. Newton's formula
    
        $$\begin{equation}
        w := w-\frac{f(w)}{f'(w)}
        \end{equation}$$
        
    2. Newton-Raphson Method in GD
    
        $$\begin{equation}
        w := w-H^{-1}\nabla_wl(w)
        \end{equation}$$
        
        $H$: Hessian Matrix
        
        $$\begin{equation}
        H_{ij}=\frac{\partial^2l(w)}{\partial w_i \partial w_j}
        \end{equation}$$

    3. Newton vs normal GD
        - YES: faster convergence, fewer iterations
        - NO:  expensive computing (inverse of a matrix)


        
        
        
        
        