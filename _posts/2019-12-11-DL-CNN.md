---
layout: post
title: "Convolutional Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/CNN/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Claim: some of the images in this session are cited from Andrew Ng's <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning</a> specialization.

Roadmap:
- Basics of CNN
    - [Intuition of CNN](#cnn)
    - [General Formula of Convolution](#formula)
    - [CNN Layers](#layers)
        - Convolution
        - [Pooling](#pool)
        - [Fully Connected](#fc)
- Widely Used CNNs
    - [LeNet-5](#lenet)
    - [AlexNet](#alexnet)
    - [VGG](#vgg)
    - Inception (the most powerful CNN as far as I know)
        - [ResNets](#res) (not CNN but prerequisite for understanding Inception)
        - [1x1 Conv](#nin) (i.e. NiN)
        - [The Inception](#inception) (We need to go deeper!)
- [Object Detection](#od)
    - Bounding Box
    - Landmark Detection
    - Sliding Windows
    - Intersection over Union
    - [YOLO (You Only Look Once)](#yolo)
        - Non-Max Suppression
        - Anchor Boxes
    - [R-CNN](#rcnn) (will talk about this in the future)
- [Face Recognition](#fr)
    - [Siamese Network](#sn)
    - Triplet Loss
    - [Binary Classification]
    - [Neural Style Transfer]

&emsp;
## Basics of CNN

- <a name="cnn"></a>**Intuition of CNN**
    
    <center><img src="../../images/DL/cnn.gif" width="500"/></center>
    <br/>
    - CNN is mostly used in Computer Vision (image classification, object detection, neural style transfer, etc.)  
    
    - **Input**: images $\rightarrow$ volume of numerical values in the shape of **width $\times$ height $\times$ color-scale** (color-scale=3 $\rightarrow$ RGB; color-scale=1 $\rightarrow$ BW)  
    
        In the gif above, the input shape is $5\times5\times3$, meaning that the image is colored and the image size $5\times5$. The "$7\times7\times3$" results from **padding**, which will be discussed below.
    
    - **Convolution**: 
        1. For each color layer of the input image, we apply a 2d **filter** that **scans** through the layer in order.
        2. For each block that the filter scans, we **multiply** the corresponding filter value and the cell value, and we **sum** them up.
        3. We **sum** up the output values from all layers of the filter (and add a bias value to it) and **output** this value to the corresponding output cell. 
        4. (If there are multiple filters, ) After the first filter finishes scanning, the next filter starts scanning and outputs into a new layer.  
    <br/>
    - In the gif above, 
        1. Apply 2 filters of the shape $3\times3\times3$.
        2. 1st filter - 1st layer - 1st block: 
        
            $$\begin{equation}
            0+0+0+0+0+0+0+(1\times-1)+0=-1
            \end{equation}$$
            
            1st filter - 2nd layer - 1st block:
            
            $$\begin{equation}
            0+0+0+0+(2\times-1)+(1\times1)+0+(2\times1)+0=1
            \end{equation}$$
            
            1st filter - 3rd layer - 1st block:
            
            $$\begin{equation}
            0+0+0+0+(2\times1)+0+0+(1\times-1)+0=1
            \end{equation}$$
            
        3. Sum up + bias $\rightarrow$ 1st cell of 1st output layer
            
            $$\begin{equation}
            -1+1+1+1=2
            \end{equation}$$
    
        4. Repeat till we finish scanning  
<br/>
- **Edge Detection & Filter**

    - Sample filters
    
        <center><img src="../../images/DL/edgedetect.png" width="500"/></center>
        
        - Gray Scale: 1 = lighter, 0 = gray, -1 = darker  
    <br/>
    - Notice that we don't really need to define any filter values. Instead, we are supposed to train the filter values.  
    All the convolution operations above are just the same as the operations in ANN. Filters here correspond to $W$ in ANN.  
    
- **Padding**

    - Problem: corner cells & edge cells are detected much fewer times than the middle cells $\rightarrow$ info loss of corner & edge
    
    - Solution: pad the edges of the image with "0" cells (as shown in the gif above)
    
- **Stride**: the step size the filter takes ($s=2$ in the gif above)

- <a name="formula"></a>**General Formula of Convolution**: 

    $$\begin{equation}
    \text{Output Size}=\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor\times\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor
    \end{equation}$$
    
    - $n\times n$: image size
    - $f\times f$: filter size
    - $p$: padding
    - $s$: stride
    - Floor: ignore the computation when the filter sweeps the region outside the image matrix  
<br/>
- <a name="layers"></a>**CNN Layers**:

    - **Convolution** (CONV): as described above
    
    - <a name="pool"></a>**Pooling** (POOL): to reduce #params & computations (most common pooling size = $2\times2$)
    
        - Max Pooling
        
            <center><img src="../../images/DL/maxpool.png" height="200"/></center>
            
            1. Divide the matrix evenly into regions
            2. Take the max value in that region as output value  
            <br/>
        - Average Pooling
        
            <center><img src="../../images/DL/avgpool.png" height="190"/></center>
            
            1. Divide the matrix evenly into regions
            2. Take the average value of the cells in that region as output value  
            <br/>
        - Stochastic Pooling
        
            <center><img src="../../images/DL/stochasticpool.png" height="200"/></center>
            
            1. Divide the matrix evenly into regions
            2. Normalize each cell based on the regional sum:
            
                $$\begin{equation}
                p_i=\frac{a_i}{\sum_{k\in R_j}{a_k}}
                \end{equation}$$
                
            3. Take a random cell based on multinomial distribution as output value  
        <br/>
    - <a name="fc"></a>**Fully Connected** (FC): to flatten the 2D/3D matrices into a single vector (each neuron is connected with all input values)
    
        <center><img src="../../images/DL/fullyconnected.png" width="300"/></center>

&emsp;
## Widely Used CNNs

<a name="lenet"></a>**LeNet-5**: LeNet-5 Digit Recognizer
        
<center><img src="../../images/DL/cnneg.png"/></center>  

|  Layer  |  Shape  | Total Size | #params |
| :-----: | :-----: | :--------: | :-----: |
| INPUT | 32 x 32 x 3 | 3072 | 0 |
| CONV1 (Layer 1) | 28 x 28 x 6 | 4704 | 156 |
| POOL1 (Layer 1) | 14 x 14 x 6 | 1176 | 0 |
| CONV2 (Layer 2) | 10 x 10 x 16 | 1600 | 416 |
| POOL2 (Layer 2) | 5 x 5 x 16 | 400 | 0 |
| FC3 (Layer 3) | 120 x 1 | 120 | 48001 |
| FC4 (Layer 4) | 84 x 1 | 84 | 10081 |
| Softmax | 10 x 1 | 10 | 841 |

- Calculation of #params for CONV: $(f\times f+1)\times n_f$
    - $f$: filter size
    - $+1$: bias
    - $n_f$: #filter
 
<br/>
<a name="alexnet"></a>**AlexNet**: winner of 2012 ImageNet Large Scale Visual Recognition Challenge  

<center><img src="../../images/DL/alexnet.png"/></center><br/>  
    
|  Layer  |  Shape  | Total Size | #params |
| :-----: | :-----: | :--------: | :-----: |
| INPUT | 227 x 227 x 3 | 154587 | 0 |
| CONV1 (Layer 1) | 55 x 55 x 96 | 290400 | 11712 |
| POOL1 (Layer 1) | 27 x 27 x 96 | 69984 | 0 |
| CONV2 (Layer 2) | 27 x 27 x 256 | 186624 | 6656 |
| POOL2 (Layer 2) | 13 x 13 x 256 | 43264 | 0 |
| CONV3 (Layer 3) | 13 x 13 x 384 | 64896 | 3840 |
| CONV4 (Layer 3) | 13 x 13 x 384 | 64896 | 3840 |
| CONV5 (Layer 3) | 13 x 13 x 256 | 43264 | 2560 |
| POOL5 (Layer 3) | 6 x 6 x 256 | 9216 | 0 |
| FC5 (Flatten) | 9216 x 1 | 9216 | 0 |
| FC6 (Layer 4) | 4096 x 1 | 4096 | 37748737 |
| FC7 (Layer 5) | 4096 x 1 | 4096 | 16777217 |
| Softmax | 1000 x 1 | 1000 | 4096000 |

- Significantly bigger than LeNet-5 (60M params to be trained)
- Require multiple GPUs to speed the training up<br/><br/>  
    
<a name="vgg"></a>**VGG**: made by Visual Geometry Group from Oxford  

<center><img src="../../images/DL/vgg.png"/></center>  

- Too large: 138M params<br/><br/>  

**Inception**  

- <a name="res"></a>**ResNets** 

    - Residual Block

        <center><img src="../../images/DL/resnet.png" width="500"/></center>

        $$\begin{equation}
        a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
        \end{equation}$$
    
        Intuition: we add activation values from layer $l$ to the activation in layer $l+2$ 

    - Why ResNets?
    
        - ResNets allow parametrization for the identity function $f(x)=x$
        - ResNets are proven to be more effective than plain networks:
        
            <center><img src="../../images/DL/resnetperf.png" width="500"/></center>
            
        - ResNets add more complexity to the NN in a very simple way
        - The idea of ResNets further inspired the development of RNN  
<br/>
- <a name="nin"></a>**1x1 Conv** (i.e. Network in Network [NiN])  

    - WHY??? This sounds like the stupidest idea ever!!
    - Watch this.
        
        <center><img src="../../images/DL/1x1pt1.png" height="300"/></center><br/>
        
        <center>In a normal CNN layer like this, we need to do in total 210M calculations.</center><br/>
        
        <center><img src="../../images/DL/1x1pt2.png" height="300"/></center><br/>
    
        <center>However, if we add a 1x1 Conv layer in between, we only need to do in total 17M calculations.</center><br/>
    
    - Therefore, 1x1 Conv is significantly more useful than what newbies expect. When we would like to keep the matrix size but reduce #layers, using 1x1 Conv can significantly reduce #computations needed, thus requiring less computing power.  
<br/>
- <a name="inception"></a>**The Inception**: We need to go deeper!

    - Inception Module
    
        <center><img src="../../images/DL/incepm.png" width="400"/></center><br/>
        
    - Inception Network
    
        <center><img src="../../images/DL/incep.png" width="500"/></center>     

&emsp;
## <a name="od"></a>Object Detection

- Object Localization $\rightarrow$ 1 obj; Detection $\rightarrow$ multiple objs.

- **Bounding Box**: to capture the obj in the img with a box
    - Params: 
        - $b_x, b_y$ = central point
        - $b_h, b_w$ = full height/width
    - New target label (in place of image classification output):
        
        $$\begin{equation}
        y=\begin{bmatrix}
        p_c \\ b_x \\ b_y \\ b_h \\ b_w \\ c_1 \\ \vdots \\ c_n
        \end{bmatrix}
        \end{equation}$$
    
        - $p_c$: "is there any object in this box?"
            - if $p_c=0$, we ignore the remaining params
        - $c_i$: class label $i$ (e.g. $c_1$: cat, $c_2$: dog, $c_3$: bird, ...)  
<br/>      
- **Landmark Detection**: to capture the obj in the img with points
    - Params: $(l_{ix},l_{iy})$ = each landmark point
    - New target label:
    
        $$\begin{equation}
        y=\begin{bmatrix}
        p_c \\ l_{1x} \\ l_{1y} \\ \vdots \\ l_{nx} \\ l_{ny} \\ c_1 \\ \vdots \\ c_n
        \end{bmatrix}
        \end{equation}$$
        
    - THE LABELS MUST BE CONSISTENT!
        - Always start from the exact same location of the object! (e.g. if you start with the left corner of the left eye for one image, you should always start with the left corner of the left eye for all images.)
        - #landmarks should be the same! 

    <br/>
    I personally have a very awful experience with Landmark Detection. When the algorithms of object detection were not yet well-known in the IT industry, I worked on a project of digital screen defects detection in a Finnish company. Since digital screen defects are 1) black & white 2) in very simple geometric shapes, the usage of bounding boxes could have significantly reduced the complexity of both data collection and NN model building.<br/>  
    However, the team insisted to use landmark detection. Due to 1) that screen defects are unstructured 2) that the number of landmark points for two different screen defects can hardly be the same, the dataset was basically unusable, and none of the models we built could learn accurate patterns from it, leading to an unfortunate failure.<br/>  
    I personally would argue that bounding box is much better than landmark detection in most practical cases.
    
- **Sliding Window**

    <center><img src="../../images/DL/sliding.gif" width="500"/></center><br/>
    
    - Apply a sliding window with a fixed size to scan every part of the img left-right and top-bottom (just like CONV), and feed each part to CNN
    - In order to capture the same type of objects in different sizes and positions in the img, shrink the img (i.e. enlarge the sliding window) and scan again, and repeat.<br/>

    - Problem: HUGE computational cost!
    - Solution: (contemporary)
        1. Convert FC layer into CONV layer
        
            <center><img src="../../images/DL/slidingfc.jpg" width="700"/></center><br/>
        
        2. Share the former FC info with latter convolutions
        
            <center><img src="../../images/DL/sliding.png" width="700"/></center><br/>
            
            1. First run of the CNN.
            2. Second run of the same CNN with a bigger size of the same img (due to sliding window). Notice that the FC info from the first run is shared in the second run.
            3. Latter runs of the same CNN with bigger sizes of the same img (due to sliding window). Notice that the FC info from all previous runs is shared in this run, thus saving computation power and memories.  
<br/>           
- **Intersection over Union**

    <center><img src="../../images/DL/iou.png" width="200"/></center>  
    <center>Is the purple box a good prediction of the car location?</center>
    
    Intersection over Union is defined as:
    
    $$\begin{equation}
    \text{IoU}=\frac{\text{area of intersection}}{\text{area of union}}
    \end{equation}$$
    
    In this case, area of intersection is the intersection between the red and purple box, and area of union is the total area covered by the red and purple box.  
    If $\text{IoU}\leq 0.5$, then the prediction box is correct. (Other threshold values are also okay but 0.5 is conventional.)
    
- <a name="yolo"></a>**YOLO (You Only Look Once)**

    <center><img src="../../images/DL/yolo.jpg" width="300"/></center>

    - **Grids**: divide the image into grids & use each grid as a bounding box
        - when $p_c=0$, we ignore the entire grid
        - $p_c=1$ only when the central point of the object $\in$ the grid
        - target output: $Y.\text{shape}=n_{\text{grid}}\times n_{\text{grid}}\times y.\text{length}$  
    <br/>
    - **Non-Max Suppression**: what happens when the grid is too small to capture the entire object?
        <center><img src="../../images/DL/nms.jpg" width="500"/></center>
        
        1. Discard all boxes with $p_c\leq 0.6$
        2. Pick the box with the largest $p_c$ as the prediction
        3. Discard any remaining box with $\text{IoU}\geq 0.5$ with the prediction
        4. Repeat till there is only one box left.  
    <br/>
    - **Anchor Boxes**: what happens when two objects overlap? (e.g. a hot girl standing in front of a car)
        <center><img src="../../images/DL/anchor.jpg" width="300"/></center>
        
        1. Predefine Anchor boxes for different objects
        2. Redefine the target value as a combination of Anchor 1 + Anchor 2
        
            $$\begin{equation}
            y=\begin{bmatrix}
            p_{c1} \\
            \vdots \\ 
            p_{c2} \\
            \vdots 
            \end{bmatrix}
            \end{equation}$$
        
        3. Each object in the image is assigned to grid cell that contains object's central point & anchor box for the grid cell with the highest $\text{IoU}$  
    <br/>   
    - **General Procedure**:
    
        1. Divide the images into grids and label the objects
        2. Train the CNN
        3. Get the prediction for each anchor box in each grid cell
        4. Get rid of low probability predictions
        5. Get final predictions through non-max suppression for each class  
<br/>
- <a name="rcnn"></a>**R-CNN**

    TO BE CONTINUED

&emsp;
## <a name="fr"></a>Face Recognition

- Face Verification vs Face Recognition
    - Verification
        - Input image, name/ID
        - Output whether the input image is that of the claimed person (1:1)
    - Recognition
        - Input image
        - Output name/ID if the image is any of the $K$ ppl in the database (1:K)  
<br/>
- <a name="sn"></a>**Siamese Network**

    - **One Shot Learning**: learn a similarity function

        The major difference between normal image classification and face recognition is that we don't have enough training examples. Therefore, rather than learning image classification, we

        1. Calculate the degree of diff between the imgs as $d$
        2. If $d\leq\tau$: same person; If $d>\tau$: diff person

    - Preparation & Objective:
    
        - 
        