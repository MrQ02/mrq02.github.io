---
layout: post
title: "GDA"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/GDA/
header-includes:
- \usepackage{amsmath}
---

## Intro

This covers the basics of Generative Learning Algorithms for classification
- [Linear Regression](#linreg)
- [Logistic Regression](#logreg)

## Two Types of Learning Algorithms

- **Discriminative Learning Algorithms**

$$\begin{equation}
\text{model }p(y|x)\text{ directly}\ \ \ (X \Rightarrow Y)
\end{equation}$$

- **Generative Learning Algorithms**

$$\begin{equation}
\text{model }p(x|y)\ \&\ p(y)\Rightarrow\text{ use Bayes Theorem to get }p(y|x) 
\end{equation}$$

&emsp;
&emsp;

## Bayes Theorem (the core of GLAs)

$$\begin{equation}
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
\end{equation}$$

- **Prior**: &emsp;&emsp;$p(y)$
- **Posterior**: $p(y\|x)$
    
- Simplification:

    $\because$ we are trying to find the output $y$ with the highest probability given $x$  
    $\therefore$ we can simplify Bayes Theorem for our purpose:

    $$\begin{align}
    \mathop{\arg\max}_ {y}{p(y|x)}&=\mathop{\arg\max}_ {y}{\frac{p(x|y)p(y)}{p(x)}} \\
    &=\mathop{\arg\max}_ {y}{p(x|y)p(y)}
    \end{align}$$

&emsp;
&emsp;

## Gaussian Discriminant Analysis (GDA)

- **Multivariate Gaussian Distribution**

    $$\begin{equation}
    p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\big)}
    \end{equation}$$
    
    Don't be scared of the formula. It is literally the same as Gaussian Distribution but with vector parameters:
    
    - mean vector: &emsp;&emsp;&nbsp;$\mu\in\mathbb{R}^n$
    - covariance matrix: $\Sigma\in\mathbb{R}^{n\times n}$  
    &emsp;  
    
    As a reminder and a comparison, here is the univariate version:
    
    $$\begin{equation}
    p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}$$
    
- GDA Model

    $$\begin{align}
    y&\sim \text{Bernoulli}{(\phi)} \\
    x|y=0&\sim N(\mu_0,\Sigma) \\
    x|y=1&\sim N(\mu_1,\Sigma) \\
    \end{align}$$
        
- Probabilistic Interpretation

    $$\begin{align}
    p(y)&=\phi^y(1-\phi)^{1-y} \\
    p(x|y=0)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\big)} \\
    p(x|y=1)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\big)}
    \end{align}$$
        
- log likelihood

    $$\begin{equation}
    l(\phi,\mu_0,\mu_1,\Sigma)=\log{\prod_{i=1}^{m}{p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)}}
    \end{equation}$$
        
- MLE

    $$\begin{align}
    \phi &= \frac{1}{m}\sum_{i=1}^m{\text{I}\{ y^{(i)}=l \}} \\
    \mu_0 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}}} \\
    \mu_1 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}}} \\
    \Sigma &= \frac{1}{m}\sum_{i=1}^m{(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T}
    \end{align}$$
    
    Derivation: just take the partial derivatives $\frac{\partial l}{\partial \text{param}}$ by yourself.
    
- What is GDA doing?

    ![](../../images/ML/GDA.png)
    
    