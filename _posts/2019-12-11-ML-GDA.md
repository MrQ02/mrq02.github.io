---
layout: post
title: "GDA"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/GDA/
header-includes:
- \usepackage{amsmath}
---

## Intro

This covers the basics of Generative Learning Algorithms for classification
- [Linear Regression](#linreg)
- [Logistic Regression](#logreg)

## Learning Algorithms

- Discriminative Learning Algorithms

$$\begin{equation}
\text{model }p(y|x)\text{ directly}\ \ \ (X \Rightarrow Y)
\end{equation}$$

- Generative Learning Algorithms

$$\begin{equation}
\text{model }p(x|y)\ \&\ p(y)\Rightarrow\text{ use Bayes Theorem to get }p(y|x) 
\end{equation}$$

&emsp;
&emsp;

## Bayes Theorem

$$\begin{equation}
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
\end{equation}$$

- **Prior**: &emsp;&emsp;$p(y)$
- **Posterior**: $p(y\|x)$
    
- Simplification:

    $\because$ we are trying to find the output $y$ with the highest probability given $x$  
    $\therefore$ we can simplify Bayes Theorem for our purpose:

    $$\begin{align}
    \mathop{\arg\max}_ {y}{p(y|x)}&=\mathop{\arg\max}_ {y}{\frac{p(x|y)p(y)}{p(x)}} \\
    &=\mathop{\arg\max}_ {y}{p(x|y)p(y)}
    \end{align}$$

- Bayes Theorem = the core of Generative Learning Algorithms

&emsp;
&emsp;

## Gaussian Discriminant Analysis (GDA)

- Assumption: Multivariate Gaussian Distribution

    $$\begin{equation}
    p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\big)}
    \end{equation}$$
    
    It is literally the same as Gaussian Distribution but with vector parameters:
    
    - mean vector: &emsp;&emsp;&nbsp;$\mu\in\mathbb{R}^n$
    - covariance matrix: $\Sigma\in\mathbb{R}^{n\times n}$  
    &emsp;  
    
    As a reminder and a comparison, here is the univariate version:
    
    $$\begin{equation}
    p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}$$
    
- Model

    $$\begin{align}
    y&\sim \text{Bernoulli}{(\phi)} \\
    x|y=0&\sim N(\mu_0,\Sigma) \\
    x|y=1&\sim N(\mu_1,\Sigma) \\
    \end{align}$$
        
- Probabilistic Interpretation

    $$\begin{align}
    p(y)&=\phi^y(1-\phi)^{1-y} \\
    p(x|y=0)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\big)} \\
    p(x|y=1)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\big)}
    \end{align}$$
        
- log likelihood

    $$\begin{equation}
    l(\phi,\mu_0,\mu_1,\Sigma)=\log{\prod_{i=1}^{m}{p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)}}
    \end{equation}$$
        
- MLE

    $$\begin{align}
    \phi &= \frac{1}{m}\sum_{i=1}^m{\text{I}\{ y^{(i)}=l \}} \\
    \mu_0 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}}} \\
    \mu_1 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}}} \\
    \Sigma &= \frac{1}{m}\sum_{i=1}^m{(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T}
    \end{align}$$
    
- GDA vs LogReg
    1. GDA
        - makes **stronger** modeling assumptions about data
        - data efficient when assumptions (Gaussian distributions) are approximately correct
    2. LogReg
        - makes **weaker** modeling assumptions about data
        - data efficient when assumptions (Gaussian distributions) are not necessarily correct (e.g. $x\|y\sim \text{Poisson}(\lambda_1)$ instead of $N(\mu_0,\Sigma)$)

&emsp;
&emsp;

## Naive Bayes Classifier

- GDA vs NB
    1. GDA: $x$ = continuous, real-valued vectors
    2. NB: &nbsp;&nbsp;$x$ = discrete-valued vectors (e.g. text classification)  
    &emsp;
- Text Encoding (more in DL/RNN)  
    We encode a text sentence into a vector of the same length as our **dictionary** (like a Python dictionary with vocabulary and their indices as key-value pairs):
    
    $$\begin{equation}
    x=\begin{bmatrix}
    0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 1 \\ 1 \\ \vdots \\ 0
    \end{bmatrix}
    \begin{matrix}
    \text{a} \\ \text{abandon} \\ \vdots \\ \text{pewdiepie} \\ \vdots \\ \text{subscribe} \\ \text{to} \\ \vdots \\ \text{zuck}
    \end{matrix}
    \end{equation}$$
    
    The original sentence was "Subscribe to Pewdiepie!", and this text encoding method uses lowercases, throws punctuations and ignores the order of the sentence. This is convenient in some cases (e.g. spam email classification) but awful in the other cases (e.g. news/report-writer bots)
    
    Notice that $x\in \\{0,1\\}^{\text{len(dict)}}$. Why notice this? Because we now have $2^\text{len(dict)}$ possible outcomes for $x$. When we have a dictionary of over 20000 words, we have a $\(2^{20000}-1\)$-dimensional parameter vector. Have fun with that, laptop.

- Assumption: Conditional Independence

    $$\begin{equation}
    p(x_i|y)=p(x_i|y,x_j)\ \ \ \forall j\neq i
    \end{equation}$$
    
    meaning: Given $y$ as the condition, $x_i$ is independent of $x_j$. 
