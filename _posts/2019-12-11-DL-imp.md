---
layout: post
title: "Improvements on Neural Networks"
date: 2019-12-01 22:29:53 +0900
permalink: /DL/imp/
header-includes:
- \usepackage{amsmath}
---

- **Improvements on Neural Networks**
    - [Train/Test Split](#split)
    - [Initialization](#init)
    - [Data Fitting](#fit) with refined [procedure](#pro)
    - [Regularization](#reg)
        - Regularization on LogReg
            - [L2 Regularization](#L2)
            - [L1 Regularization](#L1)
        - [Regularization on NN](#nnreg)
        - [Dropout](#dp)
        - [Data Augmentation](#da)
        - [Early Stopping](#es)
    - [Optimization](#op)
    - [Hyperparameter Tuning](#gd)
      
&emsp;<a name="split"></a>
## Train/Test Split

- Dataset = training set + development/validation set + test set
- Split ratio:
    - old era: 70/0/30%, 60/20/20%, ...
    - big data era: 98/1/1%, 99.5/0.4/0.1%, 99.5/0.5/0%, ... \\
    (trend: testset as small as possible)
- All 3 subsets should come from the exact same distribution (<strike>mismatch<strike>)


&emsp;<a name="init"></a>
## Initialization

- $W$ should be initialized with **small random values** to break symmetry (to make sure that different hidden nodes can learn different things)
- $b$ can be initialized to **zeros** ($\because$ symmetry is still broken when $W$ is randomly initialized)
- Different initializations $\rightarrow$ different results
- Refer to [keras documentation](https://keras.io/initializers/) for initializers.
  
&emsp;<a name="fit"></a>
## Data Fitting

**Underfitting**:

<center><img src="../../images/DL/uf.png" width="300"/></center>

**Proper fitting**:

<center><img src="../../images/DL/nof.png" width="300"/></center>

**Overfitting**:

<center><img src="../../images/DL/of.png" width="300"/></center>
&nbsp;

Tradeoff: *train error* vs *validation error*:
- ***train err* too small $\longrightarrow$ high variance (overfitting)**  
(e.g. train err = 1%; val err = 11%)
- ***train err* too big $\longrightarrow$ high bias (underfitting)**  
(e.g. train err = 17%; val err = 16%)
- ***train err* too big & *val err* even bigger $\longrightarrow$ both probs**   
(e.g. train err = 17%; val err = 34%)
- ***train err* too small & *val err* also small $\longrightarrow$ congratulations!**  
(e.g. train err = 0.5%; val err = 1%)
&nbsp;  
&nbsp;  

<a name="pro"></a>
**The Procedure**:

<center><img src="../../images/DL/fit.png" width="500"/></center>

&emsp;<a name="reg"></a>
## Regularization

Idea: add a regularization term to the original loss function:

$$\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}f(w)
\end{equation}$$

- $\lambda$: regularization parameter
- $f(w)$: regularization on $w$

How does regularization prevent overfitting?

- set $\lambda$ as big as possible $\Rightarrow w^{[l]}\approx 0$ $\Rightarrow z^{[l]}\approx 0$ $\Rightarrow$ as if some hidden nodes don't exist any more
- $\Rightarrow$ less complexity $\Rightarrow$ variance $\downarrow$

&nbsp;  
**Regularization on LogReg**:
- <a name="L2"></a>**L2 Regularization**:

$$\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\|w\|^2_2 \\
\|w\|^2_2=\sum_{j=1}^{n_x}w_j^2=w^Tw
\end{equation}$$

- <a name="L1"></a>**L1 Regularization**:

$$\begin{equation}
\mathcal{J}(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\|w\|_1 \\
\|w\|_1=\sum_{j=1}^{n_x}{|w|}
\end{equation}$$

<a name="nnreg"></a>**Regularization on NN**:

$$\begin{equation}
\mathcal{J}(W^{[k]},b^{[k]})=\frac{1}{m}\sum_{i=1}^{m}{\mathcal{L}(\hat{y}^{(i)},y^{(i)})}+\frac{\lambda}{2m}\sum_{l=1}^L{\|W^{[l]}\|^2_F}
\end{equation}$$

- Frobenius Norm: 

$$\begin{equation}
\|W^{[l]}\|^2_F=\sum_{i=1}^{n_{l-1}}\sum_{j=1}^{n_l}{(w_{ij}^{[l]})^2}
\end{equation}$$

- Weight Decay on GD:

$$\begin{align}
W^{[l]}&:=w^{[l]}-\alpha\cdot\frac{\partial{\mathcal{L}}}{\partial{W^{[l]}}} \\
&=w^{[l]}-\alpha\cdot\Big(\frac{\partial{\mathcal{L}}}{\partial{W^{[l]}}}(\text{original})+\frac{\lambda}{m}W^{[l]}\Big)
\end{align}$$

&nbsp;  
<a name="dp"></a>**Dropout**: each node has a probability to be kicked out of the NN
