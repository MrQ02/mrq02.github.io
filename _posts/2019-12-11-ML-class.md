---
layout: post
title: "Classification"
date: 2019-12-01 22:29:53 +0900
permalink: /ML/class/
header-includes:
- \usepackage{amsmath}
- \usepackage{mathtools}
---
Roadmap:
- [Logistic Regression](#logreg)
    - [Learning](#learn)
        - [Gradient Descent](#gd)
        - [Newton's Method](#newton)
        - [Normal Equation](#normal)
    - [Code Template](#code)  
<br>
- [k-Nearest Neighbors](#knn)
    - [Statistical Setting for Classification](#statsclass)  
<br>
- [Gaussian Discriminant Analysis](#gda)
    - [2 Types of Learning Algorithms & Bayes Theorem](#bayes)  
<br>
- [Naive Bayes Classifier](#nb)
    - [Laplace Smoothing](#laplace)  

&emsp;<a name="logreg"></a>
## Logistic Regression

- **Problem Setting**

    - **Data**: Observed pairs $(x,y)$, where $x\in\mathcal{X}$ & $y\in\mathcal{Y}$
        - $\mathcal{Y}=\\{-1,+1\\}\lor\\{0,1\\}$: binary classification
        - $\mathcal{Y}=\\{1,...,K\\}$: multiclass classification
    - **Goal**: Find a classifier $f$ that can map input $x$ to class $y$: $y=f(x):\ "x\in\mathcal{X}"\rightarrow\ "y\in\mathcal{Y}"$  
    
<br>
- **Model**

    $$\begin{equation}
    \hat{y}=g(w^Tx)
    \end{equation}$$

    $g(z)$: a function that converts $w^Tx$ to binary value

- Sigmoid Function (see Deep Learning for more funcs)

    $$\begin{equation}
    g(z)=\sigma(z)=\frac{1}{1+e^{-z}}
    \end{equation}$$
    
    - Derivative (you will know why we need this in Deep Learning)
    
        $$\begin{align}
        g'(z)&=\frac{d}{dz}\frac{1}{1+e^{-z}} \\
        &=\frac{e^{-z}(+1-1)}{(1+e^{-z})^2} \\
        &=g(z)(1-g(z))
        \end{align}$$

- Cost Function


    1. single training example (derivation later)
    
        $$\begin{equation}
        \mathcal{L}(\hat{y},y)=-(y\log{\hat{y}}+(1-y)\log{(1-\hat{y})})
        \end{equation}$$
        
        If $y=1\rightarrow\mathcal{L}(\hat{y},y)=-\log{\hat{y}}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\uparrow$"$\rightarrow\hat{y}=1$   
        If $y=0\rightarrow\mathcal{L}(\hat{y},y)=-\log{(1-\hat{y})}\rightarrow$ want "$\mathcal{L}\downarrow\leftrightarrow\hat{y}\downarrow$"$\rightarrow\hat{y}=0$ 
        
    2. entire training set
    
        $$\begin{equation}
        \mathcal{J}(w)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat{y}^{(i)},y^{(i)})=\text{mean}(\mathcal{L})
        \end{equation}$$

- Probabilistic Interpretation

    1. Assumptions
    
        $$\begin{align}
        P(y=1|x,w)&=\hat{y} \\
        P(y=0|x,w)&=1-\hat{y}
        \end{align}$$

    2. Probabilistic Model of LogReg
    
        $$\begin{equation}
        p(y|x,w)=\hat{y}^y(1-\hat{y})^{1-y}
        \end{equation}$$
        
    3. Likelihood Function
    
        $$\begin{equation}
        L(w)=\prod_{i=1}^{m}(\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}
        \end{equation}$$
        
    4. Log Likelihood
    
        $$\begin{align}
        l(w)&=\sum_{i=1}^{m}(y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log{(1-\hat{y}^{(i)})}) \\
        l(w)&=-\sum_{i=1}^{m}\mathcal{L}(\hat{y},y)
        \end{align}$$
        
    5. MLE
        
        $$\begin{align}
        \frac{\partial l(w)}{\partial w_j}&=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})\frac{\partial g(w^Tx)}{\partial w_j} \\
        &=(\frac{y}{g(w^Tx)}-\frac{1-y}{1-g(w^Tx)})g(w^Tx)(1-g(w^Tx))\frac{\partial(w^Tx)}{\partial w_j} \\
        &=(y(1-g(w^Tx))-(1-y)g(w^Tx))x_j \\
        &=(y-\hat{y})x_j
        \end{align}$$

- Gradient Descent

    $$\begin{align}
    w_j &:= w_j-\alpha\frac{\partial\mathcal{L}(w)}{\partial w_j} \\
    &=w_j+\alpha(y-\hat{y})x_j
    \end{align}$$
    
    Why is it also called "Gradient Ascent"?  
    $\because$ we are trying to minimize the loss function $\Leftrightarrow$ maximize the likelihood function

&emsp;<a name="knn"></a>
## k-Nearest Neighbors

- <a name="knnalg"></a>**Algorithm**

    For a new input $x$,

    1. Return the $k$ points **closest** to $x$, indexed as $x_{i_1},...,x_{i_k}$.
    2. Return the majority votes of $y_{i_1},...,y_{i_k}$.  
<br>
- **Distances** (how to measure "closest")

    - **Euclidean distance**: default measurement
    
        $$\begin{equation}
        \|u-v\|_ 2=\Big(\sum_{i=1}^n(u_i-v_i)^2\Big)^{\frac{1}{2}}
        \end{equation}$$
        
    - **$l_p$**: variation on Euclidean
    
        $$\begin{equation}
        \|u-v\|_ p=\Big(\sum_{i=1}^n|u_i-v_i|^p\Big)^{\frac{1}{p}}\ \ \ |\ p\in[1,\infty]
        \end{equation}$$
        
    - **Edit distance**: for strings
    
        <center>#modifications required to transform one string to the other</center>
    <br>
    - **Correlation distance**: for signals
    
        <center>how correlated 2 vectors are for signal detection</center>
<br>        
- **$k$**
    
    - Smaller $k$ $\Rightarrow$ smaller training error but could lead to overfitting
    - Larger $k$ $\Rightarrow$ more stable predictions due to voting  
<br>
- **Statistical Setting for Classification**
    
    - **Performance**
    
        - Prediction accuracy: $P(f(x)=y)$
        - Prediction error: $P(f(x)\neq y)$
        
    - **Key Assumption for Supervised Learning**
    
        $$\begin{equation}
        (x_i,y_i)\sim\mathcal{P}\ \ \ |\ \ \ i=1,\cdots,n
        \end{equation}$$
        
        - i.i.d. (independent & identically distributed)
        - We assume that the future should look like the past.

&emsp;<a name="gda"></a>
## Gaussian Discriminant Analysis

- <a name="bayes"></a>Learning Algorithms

    - Discriminative Learning Algorithms

    $$\begin{equation}
    \text{model }p(y|x)\text{ directly}\ \ \ (X \Rightarrow Y)
    \end{equation}$$

    - Generative Learning Algorithms

    $$\begin{equation}
    \text{model }p(x|y)\ \&\ p(y)\Rightarrow\text{ use Bayes Theorem to get }p(y|x) 
    \end{equation}$$

- Bayes Theorem

    $$\begin{equation}
    p(y|x)=\frac{p(x|y)p(y)}{p(x)}
    \end{equation}$$

    - **Prior**: &emsp;&emsp;$p(y)$
    - **Posterior**: $p(y\|x)$

    - Simplification:

        $\because$ we are trying to find the output $y$ with the highest probability given $x$  
        $\therefore$ we can simplify Bayes Theorem for our purpose:

        $$\begin{align}
        \mathop{\arg\max}_ {y}{p(y|x)}&=\mathop{\arg\max}_ {y}{\frac{p(x|y)p(y)}{p(x)}} \\
        &=\mathop{\arg\max}_ {y}{p(x|y)p(y)}
        \end{align}$$

    - Bayes Theorem = the core of Generative Learning Algorithms  
<br>
- Assumption: Multivariate Gaussian Distribution

    $$\begin{equation}
    p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\big)}
    \end{equation}$$
    
    It is literally the same as Gaussian Distribution but with vector parameters:
    
    - mean vector: &emsp;&emsp;&nbsp;$\mu\in\mathbb{R}^n$
    - covariance matrix: $\Sigma\in\mathbb{R}^{n\times n}$  
    &emsp;  
    
    As a reminder and a comparison, here is the univariate version:
    
    $$\begin{equation}
    p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}$$
    
- Model

    $$\begin{align}
    y&\sim \text{Bernoulli}{(\phi)} \\
    x|y=0&\sim N(\mu_0,\Sigma) \\
    x|y=1&\sim N(\mu_1,\Sigma) \\
    \end{align}$$
        
- Probabilistic Interpretation

    $$\begin{align}
    p(y)&=\phi^y(1-\phi)^{1-y} \\
    p(x|y=0)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\big)} \\
    p(x|y=1)&=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{\big(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\big)}
    \end{align}$$
        
- log likelihood

    $$\begin{equation}
    l(\phi,\mu_0,\mu_1,\Sigma)=\log{\prod_{i=1}^{m}{p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)}}
    \end{equation}$$
        
- MLE

    $$\begin{align}
    \phi &= \frac{1}{m}\sum_{i=1}^m{\text{I}\{ y^{(i)}=l \}} \\
    \mu_0 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=0 \}}} \\
    \mu_1 &= \frac{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}x^{(i)}}}{\sum_{i=1}^m{\text{I}\{ y^{(i)}=1 \}}} \\
    \Sigma &= \frac{1}{m}\sum_{i=1}^m{(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T}
    \end{align}$$
    
- GDA vs LogReg
    1. GDA
        - makes **stronger** modeling assumptions about data
        - data efficient when assumptions (Gaussian distributions) are approximately correct
    2. LogReg
        - makes **weaker** modeling assumptions about data
        - data efficient when assumptions (Gaussian distributions) are not necessarily correct (e.g. $x\|y\sim \text{Poisson}(\lambda_1)$ instead of $N(\mu_0,\Sigma)$)

&emsp;<a name="nb"></a>
## Naive Bayes Classifier

- GDA vs NB
    
    1. GDA: $x$ = continuous, real-valued vectors
    2. NB: &nbsp;&nbsp;$x$ = discrete-valued vectors (e.g. text classification)  
    &emsp;
- Text Encoding (more in DL/RNN)  
    
    We encode a text sentence into a vector of the same length as our **dictionary** (like a Python dictionary with vocabulary and their indices as key-value pairs):
    
    $$\begin{equation}
    x=\begin{bmatrix}
    0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 1 \\ 1 \\ \vdots \\ 0
    \end{bmatrix}
    \begin{matrix}
    \text{a} \\ \text{abandon} \\ \vdots \\ \text{pewdiepie} \\ \vdots \\ \text{subscribe} \\ \text{to} \\ \vdots \\ \text{zuck}
    \end{matrix}
    \end{equation}$$
    
    The original sentence was "Subscribe to Pewdiepie!", and this text encoding method uses lowercases, throws punctuations and ignores the order of the sentence. This is convenient in some cases (e.g. spam email classification) but awful in the other cases (e.g. news/report-writer bots)
    
    Notice that $x\in \\{0,1\\}^{\text{len(dict)}}$. Why notice this? Because we now have $2^\text{len(dict)}$ possible outcomes for $x$. When we have a dictionary of over 20000 words, we have a $\(2^{20000}-1\)$-dimensional parameter vector. Have fun with that, laptop.

- Assumption: Conditional Independence

    $$\begin{equation}
    p(x_i|y)=p(x_i|y,x_j)\ \ \ \forall j\neq i
    \end{equation}$$
    
    meaning: Given $y$ as the condition, $x_i$ is independent of $x_j$.  
    
    In the case of spam email classification, if we know that the email is spam, then whether or not "pewdiepie" is in the sentence does not change our belief of whether or not "subscribe" is in the sentence.  
    
    Therefore, we can simplify our $p(x\|y)$ into:
    
    $$\begin{equation}
    p(x_1,...,x_{\text{len(dict)}}|y)=\prod_{i=1}^{n}{p(x_i|y)}
    \end{equation}$$
    
- Model

    $$\begin{align}
    \phi_{i|y=1}&=p(x_i=1|y=1) \\
    \phi_{i|y=0}&=p(x_i=1|y=0) \\
    \phi_y&=p(y=1)
    \end{align}$$

- Joint Likelihood

    $$\begin{equation}
    \mathcal{L}(\phi_y,\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^{m}{p(x^{(i)},y^{(i)})}
    \end{equation}$$
    
- MLE
    
    $$\begin{align}
    \phi_{j|y=1}&=\frac{\sum_{i=1}^m{I\{x_j^{(i)}=1\land y^{(i)}=1\}}}{\sum_{i=1}^m{I\{y^{(i)}=1\}}} \\
    \phi_{j|y=0}&=\frac{\sum_{i=1}^m{I\{x_j^{(i)}=1\land y^{(i)}=0\}}}{\sum_{i=1}^m{I\{y^{(i)}=0\}}} \\
    \phi_y&=\frac{\sum_{i=1}^m{I\{y^{(i)}=1\}}}{m}
    \end{align}$$
    
    Quite intuitive. For example, $\phi_{j\|y=0}$ = the fraction of non-spam emails with the word $j$ in it. 
    
- Prediction
    
    $$\begin{align}
    p(y=1|x_\text{new})&=\frac{p(x_\text{new}|y=1)p(y=1)}{p(x_\text{new})} \\
    &=\frac{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)}{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)+\prod_{i=1}^n{p(x_i|y=0)}\cdot p(y=0)}
    \end{align}$$
    
    Again, the formula is tedious but very intuitive. The $y$ with the higher posterior probability will be chosen as the final prediction.
    
- Apply NB in GDA cases?

    Discretize: Just cut the continuous, real-valued $x$ into small intervals and label them with a discrete-valued scale.
    
<br><a name="laplace"></a>
### <strong>Laplace Smoothing</strong>

<u>Problem</u>: What if there is a new word "mrbeast" in the email for prediction that our NB classifier has never learnt ever since it was born?
    
A human would look it up on a dictionary, and so would our NB classifier.

Assume the word "mrbeast" is the 1234th word in the dictionary, then:

$$\begin{align}
\phi_{1234|y=1}&=\frac{\sum_{i=1}^m{I\{x_{1234}^{(i)}=1\land y^{(i)}=1\}}}{\sum_{i=1}^m{I\{y^{(i)}=1\}}}=0 \\
\phi_{1234|y=0}&=\frac{\sum_{i=1}^m{I\{x_{1234}^{(i)}=1\land y^{(i)}=0\}}}{\sum_{i=1}^m{I\{y^{(i)}=0\}}}=0 \\
\end{align}$$

Yes. NB thinks that the probability of seeing this word in either spam or non-spam email is $0$, and therefore it would predict that:

$$\begin{align}
p(y=1|x_\text{new})&=\frac{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)}{\prod_{i=1}^n{p(x_i|y=1)}\cdot p(y=1)+\prod_{i=1}^n{p(x_i|y=0)}\cdot p(y=0)} \\
&=\frac{0}{0}
\end{align}$$

Because both numerator and denominator contains $p(x_{1234\|y})=0$.

In summary, during prediction, if NB has never learnt a word $j$, there will always $\phi_j=0$ ruining the entire prediction. How do we estimate the unknown?

<u>Algorithm</u>:

$$\begin{equation}
\phi_j=\frac{\sum_{i=1}^m{I\{z^{(i)}=j\}}+1}{m+k}
\end{equation}$$

where $k=\text{#features}$ if you forget.

Let's check if it still satisfies our condition:

$$\begin{equation}
\sum_{j=1}^k{\phi_j}=\sum_{j=1}^k{\frac{\sum_{i=1}^m{I\{z^{(i)}=j\}}+1}{m+k}}=\frac{m+k}{m+k}=1
\end{equation}$$

Nice. It still satisfies the basic sum rule. The estimates in NB will now become:

$$\begin{align}
\phi_{j|y=1}&=\frac{\sum_{i=1}^m{I\{x_{j}^{(i)}=1\land y^{(i)}=1\}}+1}{\sum_{i=1}^m{I\{y^{(i)}=1\}}+2} \\
\phi_{j|y=0}&=\frac{\sum_{i=1}^m{I\{x_{j}^{(i)}=1\land y^{(i)}=0\}}+1}{\sum_{i=1}^m{I\{y^{(i)}=0\}}+2} \\
\end{align}$$